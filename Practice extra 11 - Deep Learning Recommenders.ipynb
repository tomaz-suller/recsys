{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender Systems 2024/25\n",
    "\n",
    "### Practice 9 - Deep Learning Models\n",
    "\n",
    "## The basics of Deep Learning: Multi-Layer Perceptron "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movielens10M: Verifying data consistency...\n",
      "Movielens10M: Verifying data consistency... Passed!\n",
      "DataReader: current dataset is: Movielens10M\n",
      "\tNumber of items: 10681\n",
      "\tNumber of users: 69878\n",
      "\tNumber of interactions in URM_all: 10000054\n",
      "\tValue range in URM_all: 0.50-5.00\n",
      "\tInteraction density: 1.34E-02\n",
      "\tInteractions per user:\n",
      "\t\t Min: 2.00E+01\n",
      "\t\t Avg: 1.43E+02\n",
      "\t\t Max: 7.36E+03\n",
      "\tInteractions per item:\n",
      "\t\t Min: 0.00E+00\n",
      "\t\t Avg: 9.36E+02\n",
      "\t\t Max: 3.49E+04\n",
      "\tGini Index: 0.57\n",
      "\n",
      "\tICM name: ICM_tags, Value range: 1.00 / 69.00, Num features: 10106, feature occurrences: 106820, density 9.90E-04\n",
      "\tICM name: ICM_genres, Value range: 1.00 / 1.00, Num features: 20, feature occurrences: 21564, density 1.01E-01\n",
      "\tICM name: ICM_all, Value range: 1.00 / 69.00, Num features: 10126, feature occurrences: 128384, density 1.19E-03\n",
      "\tICM name: ICM_year, Value range: 1.92E+03 / 2.01E+03, Num features: 1, feature occurrences: 10681, density 1.00E+00\n",
      "\n",
      "\n",
      "Warning: 73 (0.10 %) of 69878 users have no sampled items\n",
      "Warning: 252 (0.36 %) of 69878 users have no sampled items\n"
     ]
    }
   ],
   "source": [
    "from Data_manager.split_functions.split_train_validation_random_holdout import split_train_in_two_percentage_global_sample\n",
    "from Data_manager.Movielens.Movielens10MReader import Movielens10MReader\n",
    "\n",
    "data_reader = Movielens10MReader()\n",
    "data_loaded = data_reader.load_data()\n",
    "\n",
    "URM_all = data_loaded.get_URM_all()\n",
    "\n",
    "URM_train_val, URM_test = split_train_in_two_percentage_global_sample(URM_all, 0.8)\n",
    "URM_train, URM_val = split_train_in_two_percentage_global_sample(URM_train_val, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "if torch.backends.mps.is_available(): # if torch.cuda.is_available() if you use NVIDIA GPUs\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# create a custom dataset class\n",
    "class IrisDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a custom nn.Module class\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 16)\n",
    "        self.fc2 = nn.Linear(16, 32)\n",
    "        self.fc3 = nn.Linear(32, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss: 1.171\n",
      "Epoch 2, loss: 1.134\n",
      "Epoch 3, loss: 1.108\n",
      "Epoch 4, loss: 1.086\n",
      "Epoch 5, loss: 1.066\n",
      "Epoch 6, loss: 1.052\n",
      "Epoch 7, loss: 1.037\n",
      "Epoch 8, loss: 1.024\n",
      "Epoch 9, loss: 1.013\n",
      "Epoch 10, loss: 0.998\n",
      "Epoch 11, loss: 0.984\n",
      "Epoch 12, loss: 0.971\n",
      "Epoch 13, loss: 0.958\n",
      "Epoch 14, loss: 0.945\n",
      "Epoch 15, loss: 0.933\n",
      "Epoch 16, loss: 0.920\n",
      "Epoch 17, loss: 0.907\n",
      "Epoch 18, loss: 0.896\n",
      "Epoch 19, loss: 0.881\n",
      "Epoch 20, loss: 0.869\n",
      "Epoch 21, loss: 0.853\n",
      "Epoch 22, loss: 0.841\n",
      "Epoch 23, loss: 0.830\n",
      "Epoch 24, loss: 0.818\n",
      "Epoch 25, loss: 0.801\n",
      "Epoch 26, loss: 0.790\n",
      "Epoch 27, loss: 0.779\n",
      "Epoch 28, loss: 0.765\n",
      "Epoch 29, loss: 0.752\n",
      "Epoch 30, loss: 0.741\n",
      "Epoch 31, loss: 0.725\n",
      "Epoch 32, loss: 0.715\n",
      "Epoch 33, loss: 0.700\n",
      "Epoch 34, loss: 0.685\n",
      "Epoch 35, loss: 0.677\n",
      "Epoch 36, loss: 0.665\n",
      "Epoch 37, loss: 0.659\n",
      "Epoch 38, loss: 0.653\n",
      "Epoch 39, loss: 0.636\n",
      "Epoch 40, loss: 0.631\n",
      "Epoch 41, loss: 0.617\n",
      "Epoch 42, loss: 0.610\n",
      "Epoch 43, loss: 0.601\n",
      "Epoch 44, loss: 0.589\n",
      "Epoch 45, loss: 0.577\n",
      "Epoch 46, loss: 0.579\n",
      "Epoch 47, loss: 0.563\n",
      "Epoch 48, loss: 0.553\n",
      "Epoch 49, loss: 0.547\n",
      "Epoch 50, loss: 0.547\n",
      "Epoch 51, loss: 0.537\n",
      "Epoch 52, loss: 0.532\n",
      "Epoch 53, loss: 0.527\n",
      "Epoch 54, loss: 0.522\n",
      "Epoch 55, loss: 0.513\n",
      "Epoch 56, loss: 0.513\n",
      "Epoch 57, loss: 0.497\n",
      "Epoch 58, loss: 0.499\n",
      "Epoch 59, loss: 0.496\n",
      "Epoch 60, loss: 0.494\n",
      "Epoch 61, loss: 0.490\n",
      "Epoch 62, loss: 0.483\n",
      "Epoch 63, loss: 0.480\n",
      "Epoch 64, loss: 0.474\n",
      "Epoch 65, loss: 0.465\n",
      "Epoch 66, loss: 0.467\n",
      "Epoch 67, loss: 0.464\n",
      "Epoch 68, loss: 0.460\n",
      "Epoch 69, loss: 0.456\n",
      "Epoch 70, loss: 0.451\n",
      "Epoch 71, loss: 0.443\n",
      "Epoch 72, loss: 0.440\n",
      "Epoch 73, loss: 0.438\n",
      "Epoch 74, loss: 0.440\n",
      "Epoch 75, loss: 0.432\n",
      "Epoch 76, loss: 0.427\n",
      "Epoch 77, loss: 0.423\n",
      "Epoch 78, loss: 0.417\n",
      "Epoch 79, loss: 0.420\n",
      "Epoch 80, loss: 0.418\n",
      "Epoch 81, loss: 0.410\n",
      "Epoch 82, loss: 0.415\n",
      "Epoch 83, loss: 0.414\n",
      "Epoch 84, loss: 0.406\n",
      "Epoch 85, loss: 0.407\n",
      "Epoch 86, loss: 0.397\n",
      "Epoch 87, loss: 0.398\n",
      "Epoch 88, loss: 0.394\n",
      "Epoch 89, loss: 0.392\n",
      "Epoch 90, loss: 0.392\n",
      "Epoch 91, loss: 0.382\n",
      "Epoch 92, loss: 0.380\n",
      "Epoch 93, loss: 0.378\n",
      "Epoch 94, loss: 0.374\n",
      "Epoch 95, loss: 0.375\n",
      "Epoch 96, loss: 0.369\n",
      "Epoch 97, loss: 0.370\n",
      "Epoch 98, loss: 0.366\n",
      "Epoch 99, loss: 0.360\n",
      "Epoch 100, loss: 0.370\n",
      "Test loss: 0.359, Accuracy: 94.17\n"
     ]
    }
   ],
   "source": [
    "# create a data loader and model\n",
    "dataset = IrisDataset(X_train, y_train)\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "model = MLP()\n",
    "\n",
    "# define a loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# train the model\n",
    "for epoch in range(100):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(data_loader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    print('Epoch %d, loss: %.3f' % (epoch+1, running_loss/(i+1)))\n",
    "\n",
    "# evaluate the model\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for data in data_loader:\n",
    "        inputs, labels = data\n",
    "        outputs = model(inputs)\n",
    "        test_loss += criterion(outputs, labels).item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = correct / len(dataset)\n",
    "print('Test loss: {:.3f}, Accuracy: {:.2f}'.format(test_loss/(len(data_loader)), accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two-Tower Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repro '19 on Maurizio's Github – NeuMF: principal components and explain\n",
    "# https://github.com/MaurizioFD/RecSys2019_DeepLearning_Evaluation/blob/0fb6b7f5c396f8525316ed66cf9c9fdb03a5fa9b/Conferences/WWW/NeuMF_our_interface/NeuMF_RecommenderWrapper.py#L110C33-L110C65\n",
    "# Rows 108-113\n",
    "# Need to convert to pytorch\n",
    "def NeuCF_get_model(num_users, num_items, mf_dim=10, layers=[10], reg_layers=[0], reg_mf=0.0):\n",
    "    assert len(layers) == len(reg_layers)\n",
    "    num_layer = len(layers) #Number of layers in the MLP\n",
    "    # Input variables\n",
    "    user_input = Input(shape=(1,), dtype='int32', name = 'user_input')\n",
    "    item_input = Input(shape=(1,), dtype='int32', name = 'item_input')\n",
    "\n",
    "    # Embedding layer\n",
    "    MF_Embedding_User = Embedding(input_dim = num_users, output_dim = mf_dim, name = 'mf_embedding_user',\n",
    "                                  embeddings_initializer = 'random_normal', embeddings_regularizer = l2(reg_mf), input_length=1)\n",
    "    MF_Embedding_Item = Embedding(input_dim = num_items, output_dim = mf_dim, name = 'mf_embedding_item',\n",
    "                                  embeddings_initializer = 'random_normal', embeddings_regularizer = l2(reg_mf), input_length=1)\n",
    "\n",
    "    MLP_Embedding_User = Embedding(input_dim = num_users, output_dim = int(layers[0]/2), name = \"mlp_embedding_user\",\n",
    "                                   embeddings_initializer = 'random_normal', embeddings_regularizer = l2(reg_layers[0]), input_length=1)\n",
    "    MLP_Embedding_Item = Embedding(input_dim = num_items, output_dim = int(layers[0]/2), name = 'mlp_embedding_item',\n",
    "                                   embeddings_initializer = 'random_normal', embeddings_regularizer = l2(reg_layers[0]), input_length=1)\n",
    "\n",
    "    # MF part\n",
    "    mf_user_latent = Flatten()(MF_Embedding_User(user_input))\n",
    "    mf_item_latent = Flatten()(MF_Embedding_Item(item_input))\n",
    "    mf_vector = Multiply()([mf_user_latent, mf_item_latent]) # element-wise multiply\n",
    "\n",
    "    # MLP part\n",
    "    mlp_user_latent = Flatten()(MLP_Embedding_User(user_input))\n",
    "    mlp_item_latent = Flatten()(MLP_Embedding_Item(item_input))\n",
    "    mlp_vector = Concatenate()([mlp_user_latent, mlp_item_latent])\n",
    "    for idx in range(1, num_layer):\n",
    "        layer = nn.Dense(layers[idx], kernel_regularizer= l2(reg_layers[idx]), activation='relu', name=\"layer%d\" %idx)\n",
    "        mlp_vector = layer(mlp_vector)\n",
    "\n",
    "    # Concatenate MF and MLP parts\n",
    "    predict_vector = Concatenate()([mf_vector, mlp_vector])\n",
    "\n",
    "    # Final prediction layer\n",
    "    prediction = Dense(1, activation='sigmoid', kernel_initializer='lecun_uniform', name = \"prediction\")(predict_vector)\n",
    "\n",
    "    model = Model(inputs=[user_input, item_input],\n",
    "                  outputs=prediction)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NeuCF_get_model(num_users, num_items, mf_dim=10, layers=[10], reg_layers=[0], reg_mf=0.0):\n",
    "    assert len(layers) == len(reg_layers)\n",
    "    num_layer = len(layers) #Number of layers in the MLP\n",
    "\n",
    "    class NeuCF(nn.Module):\n",
    "        def __init__(self, num_users, num_items, mf_dim, layers, reg_layers, reg_mf):\n",
    "            super(NeuCF, self).__init__()\n",
    "            self.mf_embedding_user = nn.Embedding(num_users, mf_dim)\n",
    "            self.mf_embedding_item = nn.Embedding(num_items, mf_dim)\n",
    "            self.mlp_embedding_user = nn.Embedding(num_users, int(layers[0]/2))\n",
    "            self.mlp_embedding_item = nn.Embedding(num_items, int(layers[0]/2))\n",
    "\n",
    "            self.mlp_layers = nn.ModuleList([nn.Linear(int(layers[0]/2)*2, layers[i], bias=True) for i in range(1, num_layer)])\n",
    "            for i, layer in enumerate(self.mlp_layers):\n",
    "                nn.init.normal_(layer.weight)\n",
    "                layer.bias.data.zero_()\n",
    "                layer.weight_decay = reg_layers[i]\n",
    "\n",
    "            self.prediction_layer = nn.Linear(mf_dim + layers[-1], 1, bias=True)\n",
    "            nn.init.lecun_uniform_(self.prediction_layer.weight)\n",
    "            self.prediction_layer.bias.data.zero_()\n",
    "\n",
    "        def forward(self, user_input, item_input):\n",
    "            mf_user_latent = self.mf_embedding_user(user_input)\n",
    "            mf_item_latent = self.mf_embedding_item(item_input)\n",
    "            mf_vector = torch.mul(mf_user_latent, mf_item_latent)\n",
    "\n",
    "            mlp_user_latent = self.mlp_embedding_user(user_input)\n",
    "            mlp_item_latent = self.mlp_embedding_item(item_input)\n",
    "            mlp_vector = torch.cat((mlp_user_latent, mlp_item_latent), dim=1)\n",
    "            for layer in self.mlp_layers:\n",
    "                mlp_vector = torch.relu(layer(mlp_vector))\n",
    "\n",
    "            predict_vector = torch.cat((mf_vector, mlp_vector), dim=1)\n",
    "            prediction = torch.sigmoid(self.prediction_layer(predict_vector))\n",
    "            return prediction\n",
    "\n",
    "    model = NeuCF(num_users, num_items, mf_dim, layers, reg_layers, reg_mf)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Recommenders.BaseRecommender import BaseRecommender\n",
    "import scipy.sparse as sp\n",
    "# Variant 1:\n",
    "# 1. Make 2 embeddings of equal dimensions and concatenate\n",
    "# 2. Couple of Dense layers\n",
    "# 3. Obtain prediction (single score)\n",
    "class TwoTowerRecommender_type1(nn.Module, BaseRecommender):\n",
    "    \n",
    "    RECOMMENDER_NAME = \"\"\"TWO_TOWER_1\"\"\"\n",
    "    \n",
    "    def __init__(self, URM_train, num_users, num_items, layers=[10], reg_layers=[0], verbose = True):\n",
    "        global device\n",
    "        super().__init__()\n",
    "        BaseRecommender.__init__(self, URM_train, verbose=verbose)\n",
    "        self.device = device\n",
    "        self.mlp_embedding_user = nn.Embedding(num_users, int(layers[0]/2), device=self.device)\n",
    "        self.mlp_embedding_item = nn.Embedding(num_items, int(layers[0]/2), device=self.device)\n",
    "\n",
    "        self.mlp_layers = nn.ModuleList([\n",
    "            nn.Linear(layers[i-1], layers[i], bias=True, device=self.device) for i in range(1, len(layers))\n",
    "            ])\n",
    "        for i, layer in enumerate(self.mlp_layers):\n",
    "            nn.init.normal_(layer.weight)\n",
    "            layer.bias.data.zero_()\n",
    "            layer.weight_decay = reg_layers[i]\n",
    "\n",
    "        self.prediction_layer = nn.Linear(layers[-1], 1, bias=True, device=self.device)\n",
    "        nn.init.uniform_(self.prediction_layer.weight)\n",
    "        self.prediction_layer.bias.data.zero_()\n",
    "        self.to(self.device)\n",
    "\n",
    "    def _data_generator(self, batch_size):\n",
    "        # stupid sampling, will lead to overfit zeros\n",
    "        row_idx = np.random.choice(self.URM_train.shape[0], batch_size, replace=False).astype(np.int32)\n",
    "        col_idx = np.random.choice(self.URM_train.shape[1], batch_size, replace=False).astype(np.int32)\n",
    "        user_input = torch.tensor(row_idx, device=self.device)\n",
    "        item_input = torch.tensor(col_idx, device=self.device)\n",
    "        labels = np.zeros(batch_size, dtype=np.int32)\n",
    "        for i in range(len(labels)):\n",
    "            if self.URM_train[row_idx[i], col_idx[i]] > 0:\n",
    "                labels[i] = 1\n",
    "        labels.shape = (batch_size, 1)\n",
    "        labels = torch.tensor(labels, device=self.device)\n",
    "        yield user_input, item_input, labels\n",
    "\n",
    "    def forward(self, user_input, item_input):\n",
    "        mlp_user_latent = self.mlp_embedding_user(user_input.long().to(self.device))\n",
    "        mlp_item_latent = self.mlp_embedding_item(item_input.long().to(self.device))\n",
    "        mlp_vector = torch.cat((mlp_user_latent, mlp_item_latent), dim=1)\n",
    "        for layer in self.mlp_layers:\n",
    "            mlp_vector = torch.relu(layer(mlp_vector))\n",
    "\n",
    "        predict_vector = mlp_vector\n",
    "        prediction = torch.sigmoid(self.prediction_layer(predict_vector))\n",
    "        return prediction\n",
    "    \n",
    "    def fit(self, epochs=30, batch_size=1024, learning_rate=0.0001):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        for i in range(epochs):\n",
    "            for user_input, item_input, labels in self._data_generator(batch_size):\n",
    "                optimizer.zero_grad()\n",
    "                predictions = self.forward(user_input, item_input)\n",
    "                loss = torch.nn.BCELoss().to(self.device)\n",
    "                loss = loss(predictions, labels.float())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            self._print(\"Epoch {} finished. Loss: {}\".format(i, loss.item()))\n",
    "\n",
    "    def _compute_item_score(self, user_id_array, items_to_compute=None):\n",
    "        step = user_id_array.shape[0]\n",
    "        \n",
    "        if items_to_compute is None:\n",
    "            items_to_compute = np.arange(self.URM_train.shape[1], dtype=np.int32)\n",
    "        \n",
    "        predictions = np.empty((step,items_to_compute.shape[0]))\n",
    "        for item in items_to_compute:\n",
    "            predictions[:, item] = self.forward(\n",
    "                torch.tensor(user_id_array),\n",
    "                torch.tensor(\n",
    "                    np.ones(step, dtype=np.int32) * item)\n",
    "                ).cpu().detach().numpy().ravel()\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variant 2:\n",
    "# 1. Couple of Dense layers process user/item profiles\n",
    "# 2. Concatenate and final Dense layer to obtain prediciton\n",
    "class TwoTowerRecommender_type2(nn.Module, BaseRecommender):\n",
    "\n",
    "    RECOMMENDER_NAME = \"\"\"TWO_TOWER_2\"\"\"\n",
    "\n",
    "    def __init__(self, URM_train, num_users, num_items, layers=[10], reg_layers=[0], verbose = True):\n",
    "        global device\n",
    "        super().__init__()\n",
    "        BaseRecommender.__init__(self, URM_train, verbose=verbose)\n",
    "        self.device = device\n",
    "        layers[0] = int(layers[0]/2) # <- The first layer is split in two tower inputs at the beginning\n",
    "        self.mlp_embedding_user = nn.Embedding(num_users, layers[0], device=self.device)\n",
    "        self.mlp_embedding_item = nn.Embedding(num_items, layers[0], device=self.device) # <- It's possible to make the towers asymmetric! Mind the output dimension though\n",
    "\n",
    "        # layer_input_sizes_tower1 = layers\n",
    "        # layer_input_sizes_tower1[0] = num_items\n",
    "        self.mlp_layers_tower1 = nn.ModuleList([\n",
    "            nn.Linear(\n",
    "                layers[i-1],\n",
    "                layers[i], bias=True, device=self.device\n",
    "                ) for i in range(1, len(layers))\n",
    "            ])\n",
    "        \n",
    "        # layer_input_sizes_tower2 = layers\n",
    "        # layer_input_sizes_tower2[0] = num_users\n",
    "        self.mlp_layers_tower2 = nn.ModuleList([\n",
    "            nn.Linear(\n",
    "                layers[i-1],\n",
    "                layers[i], bias=True, device=self.device\n",
    "                ) for i in range(1, len(layers))\n",
    "            ])\n",
    "        \n",
    "        for i, layer in enumerate(self.mlp_layers_tower1):\n",
    "            nn.init.normal_(layer.weight)\n",
    "            layer.bias.data.zero_()\n",
    "            layer.weight_decay = reg_layers[i]\n",
    "\n",
    "        for i, layer in enumerate(self.mlp_layers_tower2):\n",
    "            nn.init.normal_(layer.weight)\n",
    "            layer.bias.data.zero_()\n",
    "            layer.weight_decay = reg_layers[i]\n",
    "\n",
    "        self.prediction_layer = nn.Linear(layers[-1], 1, bias=True, device=self.device)\n",
    "        nn.init.uniform_(self.prediction_layer.weight)\n",
    "        self.prediction_layer.bias.data.zero_()\n",
    "        self.to(self.device)\n",
    "\n",
    "    def _data_generator(self, batch_size):\n",
    "        # TODO: adjust sampling\n",
    "        row_idx = np.random.choice(self.URM_train.shape[0], batch_size, replace=False).astype(np.int32)\n",
    "        col_idx = np.random.choice(self.URM_train.shape[1], batch_size, replace=False).astype(np.int32)\n",
    "        user_input = torch.tensor(row_idx, device=self.device)\n",
    "        item_input = torch.tensor(col_idx, device=self.device)\n",
    "        labels = np.zeros(batch_size, dtype=np.int32)\n",
    "        for i in range(len(labels)):\n",
    "            if self.URM_train[row_idx[i], col_idx[i]] > 0:\n",
    "                labels[i] = 1\n",
    "        labels.shape = (batch_size, 1) # <- =1 for every user_item couple if an interaction exists, 0 otherwise\n",
    "        labels = torch.tensor(labels, device=self.device)\n",
    "        yield user_input, item_input, labels\n",
    "\n",
    "    def forward(self, user_input, item_input):\n",
    "        mlp_user_latent = self.mlp_embedding_user(user_input.long().to(self.device))\n",
    "        mlp_item_latent = self.mlp_embedding_item(item_input.long().to(self.device))\n",
    "\n",
    "        mlp_user_vector = mlp_user_latent\n",
    "        mlp_item_vector = mlp_item_latent\n",
    "\n",
    "        for layer in self.mlp_layers_tower1:\n",
    "            mlp_user_vector = torch.relu(layer(mlp_user_vector))\n",
    "\n",
    "        for layer in self.mlp_layers_tower2:\n",
    "            mlp_item_vector = torch.relu(layer(mlp_item_vector))\n",
    "\n",
    "        predict_vector = mlp_user_vector * mlp_item_vector # <- Merge the tensors via element-wise multiplication\n",
    "        prediction = torch.sigmoid(self.prediction_layer(predict_vector))\n",
    "        return prediction\n",
    "    \n",
    "    def fit(self, epochs=30, batch_size=1024, learning_rate=0.0001):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        for i in range(epochs):\n",
    "            for user_input, item_input, labels in self._data_generator(batch_size):\n",
    "                optimizer.zero_grad()\n",
    "                predictions = self.forward(user_input, item_input)\n",
    "                loss = torch.nn.BCELoss().to(self.device)\n",
    "                loss = loss(predictions, labels.float())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            self._print(\"Epoch {} finished. Loss: {}\".format(i, loss.item()))\n",
    "            torch.mps.empty_cache() # <- Input embeddings are pretty huge this time around...\n",
    "\n",
    "    def _compute_item_score(self, user_id_array, items_to_compute=None):\n",
    "        step = user_id_array.shape[0]\n",
    "        \n",
    "        if items_to_compute is None:\n",
    "            items_to_compute = np.arange(self.URM_train.shape[1], dtype=np.int32)\n",
    "        \n",
    "        predictions = np.empty((step,items_to_compute.shape[0]))\n",
    "        for item in items_to_compute:\n",
    "            predictions[:, item] = self.forward(\n",
    "                torch.tensor(user_id_array),\n",
    "                torch.tensor(\n",
    "                    np.ones(step, dtype=np.int32) * item)\n",
    "                ).cpu().detach().numpy().ravel()\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvaluatorHoldout: Ignoring 73 ( 0.1%) Users that have less than 1 test interactions\n",
      "EvaluatorHoldout: Ignoring 252 ( 0.4%) Users that have less than 1 test interactions\n"
     ]
    }
   ],
   "source": [
    "# Training and testing\n",
    "from Evaluation.Evaluator import EvaluatorHoldout\n",
    "\n",
    "evaluator_test = EvaluatorHoldout(URM_test, [10])\n",
    "evaluator_validation = EvaluatorHoldout(URM_val, [10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TWO_TOWER_1: URM Detected 50 ( 0.5%) items with no interactions.\n",
      "TWO_TOWER_1: Epoch 0 finished. Loss: 1.6019437313079834\n",
      "TWO_TOWER_1: Epoch 1 finished. Loss: 1.5645570755004883\n",
      "TWO_TOWER_1: Epoch 2 finished. Loss: 1.3969138860702515\n",
      "TWO_TOWER_1: Epoch 3 finished. Loss: 1.3517330884933472\n",
      "TWO_TOWER_1: Epoch 4 finished. Loss: 1.31689453125\n",
      "TWO_TOWER_1: Epoch 5 finished. Loss: 1.1823232173919678\n",
      "TWO_TOWER_1: Epoch 6 finished. Loss: 1.1204612255096436\n",
      "TWO_TOWER_1: Epoch 7 finished. Loss: 1.1037973165512085\n",
      "TWO_TOWER_1: Epoch 8 finished. Loss: 1.0747756958007812\n",
      "TWO_TOWER_1: Epoch 9 finished. Loss: 0.9497035145759583\n",
      "TWO_TOWER_1: Epoch 10 finished. Loss: 0.9518728256225586\n",
      "TWO_TOWER_1: Epoch 11 finished. Loss: 0.910079836845398\n",
      "TWO_TOWER_1: Epoch 12 finished. Loss: 0.8842508792877197\n",
      "TWO_TOWER_1: Epoch 13 finished. Loss: 0.8474003672599792\n",
      "TWO_TOWER_1: Epoch 14 finished. Loss: 0.8092023134231567\n",
      "TWO_TOWER_1: Epoch 15 finished. Loss: 0.7797098159790039\n",
      "TWO_TOWER_1: Epoch 16 finished. Loss: 0.7579502463340759\n",
      "TWO_TOWER_1: Epoch 17 finished. Loss: 0.7319083213806152\n",
      "TWO_TOWER_1: Epoch 18 finished. Loss: 0.7090221047401428\n",
      "TWO_TOWER_1: Epoch 19 finished. Loss: 0.6921154856681824\n",
      "TWO_TOWER_1: Epoch 20 finished. Loss: 0.673076868057251\n",
      "TWO_TOWER_1: Epoch 21 finished. Loss: 0.6707291603088379\n",
      "TWO_TOWER_1: Epoch 22 finished. Loss: 0.6569886803627014\n",
      "TWO_TOWER_1: Epoch 23 finished. Loss: 0.6446027755737305\n",
      "TWO_TOWER_1: Epoch 24 finished. Loss: 0.6414502859115601\n",
      "TWO_TOWER_1: Epoch 25 finished. Loss: 0.6228123903274536\n",
      "TWO_TOWER_1: Epoch 26 finished. Loss: 0.6093268394470215\n",
      "TWO_TOWER_1: Epoch 27 finished. Loss: 0.6034727692604065\n",
      "TWO_TOWER_1: Epoch 28 finished. Loss: 0.5935884118080139\n",
      "TWO_TOWER_1: Epoch 29 finished. Loss: 0.5921190977096558\n",
      "TWO_TOWER_1: Epoch 30 finished. Loss: 0.5835946798324585\n",
      "TWO_TOWER_1: Epoch 31 finished. Loss: 0.5793391466140747\n",
      "TWO_TOWER_1: Epoch 32 finished. Loss: 0.5706731677055359\n",
      "TWO_TOWER_1: Epoch 33 finished. Loss: 0.5678365230560303\n",
      "TWO_TOWER_1: Epoch 34 finished. Loss: 0.5632141828536987\n",
      "TWO_TOWER_1: Epoch 35 finished. Loss: 0.5568448305130005\n",
      "TWO_TOWER_1: Epoch 36 finished. Loss: 0.5531277656555176\n",
      "TWO_TOWER_1: Epoch 37 finished. Loss: 0.5473427176475525\n",
      "TWO_TOWER_1: Epoch 38 finished. Loss: 0.5422457456588745\n",
      "TWO_TOWER_1: Epoch 39 finished. Loss: 0.5384566783905029\n",
      "TWO_TOWER_1: Epoch 40 finished. Loss: 0.5326225757598877\n",
      "TWO_TOWER_1: Epoch 41 finished. Loss: 0.5285003781318665\n",
      "TWO_TOWER_1: Epoch 42 finished. Loss: 0.5256620645523071\n",
      "TWO_TOWER_1: Epoch 43 finished. Loss: 0.5193542242050171\n",
      "TWO_TOWER_1: Epoch 44 finished. Loss: 0.5157496333122253\n",
      "TWO_TOWER_1: Epoch 45 finished. Loss: 0.5163382887840271\n",
      "TWO_TOWER_1: Epoch 46 finished. Loss: 0.5103504061698914\n",
      "TWO_TOWER_1: Epoch 47 finished. Loss: 0.5079024434089661\n",
      "TWO_TOWER_1: Epoch 48 finished. Loss: 0.5034915208816528\n",
      "TWO_TOWER_1: Epoch 49 finished. Loss: 0.5023807287216187\n",
      "TWO_TOWER_1: Epoch 50 finished. Loss: 0.4964137077331543\n",
      "TWO_TOWER_1: Epoch 51 finished. Loss: 0.4938061833381653\n",
      "TWO_TOWER_1: Epoch 52 finished. Loss: 0.4914402961730957\n",
      "TWO_TOWER_1: Epoch 53 finished. Loss: 0.4862232506275177\n",
      "TWO_TOWER_1: Epoch 54 finished. Loss: 0.48283976316452026\n",
      "TWO_TOWER_1: Epoch 55 finished. Loss: 0.485706627368927\n",
      "TWO_TOWER_1: Epoch 56 finished. Loss: 0.4781632423400879\n",
      "TWO_TOWER_1: Epoch 57 finished. Loss: 0.47513580322265625\n",
      "TWO_TOWER_1: Epoch 58 finished. Loss: 0.47130638360977173\n",
      "TWO_TOWER_1: Epoch 59 finished. Loss: 0.46679532527923584\n",
      "TWO_TOWER_1: Epoch 60 finished. Loss: 0.46429216861724854\n",
      "TWO_TOWER_1: Epoch 61 finished. Loss: 0.46164071559906006\n",
      "TWO_TOWER_1: Epoch 62 finished. Loss: 0.4604805111885071\n",
      "TWO_TOWER_1: Epoch 63 finished. Loss: 0.45416444540023804\n",
      "TWO_TOWER_1: Epoch 64 finished. Loss: 0.4540511965751648\n",
      "TWO_TOWER_1: Epoch 65 finished. Loss: 0.45223671197891235\n",
      "TWO_TOWER_1: Epoch 66 finished. Loss: 0.44589754939079285\n",
      "TWO_TOWER_1: Epoch 67 finished. Loss: 0.4424055814743042\n",
      "TWO_TOWER_1: Epoch 68 finished. Loss: 0.44157177209854126\n",
      "TWO_TOWER_1: Epoch 69 finished. Loss: 0.44051241874694824\n",
      "TWO_TOWER_1: Epoch 70 finished. Loss: 0.4351111054420471\n",
      "TWO_TOWER_1: Epoch 71 finished. Loss: 0.4299408197402954\n",
      "TWO_TOWER_1: Epoch 72 finished. Loss: 0.4307214617729187\n",
      "TWO_TOWER_1: Epoch 73 finished. Loss: 0.42742204666137695\n",
      "TWO_TOWER_1: Epoch 74 finished. Loss: 0.4211254119873047\n",
      "TWO_TOWER_1: Epoch 75 finished. Loss: 0.4211832284927368\n",
      "TWO_TOWER_1: Epoch 76 finished. Loss: 0.4155677258968353\n",
      "TWO_TOWER_1: Epoch 77 finished. Loss: 0.41405442357063293\n",
      "TWO_TOWER_1: Epoch 78 finished. Loss: 0.4101334512233734\n",
      "TWO_TOWER_1: Epoch 79 finished. Loss: 0.40813565254211426\n",
      "TWO_TOWER_1: Epoch 80 finished. Loss: 0.4048250913619995\n",
      "TWO_TOWER_1: Epoch 81 finished. Loss: 0.40229517221450806\n",
      "TWO_TOWER_1: Epoch 82 finished. Loss: 0.3975834250450134\n",
      "TWO_TOWER_1: Epoch 83 finished. Loss: 0.3937385082244873\n",
      "TWO_TOWER_1: Epoch 84 finished. Loss: 0.39118021726608276\n",
      "TWO_TOWER_1: Epoch 85 finished. Loss: 0.38640427589416504\n",
      "TWO_TOWER_1: Epoch 86 finished. Loss: 0.37973377108573914\n",
      "TWO_TOWER_1: Epoch 87 finished. Loss: 0.37746524810791016\n",
      "TWO_TOWER_1: Epoch 88 finished. Loss: 0.37074798345565796\n",
      "TWO_TOWER_1: Epoch 89 finished. Loss: 0.3776569366455078\n",
      "TWO_TOWER_1: Epoch 90 finished. Loss: 0.3618605434894562\n",
      "TWO_TOWER_1: Epoch 91 finished. Loss: 0.3561146855354309\n",
      "TWO_TOWER_1: Epoch 92 finished. Loss: 0.348987877368927\n",
      "TWO_TOWER_1: Epoch 93 finished. Loss: 0.34547755122184753\n",
      "TWO_TOWER_1: Epoch 94 finished. Loss: 0.33795517683029175\n",
      "TWO_TOWER_1: Epoch 95 finished. Loss: 0.33255618810653687\n",
      "TWO_TOWER_1: Epoch 96 finished. Loss: 0.3214280903339386\n",
      "TWO_TOWER_1: Epoch 97 finished. Loss: 0.314333975315094\n",
      "TWO_TOWER_1: Epoch 98 finished. Loss: 0.30820444226264954\n",
      "TWO_TOWER_1: Epoch 99 finished. Loss: 0.286171555519104\n",
      "TWO_TOWER_1: Epoch 100 finished. Loss: 0.29165634512901306\n",
      "TWO_TOWER_1: Epoch 101 finished. Loss: 0.27326127886772156\n",
      "TWO_TOWER_1: Epoch 102 finished. Loss: 0.26310229301452637\n",
      "TWO_TOWER_1: Epoch 103 finished. Loss: 0.2500440776348114\n",
      "TWO_TOWER_1: Epoch 104 finished. Loss: 0.23634402453899384\n",
      "TWO_TOWER_1: Epoch 105 finished. Loss: 0.2253887951374054\n",
      "TWO_TOWER_1: Epoch 106 finished. Loss: 0.20261815190315247\n",
      "TWO_TOWER_1: Epoch 107 finished. Loss: 0.1963443160057068\n",
      "TWO_TOWER_1: Epoch 108 finished. Loss: 0.18356527388095856\n",
      "TWO_TOWER_1: Epoch 109 finished. Loss: 0.16671501100063324\n",
      "TWO_TOWER_1: Epoch 110 finished. Loss: 0.1544504314661026\n",
      "TWO_TOWER_1: Epoch 111 finished. Loss: 0.16014735400676727\n",
      "TWO_TOWER_1: Epoch 112 finished. Loss: 0.15955990552902222\n",
      "TWO_TOWER_1: Epoch 113 finished. Loss: 0.13381347060203552\n",
      "TWO_TOWER_1: Epoch 114 finished. Loss: 0.13185355067253113\n",
      "TWO_TOWER_1: Epoch 115 finished. Loss: 0.1256246715784073\n",
      "TWO_TOWER_1: Epoch 116 finished. Loss: 0.11585348099470139\n",
      "TWO_TOWER_1: Epoch 117 finished. Loss: 0.10736575722694397\n",
      "TWO_TOWER_1: Epoch 118 finished. Loss: 0.08788397908210754\n",
      "TWO_TOWER_1: Epoch 119 finished. Loss: 0.09347203373908997\n",
      "TWO_TOWER_1: Epoch 120 finished. Loss: 0.10826084017753601\n",
      "TWO_TOWER_1: Epoch 121 finished. Loss: 0.0907554179430008\n",
      "TWO_TOWER_1: Epoch 122 finished. Loss: 0.09584860503673553\n",
      "TWO_TOWER_1: Epoch 123 finished. Loss: 0.06317605078220367\n",
      "TWO_TOWER_1: Epoch 124 finished. Loss: 0.0828743577003479\n",
      "TWO_TOWER_1: Epoch 125 finished. Loss: 0.0711325854063034\n",
      "TWO_TOWER_1: Epoch 126 finished. Loss: 0.1329859048128128\n",
      "TWO_TOWER_1: Epoch 127 finished. Loss: 0.09537291526794434\n",
      "TWO_TOWER_1: Epoch 128 finished. Loss: 0.09301460534334183\n",
      "TWO_TOWER_1: Epoch 129 finished. Loss: 0.07246766239404678\n",
      "TWO_TOWER_1: Epoch 130 finished. Loss: 0.11221027374267578\n",
      "TWO_TOWER_1: Epoch 131 finished. Loss: 0.08705794811248779\n",
      "TWO_TOWER_1: Epoch 132 finished. Loss: 0.06466194987297058\n",
      "TWO_TOWER_1: Epoch 133 finished. Loss: 0.06341300904750824\n",
      "TWO_TOWER_1: Epoch 134 finished. Loss: 0.050528936088085175\n",
      "TWO_TOWER_1: Epoch 135 finished. Loss: 0.07760968804359436\n",
      "TWO_TOWER_1: Epoch 136 finished. Loss: 0.08845094591379166\n",
      "TWO_TOWER_1: Epoch 137 finished. Loss: 0.0639260858297348\n",
      "TWO_TOWER_1: Epoch 138 finished. Loss: 0.07912465929985046\n",
      "TWO_TOWER_1: Epoch 139 finished. Loss: 0.04822324216365814\n",
      "TWO_TOWER_1: Epoch 140 finished. Loss: 0.05977602303028107\n",
      "TWO_TOWER_1: Epoch 141 finished. Loss: 0.061337560415267944\n",
      "TWO_TOWER_1: Epoch 142 finished. Loss: 0.06560681760311127\n",
      "TWO_TOWER_1: Epoch 143 finished. Loss: 0.08644847571849823\n",
      "TWO_TOWER_1: Epoch 144 finished. Loss: 0.05595262348651886\n",
      "TWO_TOWER_1: Epoch 145 finished. Loss: 0.07482351362705231\n",
      "TWO_TOWER_1: Epoch 146 finished. Loss: 0.10830213129520416\n",
      "TWO_TOWER_1: Epoch 147 finished. Loss: 0.06396476924419403\n",
      "TWO_TOWER_1: Epoch 148 finished. Loss: 0.07061993330717087\n",
      "TWO_TOWER_1: Epoch 149 finished. Loss: 0.07979011535644531\n",
      "TWO_TOWER_1: Epoch 150 finished. Loss: 0.09590698778629303\n",
      "TWO_TOWER_1: Epoch 151 finished. Loss: 0.062150970101356506\n",
      "TWO_TOWER_1: Epoch 152 finished. Loss: 0.05407655984163284\n",
      "TWO_TOWER_1: Epoch 153 finished. Loss: 0.06706457585096359\n",
      "TWO_TOWER_1: Epoch 154 finished. Loss: 0.08919417858123779\n",
      "TWO_TOWER_1: Epoch 155 finished. Loss: 0.06966660916805267\n",
      "TWO_TOWER_1: Epoch 156 finished. Loss: 0.0609414279460907\n",
      "TWO_TOWER_1: Epoch 157 finished. Loss: 0.0827709287405014\n",
      "TWO_TOWER_1: Epoch 158 finished. Loss: 0.07153846323490143\n",
      "TWO_TOWER_1: Epoch 159 finished. Loss: 0.13342627882957458\n",
      "TWO_TOWER_1: Epoch 160 finished. Loss: 0.050740621984004974\n",
      "TWO_TOWER_1: Epoch 161 finished. Loss: 0.06169382482767105\n",
      "TWO_TOWER_1: Epoch 162 finished. Loss: 0.04163631424307823\n",
      "TWO_TOWER_1: Epoch 163 finished. Loss: 0.06865798681974411\n",
      "TWO_TOWER_1: Epoch 164 finished. Loss: 0.07889711856842041\n",
      "TWO_TOWER_1: Epoch 165 finished. Loss: 0.05352221429347992\n",
      "TWO_TOWER_1: Epoch 166 finished. Loss: 0.06517767161130905\n",
      "TWO_TOWER_1: Epoch 167 finished. Loss: 0.053105250000953674\n",
      "TWO_TOWER_1: Epoch 168 finished. Loss: 0.060266848653554916\n",
      "TWO_TOWER_1: Epoch 169 finished. Loss: 0.04585031047463417\n",
      "TWO_TOWER_1: Epoch 170 finished. Loss: 0.05384413152933121\n",
      "TWO_TOWER_1: Epoch 171 finished. Loss: 0.04916928708553314\n",
      "TWO_TOWER_1: Epoch 172 finished. Loss: 0.06247327849268913\n",
      "TWO_TOWER_1: Epoch 173 finished. Loss: 0.04879666492342949\n",
      "TWO_TOWER_1: Epoch 174 finished. Loss: 0.06650999933481216\n",
      "TWO_TOWER_1: Epoch 175 finished. Loss: 0.08687744289636612\n",
      "TWO_TOWER_1: Epoch 176 finished. Loss: 0.08052600175142288\n",
      "TWO_TOWER_1: Epoch 177 finished. Loss: 0.06914276629686356\n",
      "TWO_TOWER_1: Epoch 178 finished. Loss: 0.07926318794488907\n",
      "TWO_TOWER_1: Epoch 179 finished. Loss: 0.10388648509979248\n",
      "TWO_TOWER_1: Epoch 180 finished. Loss: 0.04377412050962448\n",
      "TWO_TOWER_1: Epoch 181 finished. Loss: 0.0328819639980793\n",
      "TWO_TOWER_1: Epoch 182 finished. Loss: 0.061302460730075836\n",
      "TWO_TOWER_1: Epoch 183 finished. Loss: 0.04740924760699272\n",
      "TWO_TOWER_1: Epoch 184 finished. Loss: 0.10795725136995316\n",
      "TWO_TOWER_1: Epoch 185 finished. Loss: 0.06873881816864014\n",
      "TWO_TOWER_1: Epoch 186 finished. Loss: 0.05873287469148636\n",
      "TWO_TOWER_1: Epoch 187 finished. Loss: 0.04378315806388855\n",
      "TWO_TOWER_1: Epoch 188 finished. Loss: 0.060848113149404526\n",
      "TWO_TOWER_1: Epoch 189 finished. Loss: 0.0688750296831131\n",
      "TWO_TOWER_1: Epoch 190 finished. Loss: 0.044254887849092484\n",
      "TWO_TOWER_1: Epoch 191 finished. Loss: 0.06088622286915779\n",
      "TWO_TOWER_1: Epoch 192 finished. Loss: 0.0467473529279232\n",
      "TWO_TOWER_1: Epoch 193 finished. Loss: 0.0766032412648201\n",
      "TWO_TOWER_1: Epoch 194 finished. Loss: 0.06806538999080658\n",
      "TWO_TOWER_1: Epoch 195 finished. Loss: 0.05140070617198944\n",
      "TWO_TOWER_1: Epoch 196 finished. Loss: 0.06445184350013733\n",
      "TWO_TOWER_1: Epoch 197 finished. Loss: 0.08223000168800354\n",
      "TWO_TOWER_1: Epoch 198 finished. Loss: 0.04446941241621971\n",
      "TWO_TOWER_1: Epoch 199 finished. Loss: 0.03851458057761192\n",
      "TWO_TOWER_1: Epoch 200 finished. Loss: 0.060946010053157806\n",
      "TWO_TOWER_1: Epoch 201 finished. Loss: 0.04392760619521141\n",
      "TWO_TOWER_1: Epoch 202 finished. Loss: 0.07125526666641235\n",
      "TWO_TOWER_1: Epoch 203 finished. Loss: 0.050040651112794876\n",
      "TWO_TOWER_1: Epoch 204 finished. Loss: 0.0703696757555008\n",
      "TWO_TOWER_1: Epoch 205 finished. Loss: 0.046422455459833145\n",
      "TWO_TOWER_1: Epoch 206 finished. Loss: 0.06938846409320831\n",
      "TWO_TOWER_1: Epoch 207 finished. Loss: 0.05496443435549736\n",
      "TWO_TOWER_1: Epoch 208 finished. Loss: 0.04316728562116623\n",
      "TWO_TOWER_1: Epoch 209 finished. Loss: 0.028257042169570923\n",
      "TWO_TOWER_1: Epoch 210 finished. Loss: 0.05946018546819687\n",
      "TWO_TOWER_1: Epoch 211 finished. Loss: 0.07675762474536896\n",
      "TWO_TOWER_1: Epoch 212 finished. Loss: 0.05556422844529152\n",
      "TWO_TOWER_1: Epoch 213 finished. Loss: 0.09758135676383972\n",
      "TWO_TOWER_1: Epoch 214 finished. Loss: 0.05510511249303818\n",
      "TWO_TOWER_1: Epoch 215 finished. Loss: 0.07601943612098694\n",
      "TWO_TOWER_1: Epoch 216 finished. Loss: 0.057121649384498596\n",
      "TWO_TOWER_1: Epoch 217 finished. Loss: 0.07646667957305908\n",
      "TWO_TOWER_1: Epoch 218 finished. Loss: 0.05659904330968857\n",
      "TWO_TOWER_1: Epoch 219 finished. Loss: 0.05309400334954262\n",
      "TWO_TOWER_1: Epoch 220 finished. Loss: 0.07090406119823456\n",
      "TWO_TOWER_1: Epoch 221 finished. Loss: 0.06145816296339035\n",
      "TWO_TOWER_1: Epoch 222 finished. Loss: 0.03782712668180466\n",
      "TWO_TOWER_1: Epoch 223 finished. Loss: 0.04724860191345215\n",
      "TWO_TOWER_1: Epoch 224 finished. Loss: 0.10095633566379547\n",
      "TWO_TOWER_1: Epoch 225 finished. Loss: 0.05495544150471687\n",
      "TWO_TOWER_1: Epoch 226 finished. Loss: 0.06977589428424835\n",
      "TWO_TOWER_1: Epoch 227 finished. Loss: 0.07678963243961334\n",
      "TWO_TOWER_1: Epoch 228 finished. Loss: 0.06763410568237305\n",
      "TWO_TOWER_1: Epoch 229 finished. Loss: 0.0678703784942627\n",
      "TWO_TOWER_1: Epoch 230 finished. Loss: 0.06359321624040604\n",
      "TWO_TOWER_1: Epoch 231 finished. Loss: 0.06890145689249039\n",
      "TWO_TOWER_1: Epoch 232 finished. Loss: 0.08143320679664612\n",
      "TWO_TOWER_1: Epoch 233 finished. Loss: 0.0772177129983902\n",
      "TWO_TOWER_1: Epoch 234 finished. Loss: 0.06601822376251221\n",
      "TWO_TOWER_1: Epoch 235 finished. Loss: 0.053945932537317276\n",
      "TWO_TOWER_1: Epoch 236 finished. Loss: 0.06057242676615715\n",
      "TWO_TOWER_1: Epoch 237 finished. Loss: 0.05604485049843788\n",
      "TWO_TOWER_1: Epoch 238 finished. Loss: 0.02918262965977192\n",
      "TWO_TOWER_1: Epoch 239 finished. Loss: 0.08256678283214569\n",
      "TWO_TOWER_1: Epoch 240 finished. Loss: 0.05419240891933441\n",
      "TWO_TOWER_1: Epoch 241 finished. Loss: 0.036906346678733826\n",
      "TWO_TOWER_1: Epoch 242 finished. Loss: 0.03684265911579132\n",
      "TWO_TOWER_1: Epoch 243 finished. Loss: 0.08689642697572708\n",
      "TWO_TOWER_1: Epoch 244 finished. Loss: 0.06108473986387253\n",
      "TWO_TOWER_1: Epoch 245 finished. Loss: 0.04018678516149521\n",
      "TWO_TOWER_1: Epoch 246 finished. Loss: 0.023455090820789337\n",
      "TWO_TOWER_1: Epoch 247 finished. Loss: 0.04486718773841858\n",
      "TWO_TOWER_1: Epoch 248 finished. Loss: 0.06035338342189789\n",
      "TWO_TOWER_1: Epoch 249 finished. Loss: 0.046647798269987106\n",
      "TWO_TOWER_1: Epoch 250 finished. Loss: 0.04125906899571419\n",
      "TWO_TOWER_1: Epoch 251 finished. Loss: 0.055324919521808624\n",
      "TWO_TOWER_1: Epoch 252 finished. Loss: 0.05739207565784454\n",
      "TWO_TOWER_1: Epoch 253 finished. Loss: 0.049749940633773804\n",
      "TWO_TOWER_1: Epoch 254 finished. Loss: 0.05264995992183685\n",
      "TWO_TOWER_1: Epoch 255 finished. Loss: 0.048782169818878174\n",
      "TWO_TOWER_1: Epoch 256 finished. Loss: 0.06378728151321411\n",
      "TWO_TOWER_1: Epoch 257 finished. Loss: 0.06992635130882263\n",
      "TWO_TOWER_1: Epoch 258 finished. Loss: 0.054486870765686035\n",
      "TWO_TOWER_1: Epoch 259 finished. Loss: 0.0368245467543602\n",
      "TWO_TOWER_1: Epoch 260 finished. Loss: 0.06788074970245361\n",
      "TWO_TOWER_1: Epoch 261 finished. Loss: 0.07285234332084656\n",
      "TWO_TOWER_1: Epoch 262 finished. Loss: 0.04282519966363907\n",
      "TWO_TOWER_1: Epoch 263 finished. Loss: 0.03958217427134514\n",
      "TWO_TOWER_1: Epoch 264 finished. Loss: 0.04064543545246124\n",
      "TWO_TOWER_1: Epoch 265 finished. Loss: 0.036983780562877655\n",
      "TWO_TOWER_1: Epoch 266 finished. Loss: 0.04413235932588577\n",
      "TWO_TOWER_1: Epoch 267 finished. Loss: 0.050064537674188614\n",
      "TWO_TOWER_1: Epoch 268 finished. Loss: 0.03308679535984993\n",
      "TWO_TOWER_1: Epoch 269 finished. Loss: 0.05486529320478439\n",
      "TWO_TOWER_1: Epoch 270 finished. Loss: 0.09648145735263824\n",
      "TWO_TOWER_1: Epoch 271 finished. Loss: 0.07982157170772552\n",
      "TWO_TOWER_1: Epoch 272 finished. Loss: 0.053863365203142166\n",
      "TWO_TOWER_1: Epoch 273 finished. Loss: 0.03888596594333649\n",
      "TWO_TOWER_1: Epoch 274 finished. Loss: 0.06095181405544281\n",
      "TWO_TOWER_1: Epoch 275 finished. Loss: 0.06252928078174591\n",
      "TWO_TOWER_1: Epoch 276 finished. Loss: 0.03584570810198784\n",
      "TWO_TOWER_1: Epoch 277 finished. Loss: 0.07444635033607483\n",
      "TWO_TOWER_1: Epoch 278 finished. Loss: 0.027444999665021896\n",
      "TWO_TOWER_1: Epoch 279 finished. Loss: 0.05789722129702568\n",
      "TWO_TOWER_1: Epoch 280 finished. Loss: 0.052353113889694214\n",
      "TWO_TOWER_1: Epoch 281 finished. Loss: 0.0640721544623375\n",
      "TWO_TOWER_1: Epoch 282 finished. Loss: 0.06150954216718674\n",
      "TWO_TOWER_1: Epoch 283 finished. Loss: 0.04065185412764549\n",
      "TWO_TOWER_1: Epoch 284 finished. Loss: 0.05414712056517601\n",
      "TWO_TOWER_1: Epoch 285 finished. Loss: 0.050032489001750946\n",
      "TWO_TOWER_1: Epoch 286 finished. Loss: 0.026469381526112556\n",
      "TWO_TOWER_1: Epoch 287 finished. Loss: 0.05710695683956146\n",
      "TWO_TOWER_1: Epoch 288 finished. Loss: 0.056867338716983795\n",
      "TWO_TOWER_1: Epoch 289 finished. Loss: 0.057716626673936844\n",
      "TWO_TOWER_1: Epoch 290 finished. Loss: 0.06060762703418732\n",
      "TWO_TOWER_1: Epoch 291 finished. Loss: 0.07146058231592178\n",
      "TWO_TOWER_1: Epoch 292 finished. Loss: 0.044768888503313065\n",
      "TWO_TOWER_1: Epoch 293 finished. Loss: 0.047288112342357635\n",
      "TWO_TOWER_1: Epoch 294 finished. Loss: 0.029407856985926628\n",
      "TWO_TOWER_1: Epoch 295 finished. Loss: 0.05963999032974243\n",
      "TWO_TOWER_1: Epoch 296 finished. Loss: 0.05694713070988655\n",
      "TWO_TOWER_1: Epoch 297 finished. Loss: 0.03869948536157608\n",
      "TWO_TOWER_1: Epoch 298 finished. Loss: 0.05999312549829483\n",
      "TWO_TOWER_1: Epoch 299 finished. Loss: 0.06908578425645828\n",
      "EvaluatorHoldout: Processed 34000 (48.7%) in 5.05 min. Users per second: 112\n",
      "EvaluatorHoldout: Processed 65000 (93.1%) in 10.17 min. Users per second: 107\n",
      "EvaluatorHoldout: Processed 69810 (100.0%) in 11.09 min. Users per second: 105\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PRECISION</th>\n",
       "      <th>PRECISION_RECALL_MIN_DEN</th>\n",
       "      <th>RECALL</th>\n",
       "      <th>MAP</th>\n",
       "      <th>MAP_MIN_DEN</th>\n",
       "      <th>MRR</th>\n",
       "      <th>NDCG</th>\n",
       "      <th>F1</th>\n",
       "      <th>HIT_RATE</th>\n",
       "      <th>ARHR_ALL_HITS</th>\n",
       "      <th>...</th>\n",
       "      <th>COVERAGE_USER</th>\n",
       "      <th>COVERAGE_USER_HIT</th>\n",
       "      <th>USERS_IN_GT</th>\n",
       "      <th>DIVERSITY_GINI</th>\n",
       "      <th>SHANNON_ENTROPY</th>\n",
       "      <th>RATIO_DIVERSITY_HERFINDAHL</th>\n",
       "      <th>RATIO_DIVERSITY_GINI</th>\n",
       "      <th>RATIO_SHANNON_ENTROPY</th>\n",
       "      <th>RATIO_AVERAGE_POPULARITY</th>\n",
       "      <th>RATIO_NOVELTY</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cutoff</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.005035</td>\n",
       "      <td>0.005326</td>\n",
       "      <td>0.001844</td>\n",
       "      <td>0.001481</td>\n",
       "      <td>0.001561</td>\n",
       "      <td>0.013827</td>\n",
       "      <td>0.002833</td>\n",
       "      <td>0.002699</td>\n",
       "      <td>0.047228</td>\n",
       "      <td>0.014309</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999027</td>\n",
       "      <td>0.047182</td>\n",
       "      <td>0.999027</td>\n",
       "      <td>0.028017</td>\n",
       "      <td>8.511292</td>\n",
       "      <td>0.995388</td>\n",
       "      <td>0.143894</td>\n",
       "      <td>0.75155</td>\n",
       "      <td>0.150539</td>\n",
       "      <td>0.144838</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       PRECISION PRECISION_RECALL_MIN_DEN    RECALL       MAP MAP_MIN_DEN  \\\n",
       "cutoff                                                                      \n",
       "10      0.005035                 0.005326  0.001844  0.001481    0.001561   \n",
       "\n",
       "             MRR      NDCG        F1  HIT_RATE ARHR_ALL_HITS  ...  \\\n",
       "cutoff                                                        ...   \n",
       "10      0.013827  0.002833  0.002699  0.047228      0.014309  ...   \n",
       "\n",
       "       COVERAGE_USER COVERAGE_USER_HIT USERS_IN_GT DIVERSITY_GINI  \\\n",
       "cutoff                                                              \n",
       "10          0.999027          0.047182    0.999027       0.028017   \n",
       "\n",
       "       SHANNON_ENTROPY RATIO_DIVERSITY_HERFINDAHL RATIO_DIVERSITY_GINI  \\\n",
       "cutoff                                                                   \n",
       "10            8.511292                   0.995388             0.143894   \n",
       "\n",
       "       RATIO_SHANNON_ENTROPY RATIO_AVERAGE_POPULARITY RATIO_NOVELTY  \n",
       "cutoff                                                               \n",
       "10                   0.75155                 0.150539      0.144838  \n",
       "\n",
       "[1 rows x 27 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train and test type 1\n",
    "twotower_1 = TwoTowerRecommender_type1(URM_train, URM_train.shape[0], URM_train.shape[1], layers=[10,5,2,2], reg_layers=[0,0,0,0])\n",
    "\n",
    "twotower_1.fit(epochs=300, batch_size=1024, learning_rate=0.01)\n",
    "\n",
    "results_df, _ = evaluator_test.evaluateRecommender(twotower_1)\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TWO_TOWER_2: URM Detected 64 ( 0.6%) items with no interactions.\n",
      "TWO_TOWER_2: Epoch 0 finished. Loss: 0.7134952545166016\n",
      "TWO_TOWER_2: Epoch 1 finished. Loss: 0.7043734788894653\n",
      "TWO_TOWER_2: Epoch 2 finished. Loss: 0.7000753879547119\n",
      "TWO_TOWER_2: Epoch 3 finished. Loss: 0.6923127174377441\n",
      "TWO_TOWER_2: Epoch 4 finished. Loss: 0.6904811859130859\n",
      "TWO_TOWER_2: Epoch 5 finished. Loss: 0.6877688765525818\n",
      "TWO_TOWER_2: Epoch 6 finished. Loss: 0.6841063499450684\n",
      "TWO_TOWER_2: Epoch 7 finished. Loss: 0.6899026036262512\n",
      "TWO_TOWER_2: Epoch 8 finished. Loss: 0.6785768866539001\n",
      "TWO_TOWER_2: Epoch 9 finished. Loss: 0.6749589443206787\n",
      "TWO_TOWER_2: Epoch 10 finished. Loss: 0.6726386547088623\n",
      "TWO_TOWER_2: Epoch 11 finished. Loss: 0.6690514087677002\n",
      "TWO_TOWER_2: Epoch 12 finished. Loss: 0.6679033041000366\n",
      "TWO_TOWER_2: Epoch 13 finished. Loss: 0.6649109125137329\n",
      "TWO_TOWER_2: Epoch 14 finished. Loss: 0.6612294912338257\n",
      "TWO_TOWER_2: Epoch 15 finished. Loss: 0.6584392786026001\n",
      "TWO_TOWER_2: Epoch 16 finished. Loss: 0.6563319563865662\n",
      "TWO_TOWER_2: Epoch 17 finished. Loss: 0.6536048650741577\n",
      "TWO_TOWER_2: Epoch 18 finished. Loss: 0.6516822576522827\n",
      "TWO_TOWER_2: Epoch 19 finished. Loss: 0.6485691070556641\n",
      "TWO_TOWER_2: Epoch 20 finished. Loss: 0.6455479860305786\n",
      "TWO_TOWER_2: Epoch 21 finished. Loss: 0.6433417797088623\n",
      "TWO_TOWER_2: Epoch 22 finished. Loss: 0.6422674655914307\n",
      "TWO_TOWER_2: Epoch 23 finished. Loss: 0.638909101486206\n",
      "TWO_TOWER_2: Epoch 24 finished. Loss: 0.6365035772323608\n",
      "TWO_TOWER_2: Epoch 25 finished. Loss: 0.6341733932495117\n",
      "TWO_TOWER_2: Epoch 26 finished. Loss: 0.6317551136016846\n",
      "TWO_TOWER_2: Epoch 27 finished. Loss: 0.6298935413360596\n",
      "TWO_TOWER_2: Epoch 28 finished. Loss: 0.6275430917739868\n",
      "TWO_TOWER_2: Epoch 29 finished. Loss: 0.6250976324081421\n",
      "TWO_TOWER_2: Epoch 30 finished. Loss: 0.6230564117431641\n",
      "TWO_TOWER_2: Epoch 31 finished. Loss: 0.6203457117080688\n",
      "TWO_TOWER_2: Epoch 32 finished. Loss: 0.6179828643798828\n",
      "TWO_TOWER_2: Epoch 33 finished. Loss: 0.6175510883331299\n",
      "TWO_TOWER_2: Epoch 34 finished. Loss: 0.6140402555465698\n",
      "TWO_TOWER_2: Epoch 35 finished. Loss: 0.6121993064880371\n",
      "TWO_TOWER_2: Epoch 36 finished. Loss: 0.609333872795105\n",
      "TWO_TOWER_2: Epoch 37 finished. Loss: 0.6075689196586609\n",
      "TWO_TOWER_2: Epoch 38 finished. Loss: 0.6052297353744507\n",
      "TWO_TOWER_2: Epoch 39 finished. Loss: 0.6037731766700745\n",
      "TWO_TOWER_2: Epoch 40 finished. Loss: 0.6005337238311768\n",
      "TWO_TOWER_2: Epoch 41 finished. Loss: 0.598514974117279\n",
      "TWO_TOWER_2: Epoch 42 finished. Loss: 0.5971418619155884\n",
      "TWO_TOWER_2: Epoch 43 finished. Loss: 0.5948245525360107\n",
      "TWO_TOWER_2: Epoch 44 finished. Loss: 0.5916788578033447\n",
      "TWO_TOWER_2: Epoch 45 finished. Loss: 0.5904269814491272\n",
      "TWO_TOWER_2: Epoch 46 finished. Loss: 0.5887988805770874\n",
      "TWO_TOWER_2: Epoch 47 finished. Loss: 0.5851060152053833\n",
      "TWO_TOWER_2: Epoch 48 finished. Loss: 0.5844943523406982\n",
      "TWO_TOWER_2: Epoch 49 finished. Loss: 0.583042323589325\n",
      "TWO_TOWER_2: Epoch 50 finished. Loss: 0.5801003575325012\n",
      "TWO_TOWER_2: Epoch 51 finished. Loss: 0.577556848526001\n",
      "TWO_TOWER_2: Epoch 52 finished. Loss: 0.5762785077095032\n",
      "TWO_TOWER_2: Epoch 53 finished. Loss: 0.5736712217330933\n",
      "TWO_TOWER_2: Epoch 54 finished. Loss: 0.5724462270736694\n",
      "TWO_TOWER_2: Epoch 55 finished. Loss: 0.5697765350341797\n",
      "TWO_TOWER_2: Epoch 56 finished. Loss: 0.5673409700393677\n",
      "TWO_TOWER_2: Epoch 57 finished. Loss: 0.566725492477417\n",
      "TWO_TOWER_2: Epoch 58 finished. Loss: 0.5633550882339478\n",
      "TWO_TOWER_2: Epoch 59 finished. Loss: 0.5624486207962036\n",
      "TWO_TOWER_2: Epoch 60 finished. Loss: 0.5593537092208862\n",
      "TWO_TOWER_2: Epoch 61 finished. Loss: 0.5579396486282349\n",
      "TWO_TOWER_2: Epoch 62 finished. Loss: 0.5545235872268677\n",
      "TWO_TOWER_2: Epoch 63 finished. Loss: 0.5543352365493774\n",
      "TWO_TOWER_2: Epoch 64 finished. Loss: 0.5522324442863464\n",
      "TWO_TOWER_2: Epoch 65 finished. Loss: 0.5510748624801636\n",
      "TWO_TOWER_2: Epoch 66 finished. Loss: 0.5485503673553467\n",
      "TWO_TOWER_2: Epoch 67 finished. Loss: 0.5472564697265625\n",
      "TWO_TOWER_2: Epoch 68 finished. Loss: 0.5456990599632263\n",
      "TWO_TOWER_2: Epoch 69 finished. Loss: 0.5445495843887329\n",
      "TWO_TOWER_2: Epoch 70 finished. Loss: 0.5422627329826355\n",
      "TWO_TOWER_2: Epoch 71 finished. Loss: 0.5377212762832642\n",
      "TWO_TOWER_2: Epoch 72 finished. Loss: 0.537872314453125\n",
      "TWO_TOWER_2: Epoch 73 finished. Loss: 0.5329619646072388\n",
      "TWO_TOWER_2: Epoch 74 finished. Loss: 0.5338503122329712\n",
      "TWO_TOWER_2: Epoch 75 finished. Loss: 0.5312749147415161\n",
      "TWO_TOWER_2: Epoch 76 finished. Loss: 0.5287160277366638\n",
      "TWO_TOWER_2: Epoch 77 finished. Loss: 0.5272179841995239\n",
      "TWO_TOWER_2: Epoch 78 finished. Loss: 0.5272316932678223\n",
      "TWO_TOWER_2: Epoch 79 finished. Loss: 0.5228134393692017\n",
      "TWO_TOWER_2: Epoch 80 finished. Loss: 0.5232837200164795\n",
      "TWO_TOWER_2: Epoch 81 finished. Loss: 0.5207356214523315\n",
      "TWO_TOWER_2: Epoch 82 finished. Loss: 0.520047664642334\n",
      "TWO_TOWER_2: Epoch 83 finished. Loss: 0.5183387994766235\n",
      "TWO_TOWER_2: Epoch 84 finished. Loss: 0.5153260827064514\n",
      "TWO_TOWER_2: Epoch 85 finished. Loss: 0.5127794742584229\n",
      "TWO_TOWER_2: Epoch 86 finished. Loss: 0.511003851890564\n",
      "TWO_TOWER_2: Epoch 87 finished. Loss: 0.5133005380630493\n",
      "TWO_TOWER_2: Epoch 88 finished. Loss: 0.5066403150558472\n",
      "TWO_TOWER_2: Epoch 89 finished. Loss: 0.504470944404602\n",
      "TWO_TOWER_2: Epoch 90 finished. Loss: 0.5073767900466919\n",
      "TWO_TOWER_2: Epoch 91 finished. Loss: 0.5026627779006958\n",
      "TWO_TOWER_2: Epoch 92 finished. Loss: 0.501008152961731\n",
      "TWO_TOWER_2: Epoch 93 finished. Loss: 0.5000936985015869\n",
      "TWO_TOWER_2: Epoch 94 finished. Loss: 0.4961993396282196\n",
      "TWO_TOWER_2: Epoch 95 finished. Loss: 0.4949899911880493\n",
      "TWO_TOWER_2: Epoch 96 finished. Loss: 0.4945911169052124\n",
      "TWO_TOWER_2: Epoch 97 finished. Loss: 0.49247145652770996\n",
      "TWO_TOWER_2: Epoch 98 finished. Loss: 0.4898335337638855\n",
      "TWO_TOWER_2: Epoch 99 finished. Loss: 0.49137136340141296\n",
      "EvaluatorHoldout: Processed 29000 (41.5%) in 5.15 min. Users per second: 94\n",
      "EvaluatorHoldout: Processed 58000 (83.1%) in 10.30 min. Users per second: 94\n",
      "EvaluatorHoldout: Processed 69805 (100.0%) in 12.40 min. Users per second: 94\n"
     ]
    }
   ],
   "source": [
    "# Train and test type 2\n",
    "twotower_2 = TwoTowerRecommender_type2(URM_train, URM_train.shape[0], URM_train.shape[1], layers=[10,5,2,2], reg_layers=[0,0,0,0])\n",
    "\n",
    "twotower_2.fit(epochs=100, batch_size=1024, learning_rate=0.005)\n",
    "\n",
    "results_df, _ = evaluator_test.evaluateRecommender(twotower_2)\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoEncoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Denoising Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Recommenders.MatrixFactorization.PyTorch import MF_MSE_PyTorch\n",
    "# ...\n",
    "# Autoencoder for recommendation, search on internet\n",
    "# Couple layers before and couple after bottleneck, basic noise (10 lines of code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the training structure using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $EASE^R$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only import, training and evaluation (from repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGCN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch implementation (see repo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possibly, GF-CF"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RSFramework",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
