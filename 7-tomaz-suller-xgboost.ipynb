{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to use XGBoost in RecSys\n",
    "* Run your best algorithm and select a number of recommendations higher than the target cutoff, for example if you have to compute MAP@10, get 20 recommendations\n",
    "* Build a dataframe whose samples are the user-item recommendations\n",
    "* Add for each interaction some content features: item features, user features\n",
    "* Add for each interaction some features derived by other algorithms: CBF prediction, hybrid prediction\n",
    "* Add for each interaction other miscellaneous information: profile length, item popularity .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import scipy.sparse as sps\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm.notebook import tqdm\n",
    "from xgboost import XGBRanker\n",
    "\n",
    "from Data_manager.competition import load, load_raw\n",
    "from Recommenders.BaseRecommender import BaseRecommender\n",
    "from Recommenders.Similarity.Compute_Similarity import Compute_Similarity\n",
    "from Recommenders.Hybrid import UserWideHybridRecommender, ScoresMultipleHybridRecommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_FOLDS = 10\n",
    "CUTOFF = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODELS_DIR = Path() / \"models\" / \"train\" / \"map\"\n",
    "MODELS_DIR.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34226</th>\n",
       "      <td>35729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34227</th>\n",
       "      <td>35730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34228</th>\n",
       "      <td>35731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34229</th>\n",
       "      <td>35734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34230</th>\n",
       "      <td>35735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34231 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_id\n",
       "0            0\n",
       "1            1\n",
       "2            2\n",
       "3            3\n",
       "4            4\n",
       "...        ...\n",
       "34226    35729\n",
       "34227    35730\n",
       "34228    35731\n",
       "34229    35734\n",
       "34230    35735\n",
       "\n",
       "[34231 rows x 1 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_ID_PATH = Path() / \"Data_manager_split_datasets\" / \"competition\" / \"data_target_users_test.csv\"\n",
    "test_df = pd.read_csv(TEST_ID_PATH)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBMISSION_PATH = Path(\"submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35736 38121\n"
     ]
    }
   ],
   "source": [
    "icm_df, urm_df = load_raw()\n",
    "number_users = urm_df[\"user_id\"].nunique()\n",
    "number_items = icm_df[\"item_id\"].nunique()\n",
    "print(number_users, number_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "icm_matrix, urm_all, urm_train, urm_validation, urm_test = load()\n",
    "ranker_urm = urm_validation + urm_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_user_dicts(user_dicts: list[dict]) -> dict:\n",
    "    return  {\n",
    "        user_dict[\"UserID\"]: user_dict[\"ItemID\"][0]\n",
    "        for user_dict in user_dicts\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def item_to_list() -> pl.Expr:\n",
    "    return pl.col(\"ItemID\").implode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_predictions(input_df: pd.DataFrame, drop_ids: bool = False) -> dict[int, list]:\n",
    "    output_df = input_df[[\"UserID\", \"ItemID\"]].copy()\n",
    "    if drop_ids:\n",
    "        input_df = input_df.drop(columns=[\"UserID\", \"ItemID\"])\n",
    "    output_df[\"Score\"] = XGB_model.predict(input_df)\n",
    "\n",
    "    output_dicts = (\n",
    "        pl.from_pandas(output_df)\n",
    "        .group_by(\"UserID\", \"ItemID\")\n",
    "        .agg(pl.mean(\"Score\"))\n",
    "        .sort(\"UserID\", \"Score\", descending=True)\n",
    "        .group_by(\"UserID\")\n",
    "        .head(10)\n",
    "        .group_by(\"UserID\")\n",
    "        .agg(item_to_list())\n",
    "        .to_dicts()\n",
    "    )\n",
    "    return encode_user_dicts(output_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Candidate generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_WIDE_HYBRID_BEGIN = 30\n",
    "MODELS_TO_USE = (\n",
    "    60,\n",
    "    61,\n",
    "    62,\n",
    "    63,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_GROUPS_USER_WIDE_HYBRID = 10\n",
    "MULTIPLE_SCORE_HYBRID_WEIGHTS = {\n",
    "    50: 0.253770701546336,\n",
    "    51: 0.10324855050317669,\n",
    "}\n",
    "\n",
    "\n",
    "def build_user_wide_hybrid(urm: sps.csr_matrix, models: dict[str, BaseRecommender]):\n",
    "    profile_lengths = np.ediff1d(urm.indptr)\n",
    "    sorted_users = np.argsort(profile_lengths)\n",
    "    block_size = len(sorted_users) // NUMBER_GROUPS_USER_WIDE_HYBRID\n",
    "    group_users = {}\n",
    "    for group in range(NUMBER_GROUPS_USER_WIDE_HYBRID + 1):\n",
    "        group_users[group] = sorted_users[group * block_size : (group + 1) * block_size]\n",
    "    group_recommenders = {\n",
    "        group: models.pop(str(USER_WIDE_HYBRID_BEGIN + group)) for group in range(NUMBER_GROUPS_USER_WIDE_HYBRID + 1)\n",
    "    }\n",
    "    return UserWideHybridRecommender(urm, group_users, group_recommenders)\n",
    "\n",
    "def build_score_hybrid(urm: sps.csr_matrix, models: dict[str, BaseRecommender]):\n",
    "    recommenders = [models.pop(str(index)) for index in MULTIPLE_SCORE_HYBRID_WEIGHTS.keys()]\n",
    "    weights = list(MULTIPLE_SCORE_HYBRID_WEIGHTS.values())\n",
    "    return ScoresMultipleHybridRecommender(urm, recommenders, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d8d787a9712422d91cd8c2875d987dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fold:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tomaz/git/Politecnico/Subjects/recommender-systems/recsys-competition/.venv/lib/python3.10/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator ElasticNet from version 1.6.0 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78d19a6d93fb4f2c8adda06927553c80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "User (candidate):   0%|          | 0/35736 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "314e33bdee12439fbd3d82454588f95d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "User (score):   0%|          | 0/35736 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tomaz/git/Politecnico/Subjects/recommender-systems/recsys-competition/.venv/lib/python3.10/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator ElasticNet from version 1.6.0 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56651a22c6464e8a82562e1c64618372",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "User (candidate):   0%|          | 0/35736 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b67c2f95c969455598ab37a017609db8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "User (score):   0%|          | 0/35736 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tomaz/git/Politecnico/Subjects/recommender-systems/recsys-competition/.venv/lib/python3.10/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator ElasticNet from version 1.6.0 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "990825a43f5340529bf6be37384033f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "User (candidate):   0%|          | 0/35736 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4daf419a196d4937b8aae2d0e982bb5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "User (score):   0%|          | 0/35736 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tomaz/git/Politecnico/Subjects/recommender-systems/recsys-competition/.venv/lib/python3.10/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator ElasticNet from version 1.6.0 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00a458106350437d959d265be0c04ca2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "User (candidate):   0%|          | 0/35736 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcd4944638724587b1ba28353176aadd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "User (score):   0%|          | 0/35736 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tomaz/git/Politecnico/Subjects/recommender-systems/recsys-competition/.venv/lib/python3.10/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator ElasticNet from version 1.6.0 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "415d572864e84c9c98a550b9d0083233",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "User (candidate):   0%|          | 0/35736 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49cdf4dbafc94f62b04ed3a32fcaff21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "User (score):   0%|          | 0/35736 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tomaz/git/Politecnico/Subjects/recommender-systems/recsys-competition/.venv/lib/python3.10/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator ElasticNet from version 1.6.0 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f4a1799538b48f492ff8283abb63a65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "User (candidate):   0%|          | 0/35736 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca2282770bf643a081c9ad67c476872a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "User (score):   0%|          | 0/35736 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tomaz/git/Politecnico/Subjects/recommender-systems/recsys-competition/.venv/lib/python3.10/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator ElasticNet from version 1.6.0 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31bb835aaa1245ffb42b46cccc0fa432",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "User (candidate):   0%|          | 0/35736 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "312646ac1a564fc1aee2c3f5dccf5fb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "User (score):   0%|          | 0/35736 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tomaz/git/Politecnico/Subjects/recommender-systems/recsys-competition/.venv/lib/python3.10/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator ElasticNet from version 1.6.0 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd89161fe1fe4413bb9aee373f38553a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "User (candidate):   0%|          | 0/35736 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5b3f9bd2530450fbc72fd7c089432c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "User (score):   0%|          | 0/35736 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tomaz/git/Politecnico/Subjects/recommender-systems/recsys-competition/.venv/lib/python3.10/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator ElasticNet from version 1.6.0 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "455f317c1af94fb39cb44c843ce6f442",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "User (candidate):   0%|          | 0/35736 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91adeb26b2934ea79cdcefc0b0dcadab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "User (score):   0%|          | 0/35736 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tomaz/git/Politecnico/Subjects/recommender-systems/recsys-competition/.venv/lib/python3.10/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator ElasticNet from version 1.6.0 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b0e82b24bc44d55b080ce9598b2ccb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "User (candidate):   0%|          | 0/35736 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8a2a1b49898443f9901b8d95bdb5328",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "User (score):   0%|          | 0/35736 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fold_training_dataframes: dict[int, pd.DataFrame] = {}\n",
    "for i, (train_indices, val_indices) in tqdm(\n",
    "    enumerate(KFold(NUMBER_FOLDS, shuffle=True, random_state=42).split(urm_df)),\n",
    "    total=NUMBER_FOLDS,\n",
    "    desc=\"Fold\",\n",
    "):\n",
    "    fold_urm_train_df = urm_df.iloc[train_indices]\n",
    "    fold_urm_train  = sps.csr_matrix(\n",
    "        (fold_urm_train_df.data, (fold_urm_train_df.user_id, fold_urm_train_df.item_id)),\n",
    "        shape=(number_users, number_items),\n",
    "    )\n",
    "\n",
    "    fold_models_dir = MODELS_DIR / str(i)\n",
    "    models: dict[str, BaseRecommender] = {\n",
    "        path.stem: pickle.load(path.open(\"rb\"))\n",
    "        for path in fold_models_dir.glob(\"*.pkl\")\n",
    "    }\n",
    "    if \"user_wide_hybrid\" not in models:\n",
    "        fold_user_wide_hybrid = build_user_wide_hybrid(fold_urm_train, models)\n",
    "    # if \"score_hybrid\" not in models:\n",
    "    #     fold_score_hybrid = build_score_hybrid(fold_urm_train, models)\n",
    "\n",
    "    models = {str(index): models[str(index)] for index in MODELS_TO_USE}\n",
    "    if \"user_wide_hybrid\" not in models:\n",
    "        models[\"user_wide_hybrid\"] = fold_user_wide_hybrid\n",
    "    # if \"score_hybrid\" not in models:\n",
    "    #     models[\"score_hybrid\"] = fold_score_hybrid\n",
    "\n",
    "    training_dataframe = pd.DataFrame(index=range(0, number_users), columns=[\"ItemID\"])\n",
    "    training_dataframe.index.name = \"UserID\"\n",
    "\n",
    "    recommendations_list = []\n",
    "    recommenders_list = []\n",
    "    rank_list = []\n",
    "    for user_id in tqdm(range(number_users), desc=\"User (candidate)\"):\n",
    "        user_recommendations = []\n",
    "        user_recommenders = []\n",
    "        user_rankings = []\n",
    "        for name, recommender in models.items():\n",
    "            user_recommendations.extend(\n",
    "                recommender.recommend(\n",
    "                    user_id,\n",
    "                    cutoff=CUTOFF,\n",
    "                    remove_seen_flag=True,\n",
    "                )\n",
    "            )\n",
    "            user_recommenders.extend([name] * CUTOFF)\n",
    "            user_rankings.extend(list(range(CUTOFF)))\n",
    "        recommendations_list.append(user_recommendations)\n",
    "        recommenders_list.append(user_recommenders)\n",
    "        rank_list.append(user_rankings)\n",
    "\n",
    "    training_dataframe[\"ItemID\"] = recommendations_list\n",
    "    training_dataframe[\"Recommender\"] = recommenders_list\n",
    "    training_dataframe[\"Ranking\"] = rank_list\n",
    "\n",
    "    exploded_recommender = training_dataframe[\"Recommender\"].explode()\n",
    "    exploded_ranking = training_dataframe[\"Ranking\"].explode()\n",
    "    training_dataframe = training_dataframe.explode(\"ItemID\")\n",
    "    training_dataframe[\"Recommender\"] = exploded_recommender\n",
    "    training_dataframe[\"Ranking\"] = exploded_ranking.astype(\"int\")\n",
    "\n",
    "    recommender_agreement = (\n",
    "        training_dataframe.reset_index()[[\"UserID\", \"ItemID\"]]\n",
    "        .groupby([\"UserID\", \"ItemID\"])\n",
    "        .value_counts()\n",
    "    )\n",
    "    training_dataframe[\"recommender_agreement\"] = recommender_agreement.loc[\n",
    "        list(zip(training_dataframe.index, training_dataframe[\"ItemID\"]))\n",
    "    ].to_numpy()\n",
    "\n",
    "    fold_urm_val_df = urm_df.iloc[val_indices]\n",
    "    fold_urm_val  = sps.csr_matrix(\n",
    "        (fold_urm_val_df.data, (fold_urm_val_df.user_id, fold_urm_val_df.item_id)),\n",
    "        shape=(number_users, number_items),\n",
    "    )\n",
    "    fold_urm_coo = sps.coo_matrix(fold_urm_val)\n",
    "    correct_recommendations = pd.DataFrame(\n",
    "        {\"UserID\": fold_urm_coo.row, \"ItemID\": fold_urm_coo.col}\n",
    "    )\n",
    "    training_dataframe = training_dataframe.merge(\n",
    "        correct_recommendations,\n",
    "        on=[\"UserID\", \"ItemID\"],\n",
    "        how=\"left\",\n",
    "        indicator=\"Exist\",\n",
    "    )\n",
    "    training_dataframe[\"Label\"] = training_dataframe[\"Exist\"] == \"both\"\n",
    "    training_dataframe = training_dataframe.drop(columns=[\"Exist\"])\n",
    "\n",
    "    training_dataframe = training_dataframe.set_index(\"UserID\")\n",
    "    for user_id in tqdm(training_dataframe.index.unique(), desc=\"User (score)\"):\n",
    "        for rec_label, rec_instance in models.items():\n",
    "            item_list = training_dataframe.loc[user_id, \"ItemID\"].to_list()\n",
    "\n",
    "            all_item_scores = rec_instance._compute_item_score(\n",
    "                [user_id], items_to_compute=item_list\n",
    "            )\n",
    "\n",
    "            training_dataframe.loc[user_id, rec_label] = all_item_scores[0, item_list]\n",
    "\n",
    "    training_dataframe = training_dataframe.reset_index()\n",
    "    training_dataframe = training_dataframe.rename(columns={\"index\": \"UserID\"})\n",
    "\n",
    "    training_dataframe[\"fold\"] = i\n",
    "    fold_training_dataframes[i] = training_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Recommender</th>\n",
       "      <th>Ranking</th>\n",
       "      <th>recommender_agreement</th>\n",
       "      <th>Label</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>user_wide_hybrid</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>7703</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>0.268031</td>\n",
       "      <td>0.401745</td>\n",
       "      <td>2.693894</td>\n",
       "      <td>0.085774</td>\n",
       "      <td>0.272271</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>7547</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>0.258316</td>\n",
       "      <td>0.352410</td>\n",
       "      <td>2.404292</td>\n",
       "      <td>0.066998</td>\n",
       "      <td>0.265295</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>6822</td>\n",
       "      <td>60</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>0.237978</td>\n",
       "      <td>0.143136</td>\n",
       "      <td>0.850208</td>\n",
       "      <td>0.182649</td>\n",
       "      <td>0.221734</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>572</td>\n",
       "      <td>60</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>0.234505</td>\n",
       "      <td>0.300062</td>\n",
       "      <td>1.903718</td>\n",
       "      <td>0.160482</td>\n",
       "      <td>0.222038</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3077</td>\n",
       "      <td>60</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>0.234102</td>\n",
       "      <td>0.241273</td>\n",
       "      <td>1.361896</td>\n",
       "      <td>0.182695</td>\n",
       "      <td>0.232855</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1786795</th>\n",
       "      <td>35735</td>\n",
       "      <td>37445</td>\n",
       "      <td>user_wide_hybrid</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>0.343499</td>\n",
       "      <td>0.630422</td>\n",
       "      <td>4.550226</td>\n",
       "      <td>0.199559</td>\n",
       "      <td>0.353038</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1786796</th>\n",
       "      <td>35735</td>\n",
       "      <td>37507</td>\n",
       "      <td>user_wide_hybrid</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>0.313335</td>\n",
       "      <td>0.395291</td>\n",
       "      <td>1.350077</td>\n",
       "      <td>0.171390</td>\n",
       "      <td>0.333600</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1786797</th>\n",
       "      <td>35735</td>\n",
       "      <td>36775</td>\n",
       "      <td>user_wide_hybrid</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>0.326151</td>\n",
       "      <td>0.581158</td>\n",
       "      <td>3.998512</td>\n",
       "      <td>0.191833</td>\n",
       "      <td>0.327440</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1786798</th>\n",
       "      <td>35735</td>\n",
       "      <td>34998</td>\n",
       "      <td>user_wide_hybrid</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>0.326401</td>\n",
       "      <td>0.430699</td>\n",
       "      <td>2.925698</td>\n",
       "      <td>0.127431</td>\n",
       "      <td>0.326283</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1786799</th>\n",
       "      <td>35735</td>\n",
       "      <td>37801</td>\n",
       "      <td>user_wide_hybrid</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>0.304025</td>\n",
       "      <td>0.662617</td>\n",
       "      <td>4.754726</td>\n",
       "      <td>0.184366</td>\n",
       "      <td>0.299684</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17868000 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         UserID ItemID       Recommender  Ranking  recommender_agreement  \\\n",
       "0             0   7703                60        0                      4   \n",
       "1             0   7547                60        1                      4   \n",
       "2             0   6822                60        2                      3   \n",
       "3             0    572                60        3                      5   \n",
       "4             0   3077                60        4                      4   \n",
       "...         ...    ...               ...      ...                    ...   \n",
       "1786795   35735  37445  user_wide_hybrid        5                      5   \n",
       "1786796   35735  37507  user_wide_hybrid        6                      2   \n",
       "1786797   35735  36775  user_wide_hybrid        7                      5   \n",
       "1786798   35735  34998  user_wide_hybrid        8                      2   \n",
       "1786799   35735  37801  user_wide_hybrid        9                      5   \n",
       "\n",
       "         Label        60        61        62        63  user_wide_hybrid  fold  \n",
       "0        False  0.268031  0.401745  2.693894  0.085774          0.272271     0  \n",
       "1        False  0.258316  0.352410  2.404292  0.066998          0.265295     0  \n",
       "2         True  0.237978  0.143136  0.850208  0.182649          0.221734     0  \n",
       "3        False  0.234505  0.300062  1.903718  0.160482          0.222038     0  \n",
       "4         True  0.234102  0.241273  1.361896  0.182695          0.232855     0  \n",
       "...        ...       ...       ...       ...       ...               ...   ...  \n",
       "1786795  False  0.343499  0.630422  4.550226  0.199559          0.353038     9  \n",
       "1786796  False  0.313335  0.395291  1.350077  0.171390          0.333600     9  \n",
       "1786797  False  0.326151  0.581158  3.998512  0.191833          0.327440     9  \n",
       "1786798  False  0.326401  0.430699  2.925698  0.127431          0.326283     9  \n",
       "1786799   True  0.304025  0.662617  4.754726  0.184366          0.299684     9  \n",
       "\n",
       "[17868000 rows x 12 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataframe = pd.concat(fold_training_dataframes.values())\n",
    "training_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee6b46d3b84a4c7ab2627ec1f2c7ddf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "User (candidate):   0%|          | 0/35736 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0132fe01d52d482589dfabc2c312924c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "User (score):   0%|          | 0/35736 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Recommender</th>\n",
       "      <th>Ranking</th>\n",
       "      <th>recommender_agreement</th>\n",
       "      <th>23</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>20</th>\n",
       "      <th>user_wide_hybrid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>572</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.708066</td>\n",
       "      <td>0.364245</td>\n",
       "      <td>0.184505</td>\n",
       "      <td>0.906074</td>\n",
       "      <td>0.288273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>14888</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3.389925</td>\n",
       "      <td>0.282408</td>\n",
       "      <td>0.327246</td>\n",
       "      <td>1.195710</td>\n",
       "      <td>0.312018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>452</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3.183915</td>\n",
       "      <td>0.180780</td>\n",
       "      <td>0.072435</td>\n",
       "      <td>0.279713</td>\n",
       "      <td>0.242547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>9911</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2.940626</td>\n",
       "      <td>0.226980</td>\n",
       "      <td>0.315740</td>\n",
       "      <td>1.171455</td>\n",
       "      <td>0.176901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>14931</td>\n",
       "      <td>23</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2.921493</td>\n",
       "      <td>0.238371</td>\n",
       "      <td>0.189480</td>\n",
       "      <td>0.695994</td>\n",
       "      <td>0.246573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1786795</th>\n",
       "      <td>35735</td>\n",
       "      <td>36775</td>\n",
       "      <td>user_wide_hybrid</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>8.471791</td>\n",
       "      <td>0.328802</td>\n",
       "      <td>0.358924</td>\n",
       "      <td>2.157352</td>\n",
       "      <td>0.664134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1786796</th>\n",
       "      <td>35735</td>\n",
       "      <td>37660</td>\n",
       "      <td>user_wide_hybrid</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>7.323517</td>\n",
       "      <td>0.347563</td>\n",
       "      <td>0.192113</td>\n",
       "      <td>1.368214</td>\n",
       "      <td>0.588393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1786797</th>\n",
       "      <td>35735</td>\n",
       "      <td>36920</td>\n",
       "      <td>user_wide_hybrid</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>7.601600</td>\n",
       "      <td>0.342447</td>\n",
       "      <td>0.263019</td>\n",
       "      <td>1.772518</td>\n",
       "      <td>0.581799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1786798</th>\n",
       "      <td>35735</td>\n",
       "      <td>37017</td>\n",
       "      <td>user_wide_hybrid</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>4.652850</td>\n",
       "      <td>0.303232</td>\n",
       "      <td>0.303558</td>\n",
       "      <td>2.394336</td>\n",
       "      <td>0.573921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1786799</th>\n",
       "      <td>35735</td>\n",
       "      <td>35753</td>\n",
       "      <td>user_wide_hybrid</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>5.464930</td>\n",
       "      <td>0.300776</td>\n",
       "      <td>0.086924</td>\n",
       "      <td>2.287900</td>\n",
       "      <td>0.509385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1786800 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         UserID ItemID       Recommender  Ranking  recommender_agreement  \\\n",
       "0             0    572                23        0                      4   \n",
       "1             0  14888                23        1                      4   \n",
       "2             0    452                23        2                      1   \n",
       "3             0   9911                23        3                      2   \n",
       "4             0  14931                23        4                      1   \n",
       "...         ...    ...               ...      ...                    ...   \n",
       "1786795   35735  36775  user_wide_hybrid        5                      3   \n",
       "1786796   35735  37660  user_wide_hybrid        6                      3   \n",
       "1786797   35735  36920  user_wide_hybrid        7                      3   \n",
       "1786798   35735  37017  user_wide_hybrid        8                      2   \n",
       "1786799   35735  35753  user_wide_hybrid        9                      2   \n",
       "\n",
       "               23        21        22        20  user_wide_hybrid  \n",
       "0        3.708066  0.364245  0.184505  0.906074          0.288273  \n",
       "1        3.389925  0.282408  0.327246  1.195710          0.312018  \n",
       "2        3.183915  0.180780  0.072435  0.279713          0.242547  \n",
       "3        2.940626  0.226980  0.315740  1.171455          0.176901  \n",
       "4        2.921493  0.238371  0.189480  0.695994          0.246573  \n",
       "...           ...       ...       ...       ...               ...  \n",
       "1786795  8.471791  0.328802  0.358924  2.157352          0.664134  \n",
       "1786796  7.323517  0.347563  0.192113  1.368214          0.588393  \n",
       "1786797  7.601600  0.342447  0.263019  1.772518          0.581799  \n",
       "1786798  4.652850  0.303232  0.303558  2.394336          0.573921  \n",
       "1786799  5.464930  0.300776  0.086924  2.287900          0.509385  \n",
       "\n",
       "[1786800 rows x 10 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit_models_dir = Path() / \"models\" / \"all\" / \"0\"\n",
    "models: dict[str, BaseRecommender] = {\n",
    "    path.stem: pickle.load(path.open(\"rb\"))\n",
    "    for path in submit_models_dir.glob(\"*.pkl\")\n",
    "}\n",
    "\n",
    "submission_dataframe = pd.DataFrame(index=range(0, number_users), columns=[\"ItemID\"])\n",
    "submission_dataframe.index.name = \"UserID\"\n",
    "\n",
    "recommendations_list = []\n",
    "recommenders_list = []\n",
    "rank_list = []\n",
    "for user_id in tqdm(range(number_users), desc=\"User (candidate)\"):\n",
    "    user_recommendations = []\n",
    "    user_recommenders = []\n",
    "    user_rankings = []\n",
    "    for name, recommender in models.items():\n",
    "        user_recommendations.extend(\n",
    "            recommender.recommend(\n",
    "                user_id,\n",
    "                cutoff=CUTOFF,\n",
    "                remove_seen_flag=True,\n",
    "            )\n",
    "        )\n",
    "        user_recommenders.extend([name] * CUTOFF)\n",
    "        user_rankings.extend(list(range(CUTOFF)))\n",
    "    recommendations_list.append(user_recommendations)\n",
    "    recommenders_list.append(user_recommenders)\n",
    "    rank_list.append(user_rankings)\n",
    "\n",
    "submission_dataframe[\"ItemID\"] = recommendations_list\n",
    "submission_dataframe[\"Recommender\"] = recommenders_list\n",
    "submission_dataframe[\"Ranking\"] = rank_list\n",
    "\n",
    "exploded_recommender = submission_dataframe[\"Recommender\"].explode()\n",
    "exploded_ranking = submission_dataframe[\"Ranking\"].explode()\n",
    "submission_dataframe = submission_dataframe.explode(\"ItemID\")\n",
    "submission_dataframe[\"Recommender\"] = exploded_recommender\n",
    "submission_dataframe[\"Ranking\"] = exploded_ranking.astype(\"int\")\n",
    "\n",
    "recommender_agreement = (\n",
    "    submission_dataframe.reset_index()[[\"UserID\", \"ItemID\"]]\n",
    "    .groupby([\"UserID\", \"ItemID\"])\n",
    "    .value_counts()\n",
    ")\n",
    "submission_dataframe[\"recommender_agreement\"] = recommender_agreement.loc[\n",
    "    list(zip(submission_dataframe.index, submission_dataframe[\"ItemID\"]))\n",
    "].to_numpy()\n",
    "\n",
    "\n",
    "for user_id in tqdm(submission_dataframe.index.unique(), desc=\"User (score)\"):\n",
    "    for rec_label, rec_instance in models.items():\n",
    "        item_list = submission_dataframe.loc[user_id, \"ItemID\"].to_list()\n",
    "\n",
    "        all_item_scores = rec_instance._compute_item_score(\n",
    "            [user_id], items_to_compute=item_list\n",
    "        )\n",
    "\n",
    "        submission_dataframe.loc[user_id, rec_label] = all_item_scores[0, item_list]\n",
    "\n",
    "submission_dataframe = submission_dataframe.reset_index()\n",
    "submission_dataframe = submission_dataframe.rename(columns={\"index\": \"UserID\"})\n",
    "submission_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Recommender</th>\n",
       "      <th>Ranking</th>\n",
       "      <th>recommender_agreement</th>\n",
       "      <th>23</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>20</th>\n",
       "      <th>user_wide_hybrid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>572</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.708066</td>\n",
       "      <td>0.364245</td>\n",
       "      <td>0.184505</td>\n",
       "      <td>0.906074</td>\n",
       "      <td>0.288273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>14888</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3.389925</td>\n",
       "      <td>0.282408</td>\n",
       "      <td>0.327246</td>\n",
       "      <td>1.195710</td>\n",
       "      <td>0.312018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>452</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3.183915</td>\n",
       "      <td>0.180780</td>\n",
       "      <td>0.072435</td>\n",
       "      <td>0.279713</td>\n",
       "      <td>0.242547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>9911</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2.940626</td>\n",
       "      <td>0.226980</td>\n",
       "      <td>0.315740</td>\n",
       "      <td>1.171455</td>\n",
       "      <td>0.176901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>14931</td>\n",
       "      <td>23</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2.921493</td>\n",
       "      <td>0.238371</td>\n",
       "      <td>0.189480</td>\n",
       "      <td>0.695994</td>\n",
       "      <td>0.246573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1786795</th>\n",
       "      <td>35735</td>\n",
       "      <td>36775</td>\n",
       "      <td>user_wide_hybrid</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>8.471791</td>\n",
       "      <td>0.328802</td>\n",
       "      <td>0.358924</td>\n",
       "      <td>2.157352</td>\n",
       "      <td>0.664134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1786796</th>\n",
       "      <td>35735</td>\n",
       "      <td>37660</td>\n",
       "      <td>user_wide_hybrid</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>7.323517</td>\n",
       "      <td>0.347563</td>\n",
       "      <td>0.192113</td>\n",
       "      <td>1.368214</td>\n",
       "      <td>0.588393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1786797</th>\n",
       "      <td>35735</td>\n",
       "      <td>36920</td>\n",
       "      <td>user_wide_hybrid</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>7.601600</td>\n",
       "      <td>0.342447</td>\n",
       "      <td>0.263019</td>\n",
       "      <td>1.772518</td>\n",
       "      <td>0.581799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1786798</th>\n",
       "      <td>35735</td>\n",
       "      <td>37017</td>\n",
       "      <td>user_wide_hybrid</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>4.652850</td>\n",
       "      <td>0.303232</td>\n",
       "      <td>0.303558</td>\n",
       "      <td>2.394336</td>\n",
       "      <td>0.573921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1786799</th>\n",
       "      <td>35735</td>\n",
       "      <td>35753</td>\n",
       "      <td>user_wide_hybrid</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>5.464930</td>\n",
       "      <td>0.300776</td>\n",
       "      <td>0.086924</td>\n",
       "      <td>2.287900</td>\n",
       "      <td>0.509385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1786800 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         UserID ItemID       Recommender  Ranking  recommender_agreement  \\\n",
       "0             0    572                23        0                      4   \n",
       "1             0  14888                23        1                      4   \n",
       "2             0    452                23        2                      1   \n",
       "3             0   9911                23        3                      2   \n",
       "4             0  14931                23        4                      1   \n",
       "...         ...    ...               ...      ...                    ...   \n",
       "1786795   35735  36775  user_wide_hybrid        5                      3   \n",
       "1786796   35735  37660  user_wide_hybrid        6                      3   \n",
       "1786797   35735  36920  user_wide_hybrid        7                      3   \n",
       "1786798   35735  37017  user_wide_hybrid        8                      2   \n",
       "1786799   35735  35753  user_wide_hybrid        9                      2   \n",
       "\n",
       "               23        21        22        20  user_wide_hybrid  \n",
       "0        3.708066  0.364245  0.184505  0.906074          0.288273  \n",
       "1        3.389925  0.282408  0.327246  1.195710          0.312018  \n",
       "2        3.183915  0.180780  0.072435  0.279713          0.242547  \n",
       "3        2.940626  0.226980  0.315740  1.171455          0.176901  \n",
       "4        2.921493  0.238371  0.189480  0.695994          0.246573  \n",
       "...           ...       ...       ...       ...               ...  \n",
       "1786795  8.471791  0.328802  0.358924  2.157352          0.664134  \n",
       "1786796  7.323517  0.347563  0.192113  1.368214          0.588393  \n",
       "1786797  7.601600  0.342447  0.263019  1.772518          0.581799  \n",
       "1786798  4.652850  0.303232  0.303558  2.394336          0.573921  \n",
       "1786799  5.464930  0.300776  0.086924  2.287900          0.509385  \n",
       "\n",
       "[1786800 rows x 10 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataframe = submission_dataframe\n",
    "training_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Item popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_popularity = np.ediff1d(sps.csc_matrix(urm_all).indptr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Recommender</th>\n",
       "      <th>Ranking</th>\n",
       "      <th>recommender_agreement</th>\n",
       "      <th>Label</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>user_wide_hybrid</th>\n",
       "      <th>fold</th>\n",
       "      <th>item_popularity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>7703</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>0.268031</td>\n",
       "      <td>0.401745</td>\n",
       "      <td>2.693894</td>\n",
       "      <td>0.085774</td>\n",
       "      <td>0.272271</td>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>7547</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>0.258316</td>\n",
       "      <td>0.352410</td>\n",
       "      <td>2.404292</td>\n",
       "      <td>0.066998</td>\n",
       "      <td>0.265295</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>6822</td>\n",
       "      <td>60</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>0.237978</td>\n",
       "      <td>0.143136</td>\n",
       "      <td>0.850208</td>\n",
       "      <td>0.182649</td>\n",
       "      <td>0.221734</td>\n",
       "      <td>0</td>\n",
       "      <td>203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>572</td>\n",
       "      <td>60</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>0.234505</td>\n",
       "      <td>0.300062</td>\n",
       "      <td>1.903718</td>\n",
       "      <td>0.160482</td>\n",
       "      <td>0.222038</td>\n",
       "      <td>0</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3077</td>\n",
       "      <td>60</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>0.234102</td>\n",
       "      <td>0.241273</td>\n",
       "      <td>1.361896</td>\n",
       "      <td>0.182695</td>\n",
       "      <td>0.232855</td>\n",
       "      <td>0</td>\n",
       "      <td>259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1786795</th>\n",
       "      <td>35735</td>\n",
       "      <td>37445</td>\n",
       "      <td>user_wide_hybrid</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>0.343499</td>\n",
       "      <td>0.630422</td>\n",
       "      <td>4.550226</td>\n",
       "      <td>0.199559</td>\n",
       "      <td>0.353038</td>\n",
       "      <td>9</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1786796</th>\n",
       "      <td>35735</td>\n",
       "      <td>37507</td>\n",
       "      <td>user_wide_hybrid</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>0.313335</td>\n",
       "      <td>0.395291</td>\n",
       "      <td>1.350077</td>\n",
       "      <td>0.171390</td>\n",
       "      <td>0.333600</td>\n",
       "      <td>9</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1786797</th>\n",
       "      <td>35735</td>\n",
       "      <td>36775</td>\n",
       "      <td>user_wide_hybrid</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>0.326151</td>\n",
       "      <td>0.581158</td>\n",
       "      <td>3.998512</td>\n",
       "      <td>0.191833</td>\n",
       "      <td>0.327440</td>\n",
       "      <td>9</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1786798</th>\n",
       "      <td>35735</td>\n",
       "      <td>34998</td>\n",
       "      <td>user_wide_hybrid</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>0.326401</td>\n",
       "      <td>0.430699</td>\n",
       "      <td>2.925698</td>\n",
       "      <td>0.127431</td>\n",
       "      <td>0.326283</td>\n",
       "      <td>9</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1786799</th>\n",
       "      <td>35735</td>\n",
       "      <td>37801</td>\n",
       "      <td>user_wide_hybrid</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>0.304025</td>\n",
       "      <td>0.662617</td>\n",
       "      <td>4.754726</td>\n",
       "      <td>0.184366</td>\n",
       "      <td>0.299684</td>\n",
       "      <td>9</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17868000 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         UserID ItemID       Recommender  Ranking  recommender_agreement  \\\n",
       "0             0   7703                60        0                      4   \n",
       "1             0   7547                60        1                      4   \n",
       "2             0   6822                60        2                      3   \n",
       "3             0    572                60        3                      5   \n",
       "4             0   3077                60        4                      4   \n",
       "...         ...    ...               ...      ...                    ...   \n",
       "1786795   35735  37445  user_wide_hybrid        5                      5   \n",
       "1786796   35735  37507  user_wide_hybrid        6                      2   \n",
       "1786797   35735  36775  user_wide_hybrid        7                      5   \n",
       "1786798   35735  34998  user_wide_hybrid        8                      2   \n",
       "1786799   35735  37801  user_wide_hybrid        9                      5   \n",
       "\n",
       "         Label        60        61        62        63  user_wide_hybrid  \\\n",
       "0        False  0.268031  0.401745  2.693894  0.085774          0.272271   \n",
       "1        False  0.258316  0.352410  2.404292  0.066998          0.265295   \n",
       "2         True  0.237978  0.143136  0.850208  0.182649          0.221734   \n",
       "3        False  0.234505  0.300062  1.903718  0.160482          0.222038   \n",
       "4         True  0.234102  0.241273  1.361896  0.182695          0.232855   \n",
       "...        ...       ...       ...       ...       ...               ...   \n",
       "1786795  False  0.343499  0.630422  4.550226  0.199559          0.353038   \n",
       "1786796  False  0.313335  0.395291  1.350077  0.171390          0.333600   \n",
       "1786797  False  0.326151  0.581158  3.998512  0.191833          0.327440   \n",
       "1786798  False  0.326401  0.430699  2.925698  0.127431          0.326283   \n",
       "1786799   True  0.304025  0.662617  4.754726  0.184366          0.299684   \n",
       "\n",
       "         fold  item_popularity  \n",
       "0           0               79  \n",
       "1           0               43  \n",
       "2           0              203  \n",
       "3           0               93  \n",
       "4           0              259  \n",
       "...       ...              ...  \n",
       "1786795     9               27  \n",
       "1786796     9              108  \n",
       "1786797     9               88  \n",
       "1786798     9               57  \n",
       "1786799     9               47  \n",
       "\n",
       "[17868000 rows x 13 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataframe[\"item_popularity\"] = item_popularity[\n",
    "    training_dataframe[\"ItemID\"].to_numpy().astype(int)\n",
    "]\n",
    "training_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance to closest items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity column 38121 (100.0%), 885.66 column/sec. Elapsed time 43.04 sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'float32'\n",
       "\twith 3812100 stored elements and shape (38121, 38121)>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_similarity = Compute_Similarity(icm_matrix.T).compute_similarity()\n",
    "item_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38116</th>\n",
       "      <td>0.000397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38117</th>\n",
       "      <td>0.007546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38118</th>\n",
       "      <td>0.005463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38119</th>\n",
       "      <td>0.000602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38120</th>\n",
       "      <td>0.002821</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38121 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       item_similarity\n",
       "0             0.000046\n",
       "1             0.000309\n",
       "2             0.000140\n",
       "3             0.000105\n",
       "4             0.000015\n",
       "...                ...\n",
       "38116         0.000397\n",
       "38117         0.007546\n",
       "38118         0.005463\n",
       "38119         0.000602\n",
       "38120         0.002821\n",
       "\n",
       "[38121 rows x 1 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_item_similarity_dict = {i: row.mean() for i, row in enumerate(item_similarity)}\n",
    "mean_item_similarity: pd.DataFrame = pd.Series(mean_item_similarity_dict).to_frame(name=\"item_similarity\")\n",
    "mean_item_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Recommender</th>\n",
       "      <th>Ranking</th>\n",
       "      <th>recommender_agreement</th>\n",
       "      <th>Label</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>user_wide_hybrid</th>\n",
       "      <th>fold</th>\n",
       "      <th>item_popularity</th>\n",
       "      <th>item_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>7703</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>0.268031</td>\n",
       "      <td>0.401745</td>\n",
       "      <td>2.693894</td>\n",
       "      <td>0.085774</td>\n",
       "      <td>0.272271</td>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "      <td>0.000730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>7547</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>0.258316</td>\n",
       "      <td>0.352410</td>\n",
       "      <td>2.404292</td>\n",
       "      <td>0.066998</td>\n",
       "      <td>0.265295</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>0.000170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>6822</td>\n",
       "      <td>60</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>0.237978</td>\n",
       "      <td>0.143136</td>\n",
       "      <td>0.850208</td>\n",
       "      <td>0.182649</td>\n",
       "      <td>0.221734</td>\n",
       "      <td>0</td>\n",
       "      <td>203</td>\n",
       "      <td>0.000112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>572</td>\n",
       "      <td>60</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>0.234505</td>\n",
       "      <td>0.300062</td>\n",
       "      <td>1.903718</td>\n",
       "      <td>0.160482</td>\n",
       "      <td>0.222038</td>\n",
       "      <td>0</td>\n",
       "      <td>93</td>\n",
       "      <td>0.000246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3077</td>\n",
       "      <td>60</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>0.234102</td>\n",
       "      <td>0.241273</td>\n",
       "      <td>1.361896</td>\n",
       "      <td>0.182695</td>\n",
       "      <td>0.232855</td>\n",
       "      <td>0</td>\n",
       "      <td>259</td>\n",
       "      <td>0.001331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1786795</th>\n",
       "      <td>35735</td>\n",
       "      <td>37445</td>\n",
       "      <td>user_wide_hybrid</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>0.343499</td>\n",
       "      <td>0.630422</td>\n",
       "      <td>4.550226</td>\n",
       "      <td>0.199559</td>\n",
       "      <td>0.353038</td>\n",
       "      <td>9</td>\n",
       "      <td>27</td>\n",
       "      <td>0.000132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1786796</th>\n",
       "      <td>35735</td>\n",
       "      <td>37507</td>\n",
       "      <td>user_wide_hybrid</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>0.313335</td>\n",
       "      <td>0.395291</td>\n",
       "      <td>1.350077</td>\n",
       "      <td>0.171390</td>\n",
       "      <td>0.333600</td>\n",
       "      <td>9</td>\n",
       "      <td>108</td>\n",
       "      <td>0.002226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1786797</th>\n",
       "      <td>35735</td>\n",
       "      <td>36775</td>\n",
       "      <td>user_wide_hybrid</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>0.326151</td>\n",
       "      <td>0.581158</td>\n",
       "      <td>3.998512</td>\n",
       "      <td>0.191833</td>\n",
       "      <td>0.327440</td>\n",
       "      <td>9</td>\n",
       "      <td>88</td>\n",
       "      <td>0.003954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1786798</th>\n",
       "      <td>35735</td>\n",
       "      <td>34998</td>\n",
       "      <td>user_wide_hybrid</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>0.326401</td>\n",
       "      <td>0.430699</td>\n",
       "      <td>2.925698</td>\n",
       "      <td>0.127431</td>\n",
       "      <td>0.326283</td>\n",
       "      <td>9</td>\n",
       "      <td>57</td>\n",
       "      <td>0.000664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1786799</th>\n",
       "      <td>35735</td>\n",
       "      <td>37801</td>\n",
       "      <td>user_wide_hybrid</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>0.304025</td>\n",
       "      <td>0.662617</td>\n",
       "      <td>4.754726</td>\n",
       "      <td>0.184366</td>\n",
       "      <td>0.299684</td>\n",
       "      <td>9</td>\n",
       "      <td>47</td>\n",
       "      <td>0.004306</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17868000 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         UserID ItemID       Recommender  Ranking  recommender_agreement  \\\n",
       "0             0   7703                60        0                      4   \n",
       "1             0   7547                60        1                      4   \n",
       "2             0   6822                60        2                      3   \n",
       "3             0    572                60        3                      5   \n",
       "4             0   3077                60        4                      4   \n",
       "...         ...    ...               ...      ...                    ...   \n",
       "1786795   35735  37445  user_wide_hybrid        5                      5   \n",
       "1786796   35735  37507  user_wide_hybrid        6                      2   \n",
       "1786797   35735  36775  user_wide_hybrid        7                      5   \n",
       "1786798   35735  34998  user_wide_hybrid        8                      2   \n",
       "1786799   35735  37801  user_wide_hybrid        9                      5   \n",
       "\n",
       "         Label        60        61        62        63  user_wide_hybrid  \\\n",
       "0        False  0.268031  0.401745  2.693894  0.085774          0.272271   \n",
       "1        False  0.258316  0.352410  2.404292  0.066998          0.265295   \n",
       "2         True  0.237978  0.143136  0.850208  0.182649          0.221734   \n",
       "3        False  0.234505  0.300062  1.903718  0.160482          0.222038   \n",
       "4         True  0.234102  0.241273  1.361896  0.182695          0.232855   \n",
       "...        ...       ...       ...       ...       ...               ...   \n",
       "1786795  False  0.343499  0.630422  4.550226  0.199559          0.353038   \n",
       "1786796  False  0.313335  0.395291  1.350077  0.171390          0.333600   \n",
       "1786797  False  0.326151  0.581158  3.998512  0.191833          0.327440   \n",
       "1786798  False  0.326401  0.430699  2.925698  0.127431          0.326283   \n",
       "1786799   True  0.304025  0.662617  4.754726  0.184366          0.299684   \n",
       "\n",
       "         fold  item_popularity  item_similarity  \n",
       "0           0               79         0.000730  \n",
       "1           0               43         0.000170  \n",
       "2           0              203         0.000112  \n",
       "3           0               93         0.000246  \n",
       "4           0              259         0.001331  \n",
       "...       ...              ...              ...  \n",
       "1786795     9               27         0.000132  \n",
       "1786796     9              108         0.002226  \n",
       "1786797     9               88         0.003954  \n",
       "1786798     9               57         0.000664  \n",
       "1786799     9               47         0.004306  \n",
       "\n",
       "[17868000 rows x 14 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataframe = training_dataframe.join(mean_item_similarity, on=\"ItemID\")\n",
    "training_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_popularity = np.ediff1d(sps.csr_matrix(urm_all).indptr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Recommender</th>\n",
       "      <th>Ranking</th>\n",
       "      <th>recommender_agreement</th>\n",
       "      <th>Label</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>user_wide_hybrid</th>\n",
       "      <th>fold</th>\n",
       "      <th>item_popularity</th>\n",
       "      <th>item_similarity</th>\n",
       "      <th>user_profile_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>7703</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>0.268031</td>\n",
       "      <td>0.401745</td>\n",
       "      <td>2.693894</td>\n",
       "      <td>0.085774</td>\n",
       "      <td>0.272271</td>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "      <td>0.000730</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>7547</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>0.258316</td>\n",
       "      <td>0.352410</td>\n",
       "      <td>2.404292</td>\n",
       "      <td>0.066998</td>\n",
       "      <td>0.265295</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>6822</td>\n",
       "      <td>60</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>0.237978</td>\n",
       "      <td>0.143136</td>\n",
       "      <td>0.850208</td>\n",
       "      <td>0.182649</td>\n",
       "      <td>0.221734</td>\n",
       "      <td>0</td>\n",
       "      <td>203</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>572</td>\n",
       "      <td>60</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>0.234505</td>\n",
       "      <td>0.300062</td>\n",
       "      <td>1.903718</td>\n",
       "      <td>0.160482</td>\n",
       "      <td>0.222038</td>\n",
       "      <td>0</td>\n",
       "      <td>93</td>\n",
       "      <td>0.000246</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3077</td>\n",
       "      <td>60</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>0.234102</td>\n",
       "      <td>0.241273</td>\n",
       "      <td>1.361896</td>\n",
       "      <td>0.182695</td>\n",
       "      <td>0.232855</td>\n",
       "      <td>0</td>\n",
       "      <td>259</td>\n",
       "      <td>0.001331</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1786795</th>\n",
       "      <td>35735</td>\n",
       "      <td>37445</td>\n",
       "      <td>user_wide_hybrid</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>0.343499</td>\n",
       "      <td>0.630422</td>\n",
       "      <td>4.550226</td>\n",
       "      <td>0.199559</td>\n",
       "      <td>0.353038</td>\n",
       "      <td>9</td>\n",
       "      <td>27</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1786796</th>\n",
       "      <td>35735</td>\n",
       "      <td>37507</td>\n",
       "      <td>user_wide_hybrid</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>0.313335</td>\n",
       "      <td>0.395291</td>\n",
       "      <td>1.350077</td>\n",
       "      <td>0.171390</td>\n",
       "      <td>0.333600</td>\n",
       "      <td>9</td>\n",
       "      <td>108</td>\n",
       "      <td>0.002226</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1786797</th>\n",
       "      <td>35735</td>\n",
       "      <td>36775</td>\n",
       "      <td>user_wide_hybrid</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>0.326151</td>\n",
       "      <td>0.581158</td>\n",
       "      <td>3.998512</td>\n",
       "      <td>0.191833</td>\n",
       "      <td>0.327440</td>\n",
       "      <td>9</td>\n",
       "      <td>88</td>\n",
       "      <td>0.003954</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1786798</th>\n",
       "      <td>35735</td>\n",
       "      <td>34998</td>\n",
       "      <td>user_wide_hybrid</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>0.326401</td>\n",
       "      <td>0.430699</td>\n",
       "      <td>2.925698</td>\n",
       "      <td>0.127431</td>\n",
       "      <td>0.326283</td>\n",
       "      <td>9</td>\n",
       "      <td>57</td>\n",
       "      <td>0.000664</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1786799</th>\n",
       "      <td>35735</td>\n",
       "      <td>37801</td>\n",
       "      <td>user_wide_hybrid</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>0.304025</td>\n",
       "      <td>0.662617</td>\n",
       "      <td>4.754726</td>\n",
       "      <td>0.184366</td>\n",
       "      <td>0.299684</td>\n",
       "      <td>9</td>\n",
       "      <td>47</td>\n",
       "      <td>0.004306</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17868000 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         UserID ItemID       Recommender  Ranking  recommender_agreement  \\\n",
       "0             0   7703                60        0                      4   \n",
       "1             0   7547                60        1                      4   \n",
       "2             0   6822                60        2                      3   \n",
       "3             0    572                60        3                      5   \n",
       "4             0   3077                60        4                      4   \n",
       "...         ...    ...               ...      ...                    ...   \n",
       "1786795   35735  37445  user_wide_hybrid        5                      5   \n",
       "1786796   35735  37507  user_wide_hybrid        6                      2   \n",
       "1786797   35735  36775  user_wide_hybrid        7                      5   \n",
       "1786798   35735  34998  user_wide_hybrid        8                      2   \n",
       "1786799   35735  37801  user_wide_hybrid        9                      5   \n",
       "\n",
       "         Label        60        61        62        63  user_wide_hybrid  \\\n",
       "0        False  0.268031  0.401745  2.693894  0.085774          0.272271   \n",
       "1        False  0.258316  0.352410  2.404292  0.066998          0.265295   \n",
       "2         True  0.237978  0.143136  0.850208  0.182649          0.221734   \n",
       "3        False  0.234505  0.300062  1.903718  0.160482          0.222038   \n",
       "4         True  0.234102  0.241273  1.361896  0.182695          0.232855   \n",
       "...        ...       ...       ...       ...       ...               ...   \n",
       "1786795  False  0.343499  0.630422  4.550226  0.199559          0.353038   \n",
       "1786796  False  0.313335  0.395291  1.350077  0.171390          0.333600   \n",
       "1786797  False  0.326151  0.581158  3.998512  0.191833          0.327440   \n",
       "1786798  False  0.326401  0.430699  2.925698  0.127431          0.326283   \n",
       "1786799   True  0.304025  0.662617  4.754726  0.184366          0.299684   \n",
       "\n",
       "         fold  item_popularity  item_similarity  user_profile_len  \n",
       "0           0               79         0.000730               114  \n",
       "1           0               43         0.000170               114  \n",
       "2           0              203         0.000112               114  \n",
       "3           0               93         0.000246               114  \n",
       "4           0              259         0.001331               114  \n",
       "...       ...              ...              ...               ...  \n",
       "1786795     9               27         0.000132                37  \n",
       "1786796     9              108         0.002226                37  \n",
       "1786797     9               88         0.003954                37  \n",
       "1786798     9               57         0.000664                37  \n",
       "1786799     9               47         0.004306                37  \n",
       "\n",
       "[17868000 rows x 15 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataframe[\"user_profile_len\"] = user_popularity[\n",
    "    training_dataframe[\"UserID\"].to_numpy().astype(int)\n",
    "]\n",
    "training_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User popularity bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure of how much popularity influences the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11146, 25392,  4601, ...,  8491, 21675,  8152])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_popularity_ranking = item_popularity.argsort()[::-1]\n",
    "item_popularity_ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1764602</th>\n",
       "      <td>35735</td>\n",
       "      <td>37802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1764603</th>\n",
       "      <td>35735</td>\n",
       "      <td>37803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1764604</th>\n",
       "      <td>35735</td>\n",
       "      <td>37805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1764605</th>\n",
       "      <td>35735</td>\n",
       "      <td>38000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1764606</th>\n",
       "      <td>35735</td>\n",
       "      <td>38034</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1764607 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_id  item_id\n",
       "0              0        0\n",
       "1              0        2\n",
       "2              0      120\n",
       "3              0      128\n",
       "4              0      211\n",
       "...          ...      ...\n",
       "1764602    35735    37802\n",
       "1764603    35735    37803\n",
       "1764604    35735    37805\n",
       "1764605    35735    38000\n",
       "1764606    35735    38034\n",
       "\n",
       "[1764607 rows x 2 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_id_df = urm_df[[\"user_id\", \"item_id\"]]\n",
    "item_id_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_POPULAR_THRESHOLDS = (10, 100, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>top_10</th>\n",
       "      <th>top_100</th>\n",
       "      <th>top_1000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>211</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1764602</th>\n",
       "      <td>35735</td>\n",
       "      <td>37802</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1764603</th>\n",
       "      <td>35735</td>\n",
       "      <td>37803</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1764604</th>\n",
       "      <td>35735</td>\n",
       "      <td>37805</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1764605</th>\n",
       "      <td>35735</td>\n",
       "      <td>38000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1764606</th>\n",
       "      <td>35735</td>\n",
       "      <td>38034</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1764607 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_id  item_id  top_10  top_100  top_1000\n",
       "0              0        0     0.0      0.0       0.0\n",
       "1              0        2     0.0      0.0       0.0\n",
       "2              0      120     0.0      0.0       0.0\n",
       "3              0      128     0.0      0.0       0.0\n",
       "4              0      211     0.0      0.0       1.0\n",
       "...          ...      ...     ...      ...       ...\n",
       "1764602    35735    37802     0.0      0.0       0.0\n",
       "1764603    35735    37803     0.0      0.0       0.0\n",
       "1764604    35735    37805     0.0      0.0       0.0\n",
       "1764605    35735    38000     0.0      0.0       0.0\n",
       "1764606    35735    38034     0.0      0.0       0.0\n",
       "\n",
       "[1764607 rows x 5 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for k in TOP_POPULAR_THRESHOLDS:\n",
    "    top_k_popular = item_popularity_ranking[:k]\n",
    "    item_id_df.loc[item_id_df[\"item_id\"].isin(top_k_popular), f\"top_{k}\"] = 1\n",
    "item_id_df = item_id_df.fillna(0)\n",
    "item_id_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>top_10</th>\n",
       "      <th>top_100</th>\n",
       "      <th>top_1000</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35731</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35732</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35733</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35734</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35735</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35736 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         top_10  top_100  top_1000\n",
       "user_id                           \n",
       "0           0.0      1.0       7.0\n",
       "1           2.0      3.0      10.0\n",
       "2           0.0      0.0       0.0\n",
       "3           1.0      4.0      11.0\n",
       "4           0.0      3.0      29.0\n",
       "...         ...      ...       ...\n",
       "35731       0.0      0.0       0.0\n",
       "35732       0.0      0.0       1.0\n",
       "35733       0.0      0.0       0.0\n",
       "35734       0.0      0.0       0.0\n",
       "35735       0.0      0.0       0.0\n",
       "\n",
       "[35736 rows x 3 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_top_k_df = item_id_df.groupby(\"user_id\").aggregate({f\"top_{k}\": \"sum\" for k in TOP_POPULAR_THRESHOLDS})\n",
    "user_top_k_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Recommender</th>\n",
       "      <th>Ranking</th>\n",
       "      <th>recommender_agreement</th>\n",
       "      <th>Label</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>user_wide_hybrid</th>\n",
       "      <th>fold</th>\n",
       "      <th>item_popularity</th>\n",
       "      <th>item_similarity</th>\n",
       "      <th>user_profile_len</th>\n",
       "      <th>top_10</th>\n",
       "      <th>top_100</th>\n",
       "      <th>top_1000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>7703</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>0.268031</td>\n",
       "      <td>0.401745</td>\n",
       "      <td>2.693894</td>\n",
       "      <td>0.085774</td>\n",
       "      <td>0.272271</td>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "      <td>0.000730</td>\n",
       "      <td>114</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>7547</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>0.258316</td>\n",
       "      <td>0.352410</td>\n",
       "      <td>2.404292</td>\n",
       "      <td>0.066998</td>\n",
       "      <td>0.265295</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>114</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>6822</td>\n",
       "      <td>60</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>0.237978</td>\n",
       "      <td>0.143136</td>\n",
       "      <td>0.850208</td>\n",
       "      <td>0.182649</td>\n",
       "      <td>0.221734</td>\n",
       "      <td>0</td>\n",
       "      <td>203</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>114</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>572</td>\n",
       "      <td>60</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>0.234505</td>\n",
       "      <td>0.300062</td>\n",
       "      <td>1.903718</td>\n",
       "      <td>0.160482</td>\n",
       "      <td>0.222038</td>\n",
       "      <td>0</td>\n",
       "      <td>93</td>\n",
       "      <td>0.000246</td>\n",
       "      <td>114</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3077</td>\n",
       "      <td>60</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>0.234102</td>\n",
       "      <td>0.241273</td>\n",
       "      <td>1.361896</td>\n",
       "      <td>0.182695</td>\n",
       "      <td>0.232855</td>\n",
       "      <td>0</td>\n",
       "      <td>259</td>\n",
       "      <td>0.001331</td>\n",
       "      <td>114</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1786795</th>\n",
       "      <td>35735</td>\n",
       "      <td>37445</td>\n",
       "      <td>user_wide_hybrid</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>0.343499</td>\n",
       "      <td>0.630422</td>\n",
       "      <td>4.550226</td>\n",
       "      <td>0.199559</td>\n",
       "      <td>0.353038</td>\n",
       "      <td>9</td>\n",
       "      <td>27</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>37</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1786796</th>\n",
       "      <td>35735</td>\n",
       "      <td>37507</td>\n",
       "      <td>user_wide_hybrid</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>0.313335</td>\n",
       "      <td>0.395291</td>\n",
       "      <td>1.350077</td>\n",
       "      <td>0.171390</td>\n",
       "      <td>0.333600</td>\n",
       "      <td>9</td>\n",
       "      <td>108</td>\n",
       "      <td>0.002226</td>\n",
       "      <td>37</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1786797</th>\n",
       "      <td>35735</td>\n",
       "      <td>36775</td>\n",
       "      <td>user_wide_hybrid</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>0.326151</td>\n",
       "      <td>0.581158</td>\n",
       "      <td>3.998512</td>\n",
       "      <td>0.191833</td>\n",
       "      <td>0.327440</td>\n",
       "      <td>9</td>\n",
       "      <td>88</td>\n",
       "      <td>0.003954</td>\n",
       "      <td>37</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1786798</th>\n",
       "      <td>35735</td>\n",
       "      <td>34998</td>\n",
       "      <td>user_wide_hybrid</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>0.326401</td>\n",
       "      <td>0.430699</td>\n",
       "      <td>2.925698</td>\n",
       "      <td>0.127431</td>\n",
       "      <td>0.326283</td>\n",
       "      <td>9</td>\n",
       "      <td>57</td>\n",
       "      <td>0.000664</td>\n",
       "      <td>37</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1786799</th>\n",
       "      <td>35735</td>\n",
       "      <td>37801</td>\n",
       "      <td>user_wide_hybrid</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>0.304025</td>\n",
       "      <td>0.662617</td>\n",
       "      <td>4.754726</td>\n",
       "      <td>0.184366</td>\n",
       "      <td>0.299684</td>\n",
       "      <td>9</td>\n",
       "      <td>47</td>\n",
       "      <td>0.004306</td>\n",
       "      <td>37</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17868000 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         UserID ItemID       Recommender  Ranking  recommender_agreement  \\\n",
       "0             0   7703                60        0                      4   \n",
       "1             0   7547                60        1                      4   \n",
       "2             0   6822                60        2                      3   \n",
       "3             0    572                60        3                      5   \n",
       "4             0   3077                60        4                      4   \n",
       "...         ...    ...               ...      ...                    ...   \n",
       "1786795   35735  37445  user_wide_hybrid        5                      5   \n",
       "1786796   35735  37507  user_wide_hybrid        6                      2   \n",
       "1786797   35735  36775  user_wide_hybrid        7                      5   \n",
       "1786798   35735  34998  user_wide_hybrid        8                      2   \n",
       "1786799   35735  37801  user_wide_hybrid        9                      5   \n",
       "\n",
       "         Label        60        61        62        63  user_wide_hybrid  \\\n",
       "0        False  0.268031  0.401745  2.693894  0.085774          0.272271   \n",
       "1        False  0.258316  0.352410  2.404292  0.066998          0.265295   \n",
       "2         True  0.237978  0.143136  0.850208  0.182649          0.221734   \n",
       "3        False  0.234505  0.300062  1.903718  0.160482          0.222038   \n",
       "4         True  0.234102  0.241273  1.361896  0.182695          0.232855   \n",
       "...        ...       ...       ...       ...       ...               ...   \n",
       "1786795  False  0.343499  0.630422  4.550226  0.199559          0.353038   \n",
       "1786796  False  0.313335  0.395291  1.350077  0.171390          0.333600   \n",
       "1786797  False  0.326151  0.581158  3.998512  0.191833          0.327440   \n",
       "1786798  False  0.326401  0.430699  2.925698  0.127431          0.326283   \n",
       "1786799   True  0.304025  0.662617  4.754726  0.184366          0.299684   \n",
       "\n",
       "         fold  item_popularity  item_similarity  user_profile_len  top_10  \\\n",
       "0           0               79         0.000730               114     0.0   \n",
       "1           0               43         0.000170               114     0.0   \n",
       "2           0              203         0.000112               114     0.0   \n",
       "3           0               93         0.000246               114     0.0   \n",
       "4           0              259         0.001331               114     0.0   \n",
       "...       ...              ...              ...               ...     ...   \n",
       "1786795     9               27         0.000132                37     0.0   \n",
       "1786796     9              108         0.002226                37     0.0   \n",
       "1786797     9               88         0.003954                37     0.0   \n",
       "1786798     9               57         0.000664                37     0.0   \n",
       "1786799     9               47         0.004306                37     0.0   \n",
       "\n",
       "         top_100  top_1000  \n",
       "0            1.0       7.0  \n",
       "1            1.0       7.0  \n",
       "2            1.0       7.0  \n",
       "3            1.0       7.0  \n",
       "4            1.0       7.0  \n",
       "...          ...       ...  \n",
       "1786795      0.0       0.0  \n",
       "1786796      0.0       0.0  \n",
       "1786797      0.0       0.0  \n",
       "1786798      0.0       0.0  \n",
       "1786799      0.0       0.0  \n",
       "\n",
       "[17868000 rows x 18 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataframe = training_dataframe.join(user_top_k_df, on=\"UserID\")\n",
    "training_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance to closest users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity column 35736 (100.0%), 6061.88 column/sec. Elapsed time 5.90 sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'float32'\n",
       "\twith 3573591 stored elements and shape (35736, 35736)>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_similarity = Compute_Similarity(urm_all.T).compute_similarity()\n",
    "user_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35731</th>\n",
       "      <td>0.000204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35732</th>\n",
       "      <td>0.000333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35733</th>\n",
       "      <td>0.000267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35734</th>\n",
       "      <td>0.000341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35735</th>\n",
       "      <td>0.000272</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35736 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_similarity\n",
       "0             0.000183\n",
       "1             0.000357\n",
       "2             0.000161\n",
       "3             0.000158\n",
       "4             0.001004\n",
       "...                ...\n",
       "35731         0.000204\n",
       "35732         0.000333\n",
       "35733         0.000267\n",
       "35734         0.000341\n",
       "35735         0.000272\n",
       "\n",
       "[35736 rows x 1 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_user_similarity_dict = {i: row.mean() for i, row in enumerate(user_similarity)}\n",
    "mean_user_similarity: pd.DataFrame = pd.Series(mean_user_similarity_dict).to_frame(name=\"user_similarity\")\n",
    "mean_user_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Recommender</th>\n",
       "      <th>Ranking</th>\n",
       "      <th>recommender_agreement</th>\n",
       "      <th>Label</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>user_wide_hybrid</th>\n",
       "      <th>fold</th>\n",
       "      <th>item_popularity</th>\n",
       "      <th>item_similarity</th>\n",
       "      <th>user_profile_len</th>\n",
       "      <th>top_10</th>\n",
       "      <th>top_100</th>\n",
       "      <th>top_1000</th>\n",
       "      <th>user_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>7703</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>0.268031</td>\n",
       "      <td>0.401745</td>\n",
       "      <td>2.693894</td>\n",
       "      <td>0.085774</td>\n",
       "      <td>0.272271</td>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "      <td>0.000730</td>\n",
       "      <td>114</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>7547</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>0.258316</td>\n",
       "      <td>0.352410</td>\n",
       "      <td>2.404292</td>\n",
       "      <td>0.066998</td>\n",
       "      <td>0.265295</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>114</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>6822</td>\n",
       "      <td>60</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>0.237978</td>\n",
       "      <td>0.143136</td>\n",
       "      <td>0.850208</td>\n",
       "      <td>0.182649</td>\n",
       "      <td>0.221734</td>\n",
       "      <td>0</td>\n",
       "      <td>203</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>114</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>572</td>\n",
       "      <td>60</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>0.234505</td>\n",
       "      <td>0.300062</td>\n",
       "      <td>1.903718</td>\n",
       "      <td>0.160482</td>\n",
       "      <td>0.222038</td>\n",
       "      <td>0</td>\n",
       "      <td>93</td>\n",
       "      <td>0.000246</td>\n",
       "      <td>114</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3077</td>\n",
       "      <td>60</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>0.234102</td>\n",
       "      <td>0.241273</td>\n",
       "      <td>1.361896</td>\n",
       "      <td>0.182695</td>\n",
       "      <td>0.232855</td>\n",
       "      <td>0</td>\n",
       "      <td>259</td>\n",
       "      <td>0.001331</td>\n",
       "      <td>114</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1786795</th>\n",
       "      <td>35735</td>\n",
       "      <td>37445</td>\n",
       "      <td>user_wide_hybrid</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>0.343499</td>\n",
       "      <td>0.630422</td>\n",
       "      <td>4.550226</td>\n",
       "      <td>0.199559</td>\n",
       "      <td>0.353038</td>\n",
       "      <td>9</td>\n",
       "      <td>27</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>37</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1786796</th>\n",
       "      <td>35735</td>\n",
       "      <td>37507</td>\n",
       "      <td>user_wide_hybrid</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>0.313335</td>\n",
       "      <td>0.395291</td>\n",
       "      <td>1.350077</td>\n",
       "      <td>0.171390</td>\n",
       "      <td>0.333600</td>\n",
       "      <td>9</td>\n",
       "      <td>108</td>\n",
       "      <td>0.002226</td>\n",
       "      <td>37</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1786797</th>\n",
       "      <td>35735</td>\n",
       "      <td>36775</td>\n",
       "      <td>user_wide_hybrid</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>0.326151</td>\n",
       "      <td>0.581158</td>\n",
       "      <td>3.998512</td>\n",
       "      <td>0.191833</td>\n",
       "      <td>0.327440</td>\n",
       "      <td>9</td>\n",
       "      <td>88</td>\n",
       "      <td>0.003954</td>\n",
       "      <td>37</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1786798</th>\n",
       "      <td>35735</td>\n",
       "      <td>34998</td>\n",
       "      <td>user_wide_hybrid</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>0.326401</td>\n",
       "      <td>0.430699</td>\n",
       "      <td>2.925698</td>\n",
       "      <td>0.127431</td>\n",
       "      <td>0.326283</td>\n",
       "      <td>9</td>\n",
       "      <td>57</td>\n",
       "      <td>0.000664</td>\n",
       "      <td>37</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1786799</th>\n",
       "      <td>35735</td>\n",
       "      <td>37801</td>\n",
       "      <td>user_wide_hybrid</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>0.304025</td>\n",
       "      <td>0.662617</td>\n",
       "      <td>4.754726</td>\n",
       "      <td>0.184366</td>\n",
       "      <td>0.299684</td>\n",
       "      <td>9</td>\n",
       "      <td>47</td>\n",
       "      <td>0.004306</td>\n",
       "      <td>37</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000272</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17868000 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         UserID ItemID       Recommender  Ranking  recommender_agreement  \\\n",
       "0             0   7703                60        0                      4   \n",
       "1             0   7547                60        1                      4   \n",
       "2             0   6822                60        2                      3   \n",
       "3             0    572                60        3                      5   \n",
       "4             0   3077                60        4                      4   \n",
       "...         ...    ...               ...      ...                    ...   \n",
       "1786795   35735  37445  user_wide_hybrid        5                      5   \n",
       "1786796   35735  37507  user_wide_hybrid        6                      2   \n",
       "1786797   35735  36775  user_wide_hybrid        7                      5   \n",
       "1786798   35735  34998  user_wide_hybrid        8                      2   \n",
       "1786799   35735  37801  user_wide_hybrid        9                      5   \n",
       "\n",
       "         Label        60        61        62        63  user_wide_hybrid  \\\n",
       "0        False  0.268031  0.401745  2.693894  0.085774          0.272271   \n",
       "1        False  0.258316  0.352410  2.404292  0.066998          0.265295   \n",
       "2         True  0.237978  0.143136  0.850208  0.182649          0.221734   \n",
       "3        False  0.234505  0.300062  1.903718  0.160482          0.222038   \n",
       "4         True  0.234102  0.241273  1.361896  0.182695          0.232855   \n",
       "...        ...       ...       ...       ...       ...               ...   \n",
       "1786795  False  0.343499  0.630422  4.550226  0.199559          0.353038   \n",
       "1786796  False  0.313335  0.395291  1.350077  0.171390          0.333600   \n",
       "1786797  False  0.326151  0.581158  3.998512  0.191833          0.327440   \n",
       "1786798  False  0.326401  0.430699  2.925698  0.127431          0.326283   \n",
       "1786799   True  0.304025  0.662617  4.754726  0.184366          0.299684   \n",
       "\n",
       "         fold  item_popularity  item_similarity  user_profile_len  top_10  \\\n",
       "0           0               79         0.000730               114     0.0   \n",
       "1           0               43         0.000170               114     0.0   \n",
       "2           0              203         0.000112               114     0.0   \n",
       "3           0               93         0.000246               114     0.0   \n",
       "4           0              259         0.001331               114     0.0   \n",
       "...       ...              ...              ...               ...     ...   \n",
       "1786795     9               27         0.000132                37     0.0   \n",
       "1786796     9              108         0.002226                37     0.0   \n",
       "1786797     9               88         0.003954                37     0.0   \n",
       "1786798     9               57         0.000664                37     0.0   \n",
       "1786799     9               47         0.004306                37     0.0   \n",
       "\n",
       "         top_100  top_1000  user_similarity  \n",
       "0            1.0       7.0         0.000183  \n",
       "1            1.0       7.0         0.000183  \n",
       "2            1.0       7.0         0.000183  \n",
       "3            1.0       7.0         0.000183  \n",
       "4            1.0       7.0         0.000183  \n",
       "...          ...       ...              ...  \n",
       "1786795      0.0       0.0         0.000272  \n",
       "1786796      0.0       0.0         0.000272  \n",
       "1786797      0.0       0.0         0.000272  \n",
       "1786798      0.0       0.0         0.000272  \n",
       "1786799      0.0       0.0         0.000272  \n",
       "\n",
       "[17868000 rows x 19 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataframe = training_dataframe.join(mean_user_similarity, on=\"UserID\")\n",
    "training_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-ranker data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion to categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Recommender</th>\n",
       "      <th>Ranking</th>\n",
       "      <th>recommender_agreement</th>\n",
       "      <th>Label</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>user_wide_hybrid</th>\n",
       "      <th>fold</th>\n",
       "      <th>item_popularity</th>\n",
       "      <th>item_similarity</th>\n",
       "      <th>user_profile_len</th>\n",
       "      <th>top_10</th>\n",
       "      <th>top_100</th>\n",
       "      <th>top_1000</th>\n",
       "      <th>user_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>6822</td>\n",
       "      <td>user_wide_hybrid</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>0.237978</td>\n",
       "      <td>0.143136</td>\n",
       "      <td>0.850208</td>\n",
       "      <td>0.182649</td>\n",
       "      <td>0.221734</td>\n",
       "      <td>0</td>\n",
       "      <td>203</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>114</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3074</td>\n",
       "      <td>user_wide_hybrid</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>0.211892</td>\n",
       "      <td>0.239386</td>\n",
       "      <td>1.367028</td>\n",
       "      <td>0.246037</td>\n",
       "      <td>0.211679</td>\n",
       "      <td>0</td>\n",
       "      <td>446</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>114</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>7703</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>0.268031</td>\n",
       "      <td>0.401745</td>\n",
       "      <td>2.693894</td>\n",
       "      <td>0.085774</td>\n",
       "      <td>0.272271</td>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "      <td>0.000730</td>\n",
       "      <td>114</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>7547</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>0.258316</td>\n",
       "      <td>0.352410</td>\n",
       "      <td>2.404292</td>\n",
       "      <td>0.066998</td>\n",
       "      <td>0.265295</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>114</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>6822</td>\n",
       "      <td>60</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>0.237978</td>\n",
       "      <td>0.143136</td>\n",
       "      <td>0.850208</td>\n",
       "      <td>0.182649</td>\n",
       "      <td>0.221734</td>\n",
       "      <td>0</td>\n",
       "      <td>203</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>114</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17867995</th>\n",
       "      <td>35735</td>\n",
       "      <td>36493</td>\n",
       "      <td>62</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>0.313986</td>\n",
       "      <td>0.671176</td>\n",
       "      <td>4.725080</td>\n",
       "      <td>0.194312</td>\n",
       "      <td>0.314164</td>\n",
       "      <td>6</td>\n",
       "      <td>33</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>37</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17867996</th>\n",
       "      <td>35735</td>\n",
       "      <td>37657</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>0.458836</td>\n",
       "      <td>0.886808</td>\n",
       "      <td>6.029325</td>\n",
       "      <td>0.334913</td>\n",
       "      <td>0.432514</td>\n",
       "      <td>9</td>\n",
       "      <td>93</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>37</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17867997</th>\n",
       "      <td>35735</td>\n",
       "      <td>37017</td>\n",
       "      <td>user_wide_hybrid</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>0.330222</td>\n",
       "      <td>0.552358</td>\n",
       "      <td>3.761626</td>\n",
       "      <td>0.177561</td>\n",
       "      <td>0.329455</td>\n",
       "      <td>6</td>\n",
       "      <td>58</td>\n",
       "      <td>0.003902</td>\n",
       "      <td>37</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17867998</th>\n",
       "      <td>35735</td>\n",
       "      <td>36493</td>\n",
       "      <td>user_wide_hybrid</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>0.367314</td>\n",
       "      <td>0.961225</td>\n",
       "      <td>6.795360</td>\n",
       "      <td>0.279113</td>\n",
       "      <td>0.361406</td>\n",
       "      <td>9</td>\n",
       "      <td>33</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>37</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17867999</th>\n",
       "      <td>35735</td>\n",
       "      <td>37507</td>\n",
       "      <td>user_wide_hybrid</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>0.343762</td>\n",
       "      <td>0.492112</td>\n",
       "      <td>3.257462</td>\n",
       "      <td>0.209394</td>\n",
       "      <td>0.343257</td>\n",
       "      <td>6</td>\n",
       "      <td>108</td>\n",
       "      <td>0.002226</td>\n",
       "      <td>37</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000272</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17868000 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          UserID ItemID       Recommender  Ranking  recommender_agreement  \\\n",
       "0              0   6822  user_wide_hybrid        4                      3   \n",
       "1              0   3074  user_wide_hybrid        5                      3   \n",
       "2              0   7703                60        0                      4   \n",
       "3              0   7547                60        1                      4   \n",
       "4              0   6822                60        2                      3   \n",
       "...          ...    ...               ...      ...                    ...   \n",
       "17867995   35735  36493                62        2                      5   \n",
       "17867996   35735  37657                60        0                      5   \n",
       "17867997   35735  37017  user_wide_hybrid        6                      5   \n",
       "17867998   35735  36493  user_wide_hybrid        4                      5   \n",
       "17867999   35735  37507  user_wide_hybrid        5                      4   \n",
       "\n",
       "          Label        60        61        62        63  user_wide_hybrid  \\\n",
       "0          True  0.237978  0.143136  0.850208  0.182649          0.221734   \n",
       "1         False  0.211892  0.239386  1.367028  0.246037          0.211679   \n",
       "2         False  0.268031  0.401745  2.693894  0.085774          0.272271   \n",
       "3         False  0.258316  0.352410  2.404292  0.066998          0.265295   \n",
       "4          True  0.237978  0.143136  0.850208  0.182649          0.221734   \n",
       "...         ...       ...       ...       ...       ...               ...   \n",
       "17867995  False  0.313986  0.671176  4.725080  0.194312          0.314164   \n",
       "17867996  False  0.458836  0.886808  6.029325  0.334913          0.432514   \n",
       "17867997  False  0.330222  0.552358  3.761626  0.177561          0.329455   \n",
       "17867998  False  0.367314  0.961225  6.795360  0.279113          0.361406   \n",
       "17867999  False  0.343762  0.492112  3.257462  0.209394          0.343257   \n",
       "\n",
       "          fold  item_popularity  item_similarity  user_profile_len  top_10  \\\n",
       "0            0              203         0.000112               114     0.0   \n",
       "1            0              446         0.000315               114     0.0   \n",
       "2            0               79         0.000730               114     0.0   \n",
       "3            0               43         0.000170               114     0.0   \n",
       "4            0              203         0.000112               114     0.0   \n",
       "...        ...              ...              ...               ...     ...   \n",
       "17867995     6               33         0.000083                37     0.0   \n",
       "17867996     9               93         0.000205                37     0.0   \n",
       "17867997     6               58         0.003902                37     0.0   \n",
       "17867998     9               33         0.000083                37     0.0   \n",
       "17867999     6              108         0.002226                37     0.0   \n",
       "\n",
       "          top_100  top_1000  user_similarity  \n",
       "0             1.0       7.0         0.000183  \n",
       "1             1.0       7.0         0.000183  \n",
       "2             1.0       7.0         0.000183  \n",
       "3             1.0       7.0         0.000183  \n",
       "4             1.0       7.0         0.000183  \n",
       "...           ...       ...              ...  \n",
       "17867995      0.0       0.0         0.000272  \n",
       "17867996      0.0       0.0         0.000272  \n",
       "17867997      0.0       0.0         0.000272  \n",
       "17867998      0.0       0.0         0.000272  \n",
       "17867999      0.0       0.0         0.000272  \n",
       "\n",
       "[17868000 rows x 19 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataframe = training_dataframe.sort_values(\"UserID\").reset_index(drop=True)\n",
    "training_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17868000 entries, 0 to 17867999\n",
      "Data columns (total 19 columns):\n",
      " #   Column                 Dtype  \n",
      "---  ------                 -----  \n",
      " 0   UserID                 int64  \n",
      " 1   ItemID                 object \n",
      " 2   Recommender            object \n",
      " 3   Ranking                int64  \n",
      " 4   recommender_agreement  int64  \n",
      " 5   Label                  bool   \n",
      " 6   60                     float32\n",
      " 7   61                     float32\n",
      " 8   62                     float32\n",
      " 9   63                     float32\n",
      " 10  user_wide_hybrid       float32\n",
      " 11  fold                   int64  \n",
      " 12  item_popularity        int32  \n",
      " 13  item_similarity        float32\n",
      " 14  user_profile_len       int32  \n",
      " 15  top_10                 float64\n",
      " 16  top_100                float64\n",
      " 17  top_1000               float64\n",
      " 18  user_similarity        float32\n",
      "dtypes: bool(1), float32(7), float64(3), int32(2), int64(4), object(2)\n",
      "memory usage: 1.8+ GB\n"
     ]
    }
   ],
   "source": [
    "training_dataframe.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17868000 entries, 0 to 17867999\n",
      "Data columns (total 19 columns):\n",
      " #   Column                 Dtype  \n",
      "---  ------                 -----  \n",
      " 0   UserID                 int64  \n",
      " 1   ItemID                 object \n",
      " 2   Recommender            object \n",
      " 3   Ranking                int64  \n",
      " 4   recommender_agreement  int64  \n",
      " 5   Label                  bool   \n",
      " 6   60                     float32\n",
      " 7   61                     float32\n",
      " 8   62                     float32\n",
      " 9   63                     float32\n",
      " 10  user_wide_hybrid       float32\n",
      " 11  fold                   int64  \n",
      " 12  item_popularity        int32  \n",
      " 13  item_similarity        float32\n",
      " 14  user_profile_len       int32  \n",
      " 15  top_10                 float64\n",
      " 16  top_100                float64\n",
      " 17  top_1000               float64\n",
      " 18  user_similarity        float32\n",
      "dtypes: bool(1), float32(7), float64(3), int32(2), int64(4), object(2)\n",
      "memory usage: 1.8+ GB\n"
     ]
    }
   ],
   "source": [
    "training_dataframe.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17868000 entries, 0 to 17867999\n",
      "Data columns (total 19 columns):\n",
      " #   Column                 Dtype   \n",
      "---  ------                 -----   \n",
      " 0   UserID                 category\n",
      " 1   ItemID                 category\n",
      " 2   Recommender            category\n",
      " 3   Ranking                int64   \n",
      " 4   recommender_agreement  int64   \n",
      " 5   Label                  bool    \n",
      " 6   60                     float32 \n",
      " 7   61                     float32 \n",
      " 8   62                     float32 \n",
      " 9   63                     float32 \n",
      " 10  user_wide_hybrid       float32 \n",
      " 11  fold                   int64   \n",
      " 12  item_popularity        int32   \n",
      " 13  item_similarity        float32 \n",
      " 14  user_profile_len       int32   \n",
      " 15  top_10                 float64 \n",
      " 16  top_100                float64 \n",
      " 17  top_1000               float64 \n",
      " 18  user_similarity        float32 \n",
      "dtypes: bool(1), category(3), float32(7), float64(3), int32(2), int64(3)\n",
      "memory usage: 1.6 GB\n"
     ]
    }
   ],
   "source": [
    "categorical_training_dataframe = training_dataframe\n",
    "for categorical_column in (\"UserID\", \"ItemID\", \"Recommender\"):\n",
    "    categorical_training_dataframe[categorical_column] = categorical_training_dataframe[categorical_column].astype(\"category\")\n",
    "categorical_training_dataframe.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_training_dataframe.to_parquet(\"ranker_training_data_2_no_score.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_training_dataframe = pd.read_parquet(\"ranker_training_data_2_no_score.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = categorical_training_dataframe.pop(\"fold\")\n",
    "\n",
    "train_df = categorical_training_dataframe[fold != 9]\n",
    "y_train = train_df[\"Label\"]\n",
    "X_train = train_df.drop(columns=\"Label\")\n",
    "\n",
    "val_df = categorical_training_dataframe[fold == 9]\n",
    "y_val = val_df[\"Label\"]\n",
    "X_val = val_df.drop(columns=\"Label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the ranker one first needs to specify the size of the groups, a group is the dimension you rank on, in this case each group corresponds to a user. Since we have generated a fixed number of candidates for each user, all groups have the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35736,) (35736,)\n"
     ]
    }
   ],
   "source": [
    "train_groups = X_train.groupby(\"UserID\").size().to_numpy()\n",
    "val_groups = X_val.groupby(\"UserID\").size().to_numpy()\n",
    "\n",
    "print(train_groups.shape, val_groups.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False    0.940924\n",
      "True     0.059076\n",
      "Name: Label, dtype: float64\n",
      "False    0.940924\n",
      "True     0.059076\n",
      "Name: Label, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "for y in (y_train, y_val):\n",
    "    print(pd.Series(y_train).value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 16081200 entries, 0 to 17867999\n",
      "Data columns (total 17 columns):\n",
      " #   Column                 Dtype   \n",
      "---  ------                 -----   \n",
      " 0   UserID                 int64   \n",
      " 1   ItemID                 int64   \n",
      " 2   Recommender            category\n",
      " 3   Ranking                int64   \n",
      " 4   recommender_agreement  int64   \n",
      " 5   60                     float32 \n",
      " 6   61                     float32 \n",
      " 7   62                     float32 \n",
      " 8   63                     float32 \n",
      " 9   user_wide_hybrid       float32 \n",
      " 10  item_popularity        int32   \n",
      " 11  item_similarity        float32 \n",
      " 12  user_profile_len       int32   \n",
      " 13  top_10                 float64 \n",
      " 14  top_100                float64 \n",
      " 15  top_1000               float64 \n",
      " 16  user_similarity        float32 \n",
      "dtypes: category(1), float32(7), float64(3), int32(2), int64(4)\n",
      "memory usage: 1.5 GB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1786800 entries, 256 to 17867998\n",
      "Data columns (total 17 columns):\n",
      " #   Column                 Dtype   \n",
      "---  ------                 -----   \n",
      " 0   UserID                 int64   \n",
      " 1   ItemID                 int64   \n",
      " 2   Recommender            category\n",
      " 3   Ranking                int64   \n",
      " 4   recommender_agreement  int64   \n",
      " 5   60                     float32 \n",
      " 6   61                     float32 \n",
      " 7   62                     float32 \n",
      " 8   63                     float32 \n",
      " 9   user_wide_hybrid       float32 \n",
      " 10  item_popularity        int32   \n",
      " 11  item_similarity        float32 \n",
      " 12  user_profile_len       int32   \n",
      " 13  top_10                 float64 \n",
      " 14  top_100                float64 \n",
      " 15  top_1000               float64 \n",
      " 16  user_similarity        float32 \n",
      "dtypes: category(1), float32(7), float64(3), int32(2), int64(4)\n",
      "memory usage: 172.1 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "for X in (X_train, X_val):\n",
    "    print(X.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = 50\n",
    "learning_rate = 1e-1\n",
    "reg_alpha = 1e-1\n",
    "reg_lambda = 1e-1\n",
    "max_depth = 3\n",
    "max_leaves = 0\n",
    "grow_policy = \"depthwise\"\n",
    "objective = \"pairwise\"\n",
    "booster = \"gbtree\"\n",
    "use_user_profile = False\n",
    "random_seed = None\n",
    "\n",
    "XGB_model = XGBRanker(\n",
    "    enable_categorical=True,\n",
    "    objective=\"rank:{}\".format(objective),\n",
    "    n_estimators=int(n_estimators),\n",
    "    random_state=random_seed,\n",
    "    learning_rate=learning_rate,\n",
    "    reg_alpha=reg_alpha,\n",
    "    reg_lambda=reg_lambda,\n",
    "    max_depth=int(max_depth),\n",
    "    max_leaves=int(max_leaves),\n",
    "    grow_policy=grow_policy,\n",
    "    verbosity=0,  # 2 if self.verbose else 0,\n",
    "    booster=booster,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBRanker(base_score=None, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "          colsample_bylevel=None, colsample_bynode=None, colsample_bytree=None,\n",
       "          device=None, early_stopping_rounds=None, enable_categorical=True,\n",
       "          eval_metric=None, feature_types=None, gamma=None,\n",
       "          grow_policy=&#x27;depthwise&#x27;, importance_type=None,\n",
       "          interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
       "          max_cat_threshold=None, max_cat_to_onehot=None, max_delta_step=None,\n",
       "          max_depth=3, max_leaves=0, min_child_weight=None, missing=nan,\n",
       "          monotone_constraints=None, multi_strategy=None, n_estimators=50,\n",
       "          n_jobs=None, num_parallel_tree=None, objective=&#x27;rank:pairwise&#x27;, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;XGBRanker<span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>XGBRanker(base_score=None, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "          colsample_bylevel=None, colsample_bynode=None, colsample_bytree=None,\n",
       "          device=None, early_stopping_rounds=None, enable_categorical=True,\n",
       "          eval_metric=None, feature_types=None, gamma=None,\n",
       "          grow_policy=&#x27;depthwise&#x27;, importance_type=None,\n",
       "          interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
       "          max_cat_threshold=None, max_cat_to_onehot=None, max_delta_step=None,\n",
       "          max_depth=3, max_leaves=0, min_child_weight=None, missing=nan,\n",
       "          monotone_constraints=None, multi_strategy=None, n_estimators=50,\n",
       "          n_jobs=None, num_parallel_tree=None, objective=&#x27;rank:pairwise&#x27;, ...)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "XGBRanker(base_score=None, booster='gbtree', callbacks=None,\n",
       "          colsample_bylevel=None, colsample_bynode=None, colsample_bytree=None,\n",
       "          device=None, early_stopping_rounds=None, enable_categorical=True,\n",
       "          eval_metric=None, feature_types=None, gamma=None,\n",
       "          grow_policy='depthwise', importance_type=None,\n",
       "          interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
       "          max_cat_threshold=None, max_cat_to_onehot=None, max_delta_step=None,\n",
       "          max_depth=3, max_leaves=0, min_child_weight=None, missing=nan,\n",
       "          monotone_constraints=None, multi_strategy=None, n_estimators=50,\n",
       "          n_jobs=None, num_parallel_tree=None, objective='rank:pairwise', ...)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XGB_model.fit(\n",
    "    X_train,  # .drop(columns=[\"UserID\", \"ItemID\"]),\n",
    "    y_train,\n",
    "    group=train_groups,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04285037174035326"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XGB_model.score(\n",
    "    X_train,  # .drop(columns=[\"UserID\", \"ItemID\"]),\n",
    "    y_train,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XGB_model.score(\n",
    "    X_val,  # .drop(columns=[\"UserID\", \"ItemID\"]),\n",
    "    y_val,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is trained we can use it to compute predictions. Each prediction will refer to a specific user-item pair, which we will then need to rank as we do in any other recommender model.\n",
    "\n",
    "**Important:** In order to use this model to predict the score of new datapoints (i.e., new recommendations) we have to repeat the same data processing steps but:\n",
    "- We do not need a train-label split, we can user all the data we have to compute the predictions and the features\n",
    "- The recommendation models used to generate the scores should be trained on all the available data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can have a look to the feature importance to assess which are the most informative ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: title={'center': 'Weight (Frequence)'}, xlabel='F score', ylabel='Features'>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from xgboost import plot_importance\n",
    "\n",
    "plot_importance(XGB_model, importance_type=\"weight\", title=\"Weight (Frequence)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decode predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>0</td>\n",
       "      <td>454</td>\n",
       "      <td>-0.094380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>0</td>\n",
       "      <td>8505</td>\n",
       "      <td>-0.068681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>0</td>\n",
       "      <td>8505</td>\n",
       "      <td>-0.085446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>0</td>\n",
       "      <td>14888</td>\n",
       "      <td>-0.064724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>0</td>\n",
       "      <td>2746</td>\n",
       "      <td>-0.104188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17867978</th>\n",
       "      <td>35735</td>\n",
       "      <td>36775</td>\n",
       "      <td>0.012558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17867988</th>\n",
       "      <td>35735</td>\n",
       "      <td>37657</td>\n",
       "      <td>0.097238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17867989</th>\n",
       "      <td>35735</td>\n",
       "      <td>36493</td>\n",
       "      <td>0.088098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17867996</th>\n",
       "      <td>35735</td>\n",
       "      <td>36034</td>\n",
       "      <td>0.048838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17867998</th>\n",
       "      <td>35735</td>\n",
       "      <td>37800</td>\n",
       "      <td>0.008314</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1786800 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          UserID  ItemID     Score\n",
       "256            0     454 -0.094380\n",
       "257            0    8505 -0.068681\n",
       "264            0    8505 -0.085446\n",
       "265            0   14888 -0.064724\n",
       "266            0    2746 -0.104188\n",
       "...          ...     ...       ...\n",
       "17867978   35735   36775  0.012558\n",
       "17867988   35735   37657  0.097238\n",
       "17867989   35735   36493  0.088098\n",
       "17867996   35735   36034  0.048838\n",
       "17867998   35735   37800  0.008314\n",
       "\n",
       "[1786800 rows x 3 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val_hat = X_val[[\"UserID\", \"ItemID\"]].copy()\n",
    "y_val_hat[\"Score\"] = XGB_model.predict(\n",
    "    X_val,\n",
    ")\n",
    "y_val_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'UserID': 2379, 'ItemID': [[14748, 14748, 3638, 14748, 3638, 14748]]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_items_dicts = (\n",
    "    pl.from_pandas(y_val_hat[y_val]).group_by(\"UserID\").agg(item_to_list()).to_dicts()\n",
    ")\n",
    "val_items_dicts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ItemID</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37657</th>\n",
       "      <td>0.100747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36493</th>\n",
       "      <td>0.094613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37801</th>\n",
       "      <td>0.093326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36034</th>\n",
       "      <td>0.061902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37017</th>\n",
       "      <td>0.060956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37445</th>\n",
       "      <td>0.056658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36917</th>\n",
       "      <td>0.051948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36773</th>\n",
       "      <td>0.031359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36775</th>\n",
       "      <td>0.012558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37800</th>\n",
       "      <td>0.009141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35753</th>\n",
       "      <td>-0.007819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37193</th>\n",
       "      <td>-0.025015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34998</th>\n",
       "      <td>-0.030370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37759</th>\n",
       "      <td>-0.044457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37896</th>\n",
       "      <td>-0.061464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36779</th>\n",
       "      <td>-0.065797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37507</th>\n",
       "      <td>-0.081411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36774</th>\n",
       "      <td>-0.129330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37897</th>\n",
       "      <td>-0.248943</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Score\n",
       "ItemID          \n",
       "37657   0.100747\n",
       "36493   0.094613\n",
       "37801   0.093326\n",
       "36034   0.061902\n",
       "37017   0.060956\n",
       "37445   0.056658\n",
       "36917   0.051948\n",
       "36773   0.031359\n",
       "36775   0.012558\n",
       "37800   0.009141\n",
       "35753  -0.007819\n",
       "37193  -0.025015\n",
       "34998  -0.030370\n",
       "37759  -0.044457\n",
       "37896  -0.061464\n",
       "36779  -0.065797\n",
       "37507  -0.081411\n",
       "36774  -0.129330\n",
       "37897  -0.248943"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val_hat[y_val_hat[\"UserID\"] == 35735].groupby(\"ItemID\").aggregate({\"Score\": \"mean\"}).sort_values(\"Score\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'UserID': 35735,\n",
       " 'ItemID': [[37657,\n",
       "   36493,\n",
       "   37801,\n",
       "   36034,\n",
       "   37017,\n",
       "   37445,\n",
       "   36917,\n",
       "   36773,\n",
       "   36775,\n",
       "   37800]]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_predictions_dicts = (\n",
    "    pl.from_pandas(y_val_hat)\n",
    "    .group_by(\"UserID\", \"ItemID\")\n",
    "    .agg(pl.mean(\"Score\"))\n",
    "    .sort(\"UserID\", \"Score\", descending=True)\n",
    "    .group_by(\"UserID\")\n",
    "    .head(10)\n",
    "    .group_by(\"UserID\")\n",
    "    .agg(item_to_list())\n",
    "    .to_dicts()\n",
    ")\n",
    "val_predictions_dicts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_predictions_dict = encode_user_dicts(val_predictions_dicts)\n",
    "val_items_dict = encode_user_dicts(val_items_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18314"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_items_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35736"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_predictions_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute recommendation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_at_k(predictions: dict[int, list], true_labels: dict[int, list], k=10):\n",
    "    \"\"\"\n",
    "    Compute Mean Average Precision at K (MAP@K).\n",
    "\n",
    "    Parameters:\n",
    "    - predictions: A list of lists, where each inner list contains the predicted item IDs for a user (ranked in descending order of relevance).\n",
    "    - true_labels: A list of sets, where each set contains the ground-truth relevant item IDs for the corresponding user.\n",
    "    - k: The cutoff for precision evaluation.\n",
    "\n",
    "    Returns:\n",
    "    - mean_ap: Mean Average Precision at K across all users.\n",
    "    \"\"\"\n",
    "    def average_precision_at_k(predicted, actual, k):\n",
    "        if len(predicted) > k:\n",
    "            predicted = predicted[:k]\n",
    "\n",
    "        score = 0.0\n",
    "        num_hits = 0.0\n",
    "\n",
    "        for i, p in enumerate(predicted):\n",
    "            if p in actual and p not in predicted[:i]:  # Avoid duplicates\n",
    "                num_hits += 1.0\n",
    "                score += num_hits / (i + 1.0)\n",
    "\n",
    "        if not actual:\n",
    "            return 0.0\n",
    "\n",
    "        return score / min(len(actual), k)\n",
    "\n",
    "    # Calculate AP@K for each user\n",
    "    ap_scores = [\n",
    "        average_precision_at_k(predictions[user], true, k)\n",
    "        for user, true in true_labels.items()\n",
    "    ]\n",
    "\n",
    "    # Return the mean AP@K\n",
    "    return np.mean(ap_scores) / len(ap_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.558615161058339e-06"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_at_k(val_predictions_dict, val_items_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to perform hyperparameter tuning?\n",
    "\n",
    "The issue with this method is that you need a label which should be an item the user has not interacted with but that is a correct recommendation. In practice the idea is:\n",
    "- Split the data in the usual training-validation-test\n",
    "- Split the training data in two: one part you use to train the recommenders and another you use as the hidden Label to train XGBoost\n",
    "- Evaluate your predictions on the validation data as you did for any other recommender model. Use this to select the optimal hyperparameters.\n",
    "- Given the selected hyperparameters, train the recommender models on all the available data and use all the available data to compute the features used by XGBoost.\n",
    "\n",
    "Challenge: Since the label we use for training XGBoost is the split of a split, it may happen that the actual correct recommendations are very few. This will result in a problem that is very unbalanced towards zero and will make the training difficult and the evaluation noisy. To mitigate this you may use k-fold cross validation and define the valdation result of a certain hyperparameter configuration as the average obtained with k different training-label splits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import optuna\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # Define the hyperparameter search space\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 0.01, 0.3)\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 50, 500)\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 3, 10)\n",
    "    reg_alpha = trial.suggest_float(\"reg_alpha\", 0.0, 1.0)\n",
    "    reg_lambda = trial.suggest_float(\"reg_lambda\", 0.0, 1.0)\n",
    "    subsample = trial.suggest_float(\"subsample\", 0.5, 1.0)\n",
    "    colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.5, 1.0)\n",
    "\n",
    "    # Initialize the XGBoost model\n",
    "    model = XGBRanker(\n",
    "        objective=\"rank:pairwise\",\n",
    "        learning_rate=learning_rate,\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        reg_alpha=reg_alpha,\n",
    "        reg_lambda=reg_lambda,\n",
    "        subsample=subsample,\n",
    "        colsample_bytree=colsample_bytree,\n",
    "    )\n",
    "\n",
    "    # Perform k-fold cross-validation\n",
    "    kf = KFold(n_splits=5)\n",
    "    scores = []\n",
    "\n",
    "    for train_idx, val_idx in kf.split(X_train):\n",
    "        X_train_fold, X_val_fold = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "        groups_train = groups[train_idx]\n",
    "        groups_val = groups[val_idx]\n",
    "\n",
    "        model.fit(\n",
    "            X_train_fold, y_train_fold, group=groups_train, eval_metric=\"ndcg\",\n",
    "            eval_set=[(X_val_fold, y_val_fold)], verbose=False, early_stopping_rounds=10\n",
    "        )\n",
    "        predictions = model.predict(X_val_fold)\n",
    "        score = map(predictions, y_val_fold)  # Replace with your metric\n",
    "        scores.append(score)\n",
    "\n",
    "    return sum(scores) / len(scores)  # Average score over k folds\n",
    "\n",
    "# Run the optimization\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=1)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "print(\"Best parameters:\", best_params)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Recommender</th>\n",
       "      <th>Ranking</th>\n",
       "      <th>recommender_agreement</th>\n",
       "      <th>23</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>20</th>\n",
       "      <th>user_wide_hybrid</th>\n",
       "      <th>item_popularity</th>\n",
       "      <th>item_similarity</th>\n",
       "      <th>user_profile_len</th>\n",
       "      <th>top_10</th>\n",
       "      <th>top_100</th>\n",
       "      <th>top_1000</th>\n",
       "      <th>user_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>6166</td>\n",
       "      <td>user_wide_hybrid</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142024</td>\n",
       "      <td>0.421410</td>\n",
       "      <td>0.353835</td>\n",
       "      <td>0.270525</td>\n",
       "      <td>129</td>\n",
       "      <td>0.000371</td>\n",
       "      <td>114</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>11966</td>\n",
       "      <td>user_wide_hybrid</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.145097</td>\n",
       "      <td>0.267107</td>\n",
       "      <td>0.893614</td>\n",
       "      <td>0.283992</td>\n",
       "      <td>52</td>\n",
       "      <td>0.000568</td>\n",
       "      <td>114</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2743</td>\n",
       "      <td>user_wide_hybrid</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1.452733</td>\n",
       "      <td>0.195061</td>\n",
       "      <td>0.273309</td>\n",
       "      <td>0.521364</td>\n",
       "      <td>0.284803</td>\n",
       "      <td>39</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>114</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2637</td>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.197307</td>\n",
       "      <td>0.381694</td>\n",
       "      <td>0.790003</td>\n",
       "      <td>0.218590</td>\n",
       "      <td>67</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>114</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>738</td>\n",
       "      <td>22</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1.694155</td>\n",
       "      <td>0.103107</td>\n",
       "      <td>0.370141</td>\n",
       "      <td>0.318969</td>\n",
       "      <td>0.093946</td>\n",
       "      <td>29</td>\n",
       "      <td>0.000474</td>\n",
       "      <td>114</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1786795</th>\n",
       "      <td>35735</td>\n",
       "      <td>36775</td>\n",
       "      <td>user_wide_hybrid</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>8.471791</td>\n",
       "      <td>0.328802</td>\n",
       "      <td>0.358924</td>\n",
       "      <td>2.157352</td>\n",
       "      <td>0.664134</td>\n",
       "      <td>88</td>\n",
       "      <td>0.003954</td>\n",
       "      <td>37</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1786796</th>\n",
       "      <td>35735</td>\n",
       "      <td>37445</td>\n",
       "      <td>user_wide_hybrid</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>7.411239</td>\n",
       "      <td>0.408742</td>\n",
       "      <td>0.455056</td>\n",
       "      <td>3.137702</td>\n",
       "      <td>0.680556</td>\n",
       "      <td>27</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>37</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1786797</th>\n",
       "      <td>35735</td>\n",
       "      <td>36917</td>\n",
       "      <td>user_wide_hybrid</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>8.380329</td>\n",
       "      <td>0.453475</td>\n",
       "      <td>0.383239</td>\n",
       "      <td>2.259961</td>\n",
       "      <td>0.839466</td>\n",
       "      <td>76</td>\n",
       "      <td>0.004296</td>\n",
       "      <td>37</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1786798</th>\n",
       "      <td>35735</td>\n",
       "      <td>36034</td>\n",
       "      <td>user_wide_hybrid</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>10.576296</td>\n",
       "      <td>0.430645</td>\n",
       "      <td>0.403139</td>\n",
       "      <td>2.485576</td>\n",
       "      <td>1.015675</td>\n",
       "      <td>81</td>\n",
       "      <td>0.000403</td>\n",
       "      <td>37</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1786799</th>\n",
       "      <td>35735</td>\n",
       "      <td>36493</td>\n",
       "      <td>user_wide_hybrid</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>11.521591</td>\n",
       "      <td>0.512244</td>\n",
       "      <td>0.614241</td>\n",
       "      <td>2.720541</td>\n",
       "      <td>1.047983</td>\n",
       "      <td>33</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>37</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000272</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1786800 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        UserID ItemID       Recommender  Ranking  recommender_agreement  \\\n",
       "0            0   6166  user_wide_hybrid        7                      2   \n",
       "1            0  11966  user_wide_hybrid        6                      1   \n",
       "2            0   2743  user_wide_hybrid        5                      1   \n",
       "3            0   2637                22        8                      1   \n",
       "4            0    738                22        9                      1   \n",
       "...        ...    ...               ...      ...                    ...   \n",
       "1786795  35735  36775  user_wide_hybrid        5                      3   \n",
       "1786796  35735  37445  user_wide_hybrid        4                      5   \n",
       "1786797  35735  36917  user_wide_hybrid        3                      5   \n",
       "1786798  35735  36034  user_wide_hybrid        2                      5   \n",
       "1786799  35735  36493  user_wide_hybrid        1                      5   \n",
       "\n",
       "                23        21        22        20  user_wide_hybrid  \\\n",
       "0         0.000000  0.142024  0.421410  0.353835          0.270525   \n",
       "1         0.000000  0.145097  0.267107  0.893614          0.283992   \n",
       "2         1.452733  0.195061  0.273309  0.521364          0.284803   \n",
       "3         0.000000  0.197307  0.381694  0.790003          0.218590   \n",
       "4         1.694155  0.103107  0.370141  0.318969          0.093946   \n",
       "...            ...       ...       ...       ...               ...   \n",
       "1786795   8.471791  0.328802  0.358924  2.157352          0.664134   \n",
       "1786796   7.411239  0.408742  0.455056  3.137702          0.680556   \n",
       "1786797   8.380329  0.453475  0.383239  2.259961          0.839466   \n",
       "1786798  10.576296  0.430645  0.403139  2.485576          1.015675   \n",
       "1786799  11.521591  0.512244  0.614241  2.720541          1.047983   \n",
       "\n",
       "         item_popularity  item_similarity  user_profile_len  top_10  top_100  \\\n",
       "0                    129         0.000371               114     0.0      1.0   \n",
       "1                     52         0.000568               114     0.0      1.0   \n",
       "2                     39         0.000069               114     0.0      1.0   \n",
       "3                     67         0.000177               114     0.0      1.0   \n",
       "4                     29         0.000474               114     0.0      1.0   \n",
       "...                  ...              ...               ...     ...      ...   \n",
       "1786795               88         0.003954                37     0.0      0.0   \n",
       "1786796               27         0.000132                37     0.0      0.0   \n",
       "1786797               76         0.004296                37     0.0      0.0   \n",
       "1786798               81         0.000403                37     0.0      0.0   \n",
       "1786799               33         0.000083                37     0.0      0.0   \n",
       "\n",
       "         top_1000  user_similarity  \n",
       "0             7.0         0.000183  \n",
       "1             7.0         0.000183  \n",
       "2             7.0         0.000183  \n",
       "3             7.0         0.000183  \n",
       "4             7.0         0.000183  \n",
       "...           ...              ...  \n",
       "1786795       0.0         0.000272  \n",
       "1786796       0.0         0.000272  \n",
       "1786797       0.0         0.000272  \n",
       "1786798       0.0         0.000272  \n",
       "1786799       0.0         0.000272  \n",
       "\n",
       "[1786800 rows x 17 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_training_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Recommender</th>\n",
       "      <th>Ranking</th>\n",
       "      <th>recommender_agreement</th>\n",
       "      <th>23</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>20</th>\n",
       "      <th>user_wide_hybrid</th>\n",
       "      <th>item_popularity</th>\n",
       "      <th>item_similarity</th>\n",
       "      <th>user_profile_len</th>\n",
       "      <th>top_10</th>\n",
       "      <th>top_100</th>\n",
       "      <th>top_1000</th>\n",
       "      <th>user_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>6166</td>\n",
       "      <td>user_wide_hybrid</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142024</td>\n",
       "      <td>0.421410</td>\n",
       "      <td>0.353835</td>\n",
       "      <td>0.270525</td>\n",
       "      <td>129</td>\n",
       "      <td>0.000371</td>\n",
       "      <td>114</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>11966</td>\n",
       "      <td>user_wide_hybrid</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.145097</td>\n",
       "      <td>0.267107</td>\n",
       "      <td>0.893614</td>\n",
       "      <td>0.283992</td>\n",
       "      <td>52</td>\n",
       "      <td>0.000568</td>\n",
       "      <td>114</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2743</td>\n",
       "      <td>user_wide_hybrid</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1.452733</td>\n",
       "      <td>0.195061</td>\n",
       "      <td>0.273309</td>\n",
       "      <td>0.521364</td>\n",
       "      <td>0.284803</td>\n",
       "      <td>39</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>114</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2637</td>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.197307</td>\n",
       "      <td>0.381694</td>\n",
       "      <td>0.790003</td>\n",
       "      <td>0.218590</td>\n",
       "      <td>67</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>114</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>738</td>\n",
       "      <td>22</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1.694155</td>\n",
       "      <td>0.103107</td>\n",
       "      <td>0.370141</td>\n",
       "      <td>0.318969</td>\n",
       "      <td>0.093946</td>\n",
       "      <td>29</td>\n",
       "      <td>0.000474</td>\n",
       "      <td>114</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1786795</th>\n",
       "      <td>35735</td>\n",
       "      <td>36775</td>\n",
       "      <td>user_wide_hybrid</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>8.471791</td>\n",
       "      <td>0.328802</td>\n",
       "      <td>0.358924</td>\n",
       "      <td>2.157352</td>\n",
       "      <td>0.664134</td>\n",
       "      <td>88</td>\n",
       "      <td>0.003954</td>\n",
       "      <td>37</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1786796</th>\n",
       "      <td>35735</td>\n",
       "      <td>37445</td>\n",
       "      <td>user_wide_hybrid</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>7.411239</td>\n",
       "      <td>0.408742</td>\n",
       "      <td>0.455056</td>\n",
       "      <td>3.137702</td>\n",
       "      <td>0.680556</td>\n",
       "      <td>27</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>37</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1786797</th>\n",
       "      <td>35735</td>\n",
       "      <td>36917</td>\n",
       "      <td>user_wide_hybrid</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>8.380329</td>\n",
       "      <td>0.453475</td>\n",
       "      <td>0.383239</td>\n",
       "      <td>2.259961</td>\n",
       "      <td>0.839466</td>\n",
       "      <td>76</td>\n",
       "      <td>0.004296</td>\n",
       "      <td>37</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1786798</th>\n",
       "      <td>35735</td>\n",
       "      <td>36034</td>\n",
       "      <td>user_wide_hybrid</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>10.576296</td>\n",
       "      <td>0.430645</td>\n",
       "      <td>0.403139</td>\n",
       "      <td>2.485576</td>\n",
       "      <td>1.015675</td>\n",
       "      <td>81</td>\n",
       "      <td>0.000403</td>\n",
       "      <td>37</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1786799</th>\n",
       "      <td>35735</td>\n",
       "      <td>36493</td>\n",
       "      <td>user_wide_hybrid</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>11.521591</td>\n",
       "      <td>0.512244</td>\n",
       "      <td>0.614241</td>\n",
       "      <td>2.720541</td>\n",
       "      <td>1.047983</td>\n",
       "      <td>33</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>37</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000272</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1786800 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        UserID ItemID       Recommender  Ranking  recommender_agreement  \\\n",
       "0            0   6166  user_wide_hybrid        7                      2   \n",
       "1            0  11966  user_wide_hybrid        6                      1   \n",
       "2            0   2743  user_wide_hybrid        5                      1   \n",
       "3            0   2637                22        8                      1   \n",
       "4            0    738                22        9                      1   \n",
       "...        ...    ...               ...      ...                    ...   \n",
       "1786795  35735  36775  user_wide_hybrid        5                      3   \n",
       "1786796  35735  37445  user_wide_hybrid        4                      5   \n",
       "1786797  35735  36917  user_wide_hybrid        3                      5   \n",
       "1786798  35735  36034  user_wide_hybrid        2                      5   \n",
       "1786799  35735  36493  user_wide_hybrid        1                      5   \n",
       "\n",
       "                23        21        22        20  user_wide_hybrid  \\\n",
       "0         0.000000  0.142024  0.421410  0.353835          0.270525   \n",
       "1         0.000000  0.145097  0.267107  0.893614          0.283992   \n",
       "2         1.452733  0.195061  0.273309  0.521364          0.284803   \n",
       "3         0.000000  0.197307  0.381694  0.790003          0.218590   \n",
       "4         1.694155  0.103107  0.370141  0.318969          0.093946   \n",
       "...            ...       ...       ...       ...               ...   \n",
       "1786795   8.471791  0.328802  0.358924  2.157352          0.664134   \n",
       "1786796   7.411239  0.408742  0.455056  3.137702          0.680556   \n",
       "1786797   8.380329  0.453475  0.383239  2.259961          0.839466   \n",
       "1786798  10.576296  0.430645  0.403139  2.485576          1.015675   \n",
       "1786799  11.521591  0.512244  0.614241  2.720541          1.047983   \n",
       "\n",
       "         item_popularity  item_similarity  user_profile_len  top_10  top_100  \\\n",
       "0                    129         0.000371               114     0.0      1.0   \n",
       "1                     52         0.000568               114     0.0      1.0   \n",
       "2                     39         0.000069               114     0.0      1.0   \n",
       "3                     67         0.000177               114     0.0      1.0   \n",
       "4                     29         0.000474               114     0.0      1.0   \n",
       "...                  ...              ...               ...     ...      ...   \n",
       "1786795               88         0.003954                37     0.0      0.0   \n",
       "1786796               27         0.000132                37     0.0      0.0   \n",
       "1786797               76         0.004296                37     0.0      0.0   \n",
       "1786798               81         0.000403                37     0.0      0.0   \n",
       "1786799               33         0.000083                37     0.0      0.0   \n",
       "\n",
       "         top_1000  user_similarity  \n",
       "0             7.0         0.000183  \n",
       "1             7.0         0.000183  \n",
       "2             7.0         0.000183  \n",
       "3             7.0         0.000183  \n",
       "4             7.0         0.000183  \n",
       "...           ...              ...  \n",
       "1786795       0.0         0.000272  \n",
       "1786796       0.0         0.000272  \n",
       "1786797       0.0         0.000272  \n",
       "1786798       0.0         0.000272  \n",
       "1786799       0.0         0.000272  \n",
       "\n",
       "[1786800 rows x 17 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_training_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Recommender</th>\n",
       "      <th>Ranking</th>\n",
       "      <th>recommender_agreement</th>\n",
       "      <th>23</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>20</th>\n",
       "      <th>user_wide_hybrid</th>\n",
       "      <th>item_popularity</th>\n",
       "      <th>item_similarity</th>\n",
       "      <th>user_profile_len</th>\n",
       "      <th>top_10</th>\n",
       "      <th>top_100</th>\n",
       "      <th>top_1000</th>\n",
       "      <th>user_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>user_wide_hybrid</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142024</td>\n",
       "      <td>0.421410</td>\n",
       "      <td>0.353835</td>\n",
       "      <td>0.270525</td>\n",
       "      <td>129</td>\n",
       "      <td>0.000371</td>\n",
       "      <td>114</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>user_wide_hybrid</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.145097</td>\n",
       "      <td>0.267107</td>\n",
       "      <td>0.893614</td>\n",
       "      <td>0.283992</td>\n",
       "      <td>52</td>\n",
       "      <td>0.000568</td>\n",
       "      <td>114</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>user_wide_hybrid</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1.452733</td>\n",
       "      <td>0.195061</td>\n",
       "      <td>0.273309</td>\n",
       "      <td>0.521364</td>\n",
       "      <td>0.284803</td>\n",
       "      <td>39</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>114</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.197307</td>\n",
       "      <td>0.381694</td>\n",
       "      <td>0.790003</td>\n",
       "      <td>0.218590</td>\n",
       "      <td>67</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>114</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1.694155</td>\n",
       "      <td>0.103107</td>\n",
       "      <td>0.370141</td>\n",
       "      <td>0.318969</td>\n",
       "      <td>0.093946</td>\n",
       "      <td>29</td>\n",
       "      <td>0.000474</td>\n",
       "      <td>114</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1786795</th>\n",
       "      <td>user_wide_hybrid</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>8.471791</td>\n",
       "      <td>0.328802</td>\n",
       "      <td>0.358924</td>\n",
       "      <td>2.157352</td>\n",
       "      <td>0.664134</td>\n",
       "      <td>88</td>\n",
       "      <td>0.003954</td>\n",
       "      <td>37</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1786796</th>\n",
       "      <td>user_wide_hybrid</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>7.411239</td>\n",
       "      <td>0.408742</td>\n",
       "      <td>0.455056</td>\n",
       "      <td>3.137702</td>\n",
       "      <td>0.680556</td>\n",
       "      <td>27</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>37</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1786797</th>\n",
       "      <td>user_wide_hybrid</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>8.380329</td>\n",
       "      <td>0.453475</td>\n",
       "      <td>0.383239</td>\n",
       "      <td>2.259961</td>\n",
       "      <td>0.839466</td>\n",
       "      <td>76</td>\n",
       "      <td>0.004296</td>\n",
       "      <td>37</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1786798</th>\n",
       "      <td>user_wide_hybrid</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>10.576296</td>\n",
       "      <td>0.430645</td>\n",
       "      <td>0.403139</td>\n",
       "      <td>2.485576</td>\n",
       "      <td>1.015675</td>\n",
       "      <td>81</td>\n",
       "      <td>0.000403</td>\n",
       "      <td>37</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1786799</th>\n",
       "      <td>user_wide_hybrid</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>11.521591</td>\n",
       "      <td>0.512244</td>\n",
       "      <td>0.614241</td>\n",
       "      <td>2.720541</td>\n",
       "      <td>1.047983</td>\n",
       "      <td>33</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>37</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000272</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1786800 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Recommender  Ranking  recommender_agreement         23  \\\n",
       "0        user_wide_hybrid        7                      2   0.000000   \n",
       "1        user_wide_hybrid        6                      1   0.000000   \n",
       "2        user_wide_hybrid        5                      1   1.452733   \n",
       "3                      22        8                      1   0.000000   \n",
       "4                      22        9                      1   1.694155   \n",
       "...                   ...      ...                    ...        ...   \n",
       "1786795  user_wide_hybrid        5                      3   8.471791   \n",
       "1786796  user_wide_hybrid        4                      5   7.411239   \n",
       "1786797  user_wide_hybrid        3                      5   8.380329   \n",
       "1786798  user_wide_hybrid        2                      5  10.576296   \n",
       "1786799  user_wide_hybrid        1                      5  11.521591   \n",
       "\n",
       "               21        22        20  user_wide_hybrid  item_popularity  \\\n",
       "0        0.142024  0.421410  0.353835          0.270525              129   \n",
       "1        0.145097  0.267107  0.893614          0.283992               52   \n",
       "2        0.195061  0.273309  0.521364          0.284803               39   \n",
       "3        0.197307  0.381694  0.790003          0.218590               67   \n",
       "4        0.103107  0.370141  0.318969          0.093946               29   \n",
       "...           ...       ...       ...               ...              ...   \n",
       "1786795  0.328802  0.358924  2.157352          0.664134               88   \n",
       "1786796  0.408742  0.455056  3.137702          0.680556               27   \n",
       "1786797  0.453475  0.383239  2.259961          0.839466               76   \n",
       "1786798  0.430645  0.403139  2.485576          1.015675               81   \n",
       "1786799  0.512244  0.614241  2.720541          1.047983               33   \n",
       "\n",
       "         item_similarity  user_profile_len  top_10  top_100  top_1000  \\\n",
       "0               0.000371               114     0.0      1.0       7.0   \n",
       "1               0.000568               114     0.0      1.0       7.0   \n",
       "2               0.000069               114     0.0      1.0       7.0   \n",
       "3               0.000177               114     0.0      1.0       7.0   \n",
       "4               0.000474               114     0.0      1.0       7.0   \n",
       "...                  ...               ...     ...      ...       ...   \n",
       "1786795         0.003954                37     0.0      0.0       0.0   \n",
       "1786796         0.000132                37     0.0      0.0       0.0   \n",
       "1786797         0.004296                37     0.0      0.0       0.0   \n",
       "1786798         0.000403                37     0.0      0.0       0.0   \n",
       "1786799         0.000083                37     0.0      0.0       0.0   \n",
       "\n",
       "         user_similarity  \n",
       "0               0.000183  \n",
       "1               0.000183  \n",
       "2               0.000183  \n",
       "3               0.000183  \n",
       "4               0.000183  \n",
       "...                  ...  \n",
       "1786795         0.000272  \n",
       "1786796         0.000272  \n",
       "1786797         0.000272  \n",
       "1786798         0.000272  \n",
       "1786799         0.000272  \n",
       "\n",
       "[1786800 rows x 15 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_training_dataframe.drop(columns=[\"UserID\", \"ItemID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{35735: [37657, 36493, 36773, 36917, 36034, 37445, 37660, 36920, 37865, 36775],\n",
       " 35734: [37069, 36610, 36168, 35345, 37550, 37067, 35093, 36880, 37803, 36094],\n",
       " 35733: [37853, 37388, 37372, 36056, 38072, 37540, 37858, 36676, 28418, 28304],\n",
       " 35732: [27590, 37317, 27644, 31350, 38005, 33330, 37402, 27814, 37109, 36919],\n",
       " 35731: [37739, 36263, 38027, 37427, 36525, 37623, 28119, 35394, 36856, 27349],\n",
       " 35730: [28247, 38027, 37211, 37420, 37739, 37874, 27350, 28119, 37719, 37631],\n",
       " 35729: [36844, 26093, 24415, 24417, 35548, 36527, 26581, 27531, 37461, 27067],\n",
       " 35728: [35370, 35777, 36665, 14090, 19651, 28374, 37565, 27497, 27510, 36346],\n",
       " 35727: [37280, 27520, 28088, 28203, 37416, 37291, 37712, 28097, 14664, 27693],\n",
       " 35726: [37851, 37940, 37527, 28447, 27768, 37740, 28234, 36866, 38045, 37510],\n",
       " 35725: [36652, 37540, 28458, 37431, 37661, 37785, 38018, 28547, 37924, 27623],\n",
       " 35724: [28281, 37178, 37940, 36866, 27346, 36504, 26869, 27689, 27756, 37461],\n",
       " 35723: [28461, 28247, 36065, 37739, 36567, 27161, 37211, 28424, 37899, 27662],\n",
       " 35722: [37731, 27593, 37902, 27364, 35269, 36063, 27720, 27485, 20545, 36718],\n",
       " 35721: [37958, 36027, 37983, 20107, 37120, 37785, 35906, 36016, 38018, 28418],\n",
       " 35720: [33877, 28165, 35531, 37372, 20106, 36346, 35882, 35162, 37958, 36676],\n",
       " 35719: [26188, 27505, 26609, 26225, 27772, 27374, 35904, 28306, 37498, 36929],\n",
       " 35718: [28164, 34482, 37158, 27828, 26917, 35566, 37225, 35561, 27416, 37666],\n",
       " 35717: [35951, 35584, 31577, 28547, 27623, 26932, 37177, 37124, 37222, 26688],\n",
       " 35716: [28419, 27134, 26795, 35278, 35219, 27807, 27604, 27751, 20331, 34174],\n",
       " 35715: [37910, 38112, 27638, 28133, 26351, 28241, 37105, 34775, 36397, 26105],\n",
       " 35714: [27807, 27720, 35374, 26424, 35561, 26922, 26330, 37433, 28334, 28288],\n",
       " 35713: [31336, 27045, 27815, 19675, 32257, 33919, 32226, 25750, 26001, 32402],\n",
       " 35712: [36806, 36579, 32572, 26357, 36974, 38115, 27338, 28110, 24704, 26963],\n",
       " 35711: [30374, 34421, 35218, 35016, 36467, 14673, 27172, 14462, 26761, 26811],\n",
       " 35710: [36902, 26848, 34426, 34119, 22822, 28012, 28162, 36826, 36595, 35017],\n",
       " 35709: [28162, 26475, 33617, 37985, 34426, 37637, 35945, 35017, 31381, 30908],\n",
       " 35708: [26794, 27106, 27067, 25415, 28025, 27655, 28020, 27583, 28292, 36802],\n",
       " 35707: [34983, 26851, 2474, 25591, 37604, 34985, 27675, 26609, 36241, 26263],\n",
       " 35706: [33570, 34254, 37160, 37870, 37884, 35960, 11040, 37259, 35612, 27720],\n",
       " 35705: [36844, 26794, 36439, 25417, 26302, 25436, 37510, 23844, 37740, 28532],\n",
       " 35704: [28273, 25415, 37880, 27784, 28513, 27709, 27580, 27423, 36001, 37872],\n",
       " 35703: [27580, 31020, 28306, 20576, 27671, 25639, 27047, 27443, 28226, 25640],\n",
       " 35702: [28262, 37913, 27720, 37527, 25412, 38107, 25406, 25369, 25414, 38066],\n",
       " 35701: [28470, 37731, 27852, 28521, 31947, 28094, 25415, 20479, 27513, 37607],\n",
       " 35700: [27859, 36438, 25418, 31045, 35254, 32063, 25420, 26682, 28110, 28001],\n",
       " 35699: [36397, 27205, 33928, 27485, 37105, 34318, 33743, 28177, 34110, 27357],\n",
       " 35698: [35438, 36676, 36755, 35187, 36249, 35650, 26356, 36346, 36940, 35876],\n",
       " 35697: [35218, 34421, 35654, 35016, 14673, 26242, 14462, 35151, 33126, 35240],\n",
       " 35696: [37382, 31751, 32165, 25784, 27027, 26099, 28025, 7964, 37733, 24915],\n",
       " 35695: [34833, 27650, 34848, 25795, 36330, 34509, 36813, 33214, 24188, 14343],\n",
       " 35694: [38027, 27426, 37211, 37874, 35995, 28119, 37321, 28351, 36065, 36263],\n",
       " 35693: [34201, 24936, 25249, 20289, 27216, 24922, 24935, 32537, 36025, 33479],\n",
       " 35692: [35932, 36991, 33825, 28519, 36297, 27008, 34425, 26356, 35125, 27024],\n",
       " 35691: [27972, 20399, 26471, 25046, 35799, 37001, 26972, 28100, 27135, 27423],\n",
       " 35690: [34645, 36635, 36701, 36548, 36055, 32572, 26429, 35099, 26646, 28110],\n",
       " 35689: [14343, 37925, 37541, 24912, 27607, 28361, 27328, 24755, 36900, 37250],\n",
       " 35688: [27759, 28432, 36008, 36144, 37565, 36645, 26932, 35187, 26390, 36665],\n",
       " 35687: [28323, 28479, 28438, 28466, 35428, 31874, 31875, 27537, 26256, 33263],\n",
       " 35686: [37712, 28413, 37864, 23712, 25077, 27165, 25352, 28192, 27912, 28129],\n",
       " 35685: [24878, 26797, 34873, 34425, 28331, 34362, 35404, 36298, 34732, 35169],\n",
       " 35684: [24878, 26430, 36980, 26107, 27008, 35988, 34425, 33825, 28064, 26397],\n",
       " 35683: [36544, 36665, 24629, 25918, 33785, 36452, 35370, 37372, 35989, 26906],\n",
       " 35682: [25968, 28104, 25863, 27290, 27348, 20175, 36532, 20176, 37816, 26930],\n",
       " 35681: [36355, 20397, 37108, 37711, 36069, 37578, 14681, 28026, 36397, 27317],\n",
       " 35680: [36296, 35960, 32380, 14681, 32001, 31876, 31877, 26113, 35151, 31875],\n",
       " 35679: [28088, 26113, 28442, 38045, 27693, 28441, 37331, 37515, 38109, 37202],\n",
       " 35678: [26217, 25486, 34543, 26394, 28229, 26553, 26751, 26453, 26170, 37541],\n",
       " 35677: [27689, 24186, 28561, 36866, 27106, 32146, 28025, 37510, 35548, 37287],\n",
       " 35676: [25295, 25570, 34694, 26217, 32314, 34684, 26868, 27796, 34543, 26553],\n",
       " 35675: [30927, 37840, 28416, 28306, 26225, 35647, 25353, 26827, 27030, 34317],\n",
       " 35674: [25179, 34211, 36536, 26971, 27008, 25210, 25500, 25119, 35077, 36433],\n",
       " 35673: [24500, 27739, 37268, 37343, 34211, 36544, 36249, 34892, 35041, 25496],\n",
       " 35672: [32718, 20498, 25440, 33263, 33284, 33003, 8095, 8072, 7964, 20063],\n",
       " 35671: [26875, 26827, 27443, 25435, 20305, 25234, 27913, 20744, 26471, 26609],\n",
       " 35670: [23846, 36439, 23841, 23712, 25418, 25419, 25426, 25411, 25435, 20305],\n",
       " 35669: [30169, 34167, 37984, 27783, 27626, 37913, 27067, 2476, 11128, 25718],\n",
       " 35668: [25286, 33791, 25465, 25466, 25301, 31873, 25136, 33766, 26652, 32754],\n",
       " 35667: [34795, 28519, 35370, 36144, 36893, 34290, 36536, 36357, 26698, 35397],\n",
       " 35666: [34543, 24773, 35414, 37186, 27128, 28501, 34790, 25055, 27800, 37319],\n",
       " 35665: [26078, 35216, 10564, 34135, 28298, 32714, 31842, 35570, 35473, 31614],\n",
       " 35664: [24771, 32843, 34118, 22903, 32890, 33734, 36344, 36002, 30925, 34340],\n",
       " 35663: [30914, 25565, 26513, 22819, 22870, 33159, 31325, 33220, 22896, 30885],\n",
       " 35662: [34206, 27579, 26809, 27575, 27655, 36738, 36083, 37940, 27345, 33297],\n",
       " 35661: [34790, 37379, 24538, 24384, 27424, 36595, 26762, 25055, 34629, 24408],\n",
       " 35660: [26875, 26113, 20535, 26298, 19508, 31811, 25352, 31803, 11134, 26007],\n",
       " 35659: [22067, 22217, 22208, 22190, 35386, 22188, 22154, 22200, 22196, 22179],\n",
       " 35658: [31021, 25436, 37041, 27225, 26712, 27067, 26424, 14579, 26950, 25415],\n",
       " 35657: [37108, 38042, 27110, 35784, 22825, 35847, 28219, 28427, 33904, 26848],\n",
       " 35656: [31846, 27575, 31849, 34602, 27577, 24774, 36370, 28551, 25392, 27300],\n",
       " 35655: [27502, 25106, 34770, 34253, 24022, 25732, 33908, 28315, 28323, 27537],\n",
       " 35654: [31999, 34324, 32063, 28110, 31998, 33850, 33319, 25136, 35581, 26963],\n",
       " 35653: [25486, 25208, 26868, 25565, 27800, 25246, 25243, 27426, 26202, 32778],\n",
       " 35652: [26254, 20574, 28065, 28226, 27231, 37341, 21095, 36453, 28137, 20575],\n",
       " 35651: [14652, 35012, 34415, 34212, 28251, 20488, 37994, 36635, 27485, 28014],\n",
       " 35650: [25426, 25392, 37592, 27844, 28226, 28415, 28198, 28200, 25699, 36920],\n",
       " 35649: [30828, 30827, 24388, 26501, 27509, 23847, 30955, 24993, 23840, 23841],\n",
       " 35648: [27318, 19618, 35096, 37985, 23845, 36612, 36468, 31657, 27580, 26914],\n",
       " 35647: [27611, 31020, 37432, 27844, 25392, 25426, 36612, 36223, 14579, 20575],\n",
       " 35646: [35897, 26134, 26639, 28285, 34849, 26680, 35530, 25572, 28373, 25571],\n",
       " 35645: [36906, 34747, 28470, 14620, 37535, 28442, 36355, 26886, 28357, 37527],\n",
       " 35644: [27578, 37622, 30855, 36876, 11135, 20488, 37960, 20712, 27989, 28530],\n",
       " 35643: [36806, 37515, 28192, 37527, 28353, 28097, 28100, 11135, 28428, 27732],\n",
       " 35642: [24182, 37775, 37108, 37573, 37913, 27861, 38069, 28154, 28490, 27713],\n",
       " 35641: [36157, 28100, 28213, 33965, 35647, 27917, 28203, 27732, 34674, 27231],\n",
       " 35640: [28053, 20766, 14621, 37341, 27208, 36064, 27690, 38006, 28245, 31877],\n",
       " 35639: [26038, 25570, 33277, 32624, 33540, 25225, 7789, 32673, 33382, 12335],\n",
       " 35638: [36806, 31885, 37168, 31887, 28100, 37416, 37291, 28129, 37871, 28192],\n",
       " 35637: [28101, 28353, 25224, 25716, 28395, 38033, 34658, 35302, 27288, 27310],\n",
       " 35636: [38016, 37689, 37653, 28441, 31036, 31037, 36502, 30326, 20776, 19720],\n",
       " 35635: [34841, 28414, 32071, 27989, 28354, 28464, 28198, 20574, 25406, 27544],\n",
       " 35634: [27111, 20739, 20574, 27709, 33724, 20575, 31181, 36087, 33698, 27172],\n",
       " 35633: [26131, 37132, 20766, 14654, 36832, 20763, 37919, 37712, 27539, 37416],\n",
       " 35632: [26785, 36468, 28100, 27190, 27732, 27520, 27745, 28397, 27423, 25420],\n",
       " 35631: [27675, 37984, 28158, 20488, 20717, 28200, 37009, 20747, 38092, 38069],\n",
       " 35630: [37622, 27709, 37713, 28129, 28020, 37930, 37424, 11135, 20388, 11134],\n",
       " 35629: [37280, 27917, 27732, 38100, 28353, 28100, 37507, 28020, 36448, 28464],\n",
       " 35628: [32916, 27091, 20487, 20488, 37667, 30650, 27989, 34129, 20205, 31012],\n",
       " 35627: [37768, 38092, 37667, 37721, 20488, 30961, 20487, 30963, 32081, 31012],\n",
       " 35626: [37209, 35845, 26730, 38007, 32104, 32000, 32114, 37014, 34349, 37646],\n",
       " 35625: [28252, 25415, 19938, 36999, 19939, 28097, 38115, 14709, 37712, 37026],\n",
       " 35624: [27423, 28100, 31975, 20488, 28353, 27989, 31974, 37667, 38092, 31973],\n",
       " 35623: [25039, 24455, 24382, 34963, 26647, 24045, 36005, 24189, 36489, 33949],\n",
       " 35622: [35591, 27696, 27963, 36129, 36622, 20740, 28500, 37198, 26851, 27917],\n",
       " 35621: [14621, 28158, 27092, 37774, 20632, 28306, 27693, 28550, 35012, 35444],\n",
       " 35620: [37930, 27583, 38116, 20740, 28448, 31584, 37497, 31580, 28004, 27784],\n",
       " 35619: [37491, 20740, 28318, 38022, 37341, 36943, 30954, 37550, 36129, 35441],\n",
       " 35618: [27193, 37101, 28306, 32055, 37329, 28097, 28397, 27917, 24181, 36308],\n",
       " 35617: [37893, 26780, 35520, 14655, 27841, 35521, 36463, 37241, 37904, 28368],\n",
       " 35616: [36361, 28359, 35253, 14709, 37712, 37257, 27768, 20677, 28176, 37855],\n",
       " 35615: [31581, 31583, 36877, 37101, 32069, 20305, 14612, 31585, 19980, 20728],\n",
       " 35614: [36645, 35900, 23893, 36231, 26016, 35468, 34783, 35077, 36830, 35584],\n",
       " 35613: [37527, 37713, 36569, 14654, 35708, 27208, 26937, 20576, 20575, 28343],\n",
       " 35612: [36427, 31876, 28197, 28353, 28337, 26909, 25435, 27047, 26682, 35012],\n",
       " 35611: [37574, 27376, 37362, 36541, 14657, 36409, 37828, 31039, 28443, 27784],\n",
       " 35610: [32050, 37813, 26302, 27298, 38045, 35160, 25076, 37902, 26923, 35790],\n",
       " 35609: [20634, 30503, 20635, 25699, 37865, 28540, 27437, 37910, 28562, 37942],\n",
       " 35608: [27769, 31765, 27236, 27374, 37774, 27278, 27242, 37053, 28169, 37022],\n",
       " 35607: [28353, 36047, 14621, 28205, 27812, 27805, 28549, 27543, 38030, 31977],\n",
       " 35606: [28349, 27963, 27607, 27935, 28476, 27709, 20632, 27304, 28353, 36107],\n",
       " 35605: [25518, 26885, 25606, 24412, 31847, 27105, 31967, 34819, 27122, 34261],\n",
       " 35604: [26727, 35556, 37886, 25415, 36006, 35308, 38117, 36488, 25431, 27134],\n",
       " 35603: [27923, 27092, 27221, 27784, 36108, 25411, 20633, 35521, 28353, 36937],\n",
       " 35602: [28246, 24179, 20635, 25363, 36260, 37526, 28008, 20124, 34906, 20125],\n",
       " 35601: [28065, 35664, 27154, 36397, 20696, 36570, 27647, 36157, 27472, 26810],\n",
       " 35600: [29856, 25566, 31963, 36825, 31720, 31721, 28110, 31562, 31608, 31723],\n",
       " 35599: [37108, 26298, 36658, 25039, 36149, 27809, 37026, 27424, 22871, 27601],\n",
       " 35598: [28405, 37515, 38039, 20575, 20574, 31033, 31034, 31031, 27978, 31583],\n",
       " 35597: [36876, 37998, 37041, 20574, 20575, 27709, 37895, 28260, 28200, 37880],\n",
       " 35596: [31975, 28504, 37223, 28100, 20575, 19986, 20574, 37329, 38102, 27709],\n",
       " 35595: [27190, 20575, 20305, 28353, 37930, 28265, 27917, 25594, 20574, 27732],\n",
       " 35594: [36397, 28474, 28065, 36004, 27176, 27295, 31980, 28395, 36570, 28097],\n",
       " 35593: [27706, 37689, 36950, 37515, 27959, 36832, 37310, 37527, 27859, 36004],\n",
       " 35592: [27831, 28100, 28456, 20633, 28306, 20575, 27616, 28137, 20574, 20632],\n",
       " 35591: [27677, 21728, 27709, 27784, 27690, 25423, 21729, 20575, 28353, 37902],\n",
       " 35590: [27791, 36570, 27579, 26254, 27696, 31020, 27262, 28065, 26302, 36069],\n",
       " 35589: [36110, 37515, 37836, 20574, 37910, 34902, 28226, 25369, 14654, 28442],\n",
       " 35588: [14654, 25416, 25343, 25560, 25436, 24417, 24416, 21096, 20724, 25431],\n",
       " 35587: [28474, 36123, 37864, 28226, 27978, 37895, 28273, 27495, 21094, 27852],\n",
       " 35586: [28490, 28466, 20575, 28438, 36694, 28472, 37984, 31020, 28077, 38107],\n",
       " 35585: [20576, 25591, 25435, 27917, 21095, 25560, 25417, 25416, 25418, 27701],\n",
       " 35584: [20634, 26818, 31961, 31962, 37527, 27167, 35715, 31720, 37524, 30168],\n",
       " 35583: [37913, 37290, 37592, 25416, 27812, 28097, 27878, 21096, 25415, 38107],\n",
       " 35582: [28252, 27356, 27706, 29593, 24415, 27090, 26298, 27231, 25431, 37813],\n",
       " 35581: [28363, 20574, 14664, 36957, 37417, 28096, 37424, 36663, 27601, 36751],\n",
       " 35580: [35524, 20575, 26533, 35626, 34684, 26309, 34695, 25795, 25571, 37317],\n",
       " 35579: [27693, 20576, 28100, 27447, 37467, 14645, 27318, 26619, 28275, 26225],\n",
       " 35578: [14568, 37745, 35765, 27303, 37856, 38091, 36037, 27741, 12829, 37603],\n",
       " 35577: [36364, 20564, 20565, 27979, 26217, 20567, 33467, 26023, 27352, 27031],\n",
       " 35576: [24896, 31336, 19675, 35207, 33249, 33983, 25750, 23846, 34658, 23847],\n",
       " 35575: [20576, 20574, 27262, 20555, 27709, 37775, 27423, 28353, 20375, 27745],\n",
       " 35574: [28470, 31026, 20644, 35254, 35521, 25418, 36439, 31033, 25434, 31032],\n",
       " 35573: [26928, 31810, 27135, 26298, 35939, 27744, 25392, 27655, 27785, 32813],\n",
       " 35572: [28262, 26761, 36947, 20667, 24412, 31994, 28316, 20676, 26767, 26412],\n",
       " 35571: [26368, 37579, 27957, 26427, 28331, 32670, 35429, 34342, 27008, 35169],\n",
       " 35570: [28008, 27926, 31732, 28323, 28368, 20535, 37026, 37535, 28474, 37423],\n",
       " 35569: [26269, 36982, 21095, 27423, 35217, 37039, 28100, 35847, 26959, 37985],\n",
       " 35568: [27785, 20637, 31852, 37058, 27135, 25077, 24423, 28267, 36001, 27601],\n",
       " 35567: [28264, 28065, 27579, 27675, 36844, 27831, 37960, 20532, 36230, 28282],\n",
       " 35566: [20574, 33354, 27345, 20529, 20576, 25392, 27455, 28025, 25389, 24389],\n",
       " 35565: [27506, 14654, 26875, 25415, 27231, 24417, 25416, 27030, 26827, 26535],\n",
       " 35564: [28481, 37497, 37259, 28428, 28049, 28226, 28097, 37585, 31582, 37807],\n",
       " 35563: [27236, 27785, 26254, 27649, 25415, 36663, 31978, 25167, 26712, 28226],\n",
       " 35562: [37481, 37510, 36844, 28281, 37636, 36866, 26491, 27633, 20730, 32057],\n",
       " 35561: [28292, 20677, 25167, 25419, 35715, 25418, 26420, 36438, 35662, 20730],\n",
       " 35560: [36560, 36078, 31345, 27167, 20632, 20554, 31343, 20523, 27745, 20563],\n",
       " 35559: [27255, 34702, 28248, 37062, 36008, 35370, 27481, 37388, 36652, 35283],\n",
       " 35558: [28088, 37371, 37432, 37925, 12442, 35985, 20488, 27693, 14586, 36086],\n",
       " 35557: [37881, 35584, 35777, 36052, 35370, 28547, 35988, 27739, 27957, 36458],\n",
       " 35556: [37423, 26351, 28088, 27723, 27831, 27693, 20499, 28479, 38057, 35466],\n",
       " 35555: [27805, 30755, 25716, 36873, 20499, 20388, 28353, 27271, 27577, 20554],\n",
       " 35554: [27607, 27858, 36123, 28332, 31765, 27179, 28368, 35893, 27345, 37038],\n",
       " 35553: [37497, 36984, 20500, 20633, 35278, 36750, 37742, 28476, 37276, 31580],\n",
       " 35552: [27580, 28353, 21095, 25077, 25591, 23712, 25416, 25343, 20576, 20574],\n",
       " 35551: [36299, 27504, 36509, 21095, 36558, 35831, 37261, 28226, 34662, 35436],\n",
       " 35550: [31026, 26399, 32057, 31034, 31031, 25423, 31032, 31020, 14615, 14623],\n",
       " 35549: [26960, 36397, 37813, 26813, 20555, 35943, 14546, 25591, 28268, 20375],\n",
       " 35548: [28213, 26984, 26681, 37037, 27785, 14673, 27638, 28005, 27786, 15332],\n",
       " 35547: [27092, 35304, 36004, 20607, 28387, 27040, 27709, 27654, 26404, 26276],\n",
       " 35546: [36866, 38024, 37940, 36744, 32701, 35548, 33625, 27003, 37218, 28364],\n",
       " 35545: [31021, 28192, 28476, 14579, 20485, 27835, 28038, 20483, 36616, 20632],\n",
       " 35544: [14615, 28401, 14671, 25418, 31797, 21730, 26379, 20484, 20607, 14623],\n",
       " 35543: [31567, 28353, 20479, 27977, 26803, 27231, 35160, 24495, 26041, 27675],\n",
       " 35542: [20574, 25076, 20594, 25437, 25591, 25392, 28306, 28025, 25369, 27779],\n",
       " 35541: [27262, 14586, 26875, 20481, 7905, 27172, 27254, 37314, 28226, 37592],\n",
       " 35540: [20704, 35218, 37014, 14673, 34805, 25415, 25379, 34569, 25380, 35012],\n",
       " 35539: [26194, 26298, 26619, 14579, 25392, 20305, 25417, 26854, 25343, 27030],\n",
       " 35538: [14317, 24965, 26467, 18282, 25701, 20305, 23846, 26501, 35160, 27208],\n",
       " 35537: [36841, 27864, 25179, 27497, 31825, 37595, 33877, 36061, 27726, 36867],\n",
       " 35536: [27798, 36397, 27650, 26170, 33660, 26480, 37105, 36261, 28390, 14343],\n",
       " 35535: [37913, 23703, 20397, 37813, 35879, 38069, 20456, 20457, 38016, 27455],\n",
       " 35534: [26715, 37026, 26552, 14709, 28129, 37807, 27784, 27761, 38087, 20457],\n",
       " 35533: [32388, 27524, 25233, 25187, 8324, 33514, 24800, 25508, 24768, 30755],\n",
       " 35532: [25853, 35807, 30755, 33823, 24686, 25187, 38012, 23712, 25140, 26778],\n",
       " 35531: [34833, 34104, 30919, 20290, 26474, 28006, 34213, 36330, 26654, 33814],\n",
       " 35530: [28371, 25897, 28421, 31977, 37833, 27814, 37631, 35431, 20315, 26018],\n",
       " 35529: [28273, 20632, 20575, 28100, 38094, 37259, 37864, 25415, 37432, 37834],\n",
       " 35528: [27231, 27807, 33350, 34896, 30184, 37038, 37742, 37509, 20500, 37376],\n",
       " 35527: [28456, 20574, 28554, 32055, 27709, 20548, 28226, 36070, 20748, 27423],\n",
       " 35526: [20546, 27300, 27374, 24388, 26505, 26886, 27978, 25437, 37484, 20420],\n",
       " 35525: [28200, 31947, 35160, 27784, 28226, 34427, 25437, 26712, 20420, 20744],\n",
       " 35524: [27403, 28208, 37139, 20526, 36336, 26025, 27451, 28436, 20413, 21096],\n",
       " 35523: [26835, 20402, 24416, 25380, 20401, 26744, 25415, 24417, 27917, 20406],\n",
       " 35522: [36608, 32065, 34791, 26750, 28504, 19255, 37588, 20773, 37477, 36813],\n",
       " 35521: [27525, 36806, 28252, 36004, 38115, 27455, 28200, 37666, 27541, 28241],\n",
       " 35520: [37472, 35350, 20550, 27221, 36397, 27154, 21095, 36937, 27129, 15330],\n",
       " 35519: [26803, 27193, 27977, 27074, 20575, 24416, 28250, 20566, 20393, 20392],\n",
       " 35518: [32047, 27165, 27374, 27923, 20633, 27844, 20392, 20632, 37866, 20715],\n",
       " 35517: [36677, 20107, 26606, 34524, 36757, 28243, 35760, 37177, 36942, 37431],\n",
       " 35516: [37291, 20387, 36213, 20717, 28180, 28353, 37416, 36083, 20386, 25640],\n",
       " 35515: [20714, 28464, 25397, 27236, 37479, 35601, 28538, 27254, 25398, 36921],\n",
       " 35514: [25415, 38027, 11134, 25392, 27930, 37381, 37153, 27172, 28096, 11135],\n",
       " 35513: [34200, 36573, 27356, 34672, 28552, 37864, 37720, 27878, 20575, 20535],\n",
       " 35512: [20593, 20575, 26254, 27859, 25416, 20576, 25415, 20386, 14414, 27345],\n",
       " 35511: [26357, 20386, 27390, 20716, 28048, 27072, 36825, 25566, 20718, 28110],\n",
       " 35510: [37535, 28114, 20479, 26535, 27401, 25415, 20388, 20715, 26922, 20714],\n",
       " 35509: [20606, 27745, 27047, 27030, 24965, 20714, 37527, 31045, 27913, 20388],\n",
       " 35508: [37640, 27423, 27057, 36107, 27647, 25415, 36570, 28353, 37083, 26332],\n",
       " 35507: [26886, 20576, 20376, 25436, 25507, 26992, 27978, 20305, 26861, 33994],\n",
       " 35506: [26624, 31883, 27561, 31880, 27595, 26852, 31887, 37156, 31886, 20673],\n",
       " 35505: [28110, 35332, 34135, 36825, 24455, 20362, 26429, 26202, 20481, 25851],\n",
       " 35504: [26631, 28008, 33906, 26886, 28100, 20574, 26105, 32081, 27671, 27423],\n",
       " 35503: [20532, 33394, 25968, 25411, 32788, 32643, 36713, 26654, 27450, 26206],\n",
       " 35502: [20632, 26269, 27675, 20344, 28476, 24495, 28200, 14546, 27575, 28268],\n",
       " 35501: [20698, 36872, 27250, 14654, 20344, 27583, 14100, 27236, 37497, 20343],\n",
       " 35500: [28474, 20343, 27852, 27495, 24574, 26535, 26867, 25560, 28114, 27185],\n",
       " 35499: [24495, 24388, 28200, 23712, 25077, 28029, 20575, 14215, 28537, 25352],\n",
       " 35498: [25090, 34843, 26951, 27518, 32763, 20339, 25486, 20338, 36364, 26208],\n",
       " 35497: [26395, 20340, 26008, 35490, 26842, 8324, 25965, 24568, 26875, 24768],\n",
       " 35496: [35282, 33887, 35047, 28323, 28368, 35723, 25398, 28438, 21826, 28292],\n",
       " 35495: [30911, 32898, 36707, 30915, 27234, 30918, 30920, 35081, 35660, 34305],\n",
       " 35494: [32606, 28523, 28504, 37666, 37209, 34719, 36110, 33320, 38102, 20325],\n",
       " 35493: [33933, 34695, 34696, 34728, 34694, 33708, 34168, 27776, 20325, 35086],\n",
       " 35492: [36426, 30293, 28256, 35086, 33068, 33214, 26645, 35849, 37567, 27426],\n",
       " 35491: [27298, 26819, 36004, 37038, 28241, 37551, 25889, 37742, 27891, 27441],\n",
       " 35490: [28015, 36318, 20718, 38004, 36838, 35956, 7905, 25077, 36359, 25243],\n",
       " 35489: [26254, 25392, 25437, 26923, 26033, 27509, 28025, 26501, 26548, 27846],\n",
       " 35488: [25458, 33091, 37740, 36002, 25397, 28561, 25583, 27650, 37287, 28282],\n",
       " 35487: [25419, 25392, 25418, 25436, 25417, 25411, 25343, 23840, 25167, 24993],\n",
       " 35486: [37777, 37329, 34427, 37428, 25435, 32056, 25563, 25426, 24412, 36308],\n",
       " 35485: [25436, 23929, 25640, 25415, 26254, 2461, 27074, 12458, 25352, 11128],\n",
       " 35484: [37642, 25435, 25699, 27772, 24574, 28198, 26609, 27030, 26827, 37479],\n",
       " 35483: [20574, 36069, 14683, 20576, 25435, 24416, 25416, 26254, 21096, 27154],\n",
       " 35482: [25365, 26717, 27208, 11079, 25434, 25423, 20635, 27745, 31996, 11098],\n",
       " 35481: [34773, 26515, 35251, 26657, 36996, 25436, 26923, 26854, 37146, 26535],\n",
       " 35480: [28020, 25435, 20575, 25417, 25436, 20574, 27583, 36438, 27143, 26420],\n",
       " 35479: [34064, 20593, 27374, 27318, 36281, 27709, 28226, 20298, 27208, 27779],\n",
       " 35478: [28562, 31031, 37798, 20299, 27188, 34658, 26230, 20652, 35805, 27992],\n",
       " 35477: [24192, 26515, 20499, 35078, 20525, 27485, 37742, 31766, 20298, 35566],\n",
       " 35476: [14317, 30755, 33823, 25701, 23840, 25541, 25234, 23845, 24870, 26653],\n",
       " 35475: [30293, 32015, 20480, 30905, 25039, 30326, 33214, 26210, 34318, 34213],\n",
       " 35474: [27407, 36108, 20576, 36020, 28097, 26886, 27774, 27462, 37509, 20575],\n",
       " 35473: [24988, 27072, 33912, 33743, 25953, 34318, 33814, 20289, 20481, 32068],\n",
       " 35472: [25876, 26217, 26453, 25570, 25567, 24178, 25295, 26929, 27031, 25486],\n",
       " 35471: [31993, 25908, 23601, 33023, 31996, 32713, 35127, 33816, 27381, 33988],\n",
       " 35470: [33742, 33961, 20479, 34318, 35525, 32070, 20481, 33766, 34359, 32071],\n",
       " 35469: [32067, 32069, 33961, 20633, 20481, 27137, 33814, 20632, 28106, 25392],\n",
       " 35468: [36549, 28048, 26480, 37984, 25953, 27072, 28004, 37182, 37994, 27797],\n",
       " 35467: [20277, 26052, 35067, 27298, 36523, 30949, 20294, 25888, 36045, 35189],\n",
       " 35466: [36919, 25301, 35769, 11079, 25375, 31996, 26755, 36427, 26263, 27462],\n",
       " 35465: [28476, 37331, 28354, 38112, 20344, 14621, 27591, 28275, 28549, 37512],\n",
       " 35464: [20261, 27395, 27090, 27298, 37845, 27735, 28060, 36489, 26590, 20575],\n",
       " 35463: [27342, 26342, 27438, 3101, 22702, 27062, 25393, 24768, 24412, 30934],\n",
       " 35462: [25578, 30888, 26377, 26744, 27355, 36337, 36095, 25736, 38000, 20450],\n",
       " 35461: [27078, 27074, 26825, 27509, 8324, 20300, 26033, 26080, 20335, 20420],\n",
       " 35460: [26552, 26886, 27457, 20697, 20757, 26505, 28025, 27852, 27583, 27619],\n",
       " 35459: [31877, 35444, 27771, 31875, 31874, 21094, 34742, 30893, 35901, 35095],\n",
       " 35458: [28190, 28405, 28470, 37880, 28273, 27047, 28354, 14654, 27231, 20575],\n",
       " 35457: [20935, 27734, 34844, 28260, 35362, 20325, 35960, 35581, 1953, 34403],\n",
       " 35456: [14343, 27650, 26351, 27176, 26035, 36974, 20575, 30907, 20574, 22899],\n",
       " 35455: [25838, 28192, 36010, 35308, 27593, 25077, 28161, 27360, 25343, 27172],\n",
       " 35454: [27176, 20575, 28112, 27231, 26429, 36291, 27001, 27807, 22265, 28361],\n",
       " 35453: [20574, 20500, 20576, 27760, 27583, 25437, 38100, 27374, 27844, 36069],\n",
       " 35452: [28270, 35922, 27785, 27981, 36362, 31979, 26977, 27137, 35861, 27257],\n",
       " 35451: [25076, 20479, 25426, 26854, 14654, 25423, 27844, 24388, 25415, 25392],\n",
       " 35450: [34232, 25507, 33589, 27300, 37108, 27250, 24416, 28230, 26519, 37585],\n",
       " 35449: [27693, 27067, 28353, 27859, 37331, 27216, 28275, 20208, 21728, 27208],\n",
       " 35448: [20574, 37675, 38100, 25415, 37866, 28200, 27818, 31847, 28337, 27254],\n",
       " 35447: [34303, 28169, 35491, 20632, 35630, 36397, 28476, 32583, 35520, 35521],\n",
       " 35446: [37888, 26607, 25090, 27800, 25055, 26208, 37291, 28501, 20173, 20676],\n",
       " 35445: [34120, 20677, 37902, 37341, 36490, 35995, 34261, 37939, 34521, 27834],\n",
       " 35444: [37427, 37402, 28349, 27140, 28480, 31383, 37201, 36761, 36142, 34790],\n",
       " 35443: [27898, 20107, 25119, 25210, 27556, 24629, 37372, 36757, 35041, 27481],\n",
       " 35442: [34201, 20479, 27106, 14654, 35393, 27457, 27374, 36697, 28200, 35790],\n",
       " 35441: [20507, 27963, 20587, 27786, 28544, 27463, 26984, 27123, 35742, 37676],\n",
       " 35440: [28476, 30950, 35326, 37454, 30955, 38045, 20500, 30952, 30954, 37038],\n",
       " 35439: [26254, 27861, 28154, 27420, 20594, 20124, 27221, 25435, 27968, 25594],\n",
       " 35438: [20484, 27089, 26590, 34810, 27090, 35088, 28292, 28364, 28333, 28562],\n",
       " 35437: [27943, 27445, 28252, 28273, 37882, 37880, 27420, 38115, 26886, 25362],\n",
       " 35436: [26038, 26652, 34689, 25571, 32739, 34696, 34694, 34684, 8400, 26309],\n",
       " 35435: [36001, 25362, 25366, 20124, 34343, 36382, 7889, 35056, 7897, 27777],\n",
       " 35434: [37675, 27771, 35012, 20747, 34415, 27443, 25889, 27047, 25362, 24428],\n",
       " 35433: [35777, 35429, 27258, 35370, 37388, 35277, 35125, 35989, 36231, 28547],\n",
       " 35432: [36346, 37853, 37372, 37661, 36676, 38028, 28418, 37540, 38018, 20106],\n",
       " 35431: [26064, 37344, 37404, 37983, 32921, 35650, 37748, 34149, 23922, 32370],\n",
       " 35430: [30842, 25435, 36034, 27074, 27852, 25948, 35456, 20298, 38000, 36918],\n",
       " 35429: [27723, 34166, 28097, 26225, 31029, 26827, 28306, 31033, 27745, 28100],\n",
       " 35428: [35072, 30556, 26361, 34964, 25572, 34532, 34565, 25567, 25794, 25569],\n",
       " 35427: [26932, 27026, 26606, 25131, 14227, 35843, 5843, 26356, 27983, 34290],\n",
       " 35426: [33926, 35760, 19588, 34686, 35404, 34149, 20106, 35370, 35125, 27481],\n",
       " 35425: [34524, 33751, 35746, 37914, 35876, 33655, 35390, 33464, 34332, 36061],\n",
       " 35424: [34749, 20107, 35749, 34290, 35470, 35370, 27026, 34732, 26698, 35429],\n",
       " 35423: [27739, 24878, 33825, 35125, 33964, 34425, 26427, 27024, 26368, 26016],\n",
       " 35422: [34689, 34695, 34004, 25039, 33339, 34168, 26647, 32902, 29705, 33507],\n",
       " 35421: [20704, 20073, 33717, 36707, 20294, 33744, 30761, 34767, 20501, 35612],\n",
       " 35420: [26453, 20030, 36643, 20339, 27520, 27800, 26572, 31879, 37186, 31882],\n",
       " 35419: [36530, 31336, 26038, 36303, 25639, 11837, 26508, 34666, 36447, 27537],\n",
       " 35418: [33143, 36297, 36061, 36240, 35992, 37850, 33655, 26411, 35963, 36597],\n",
       " 35417: [33044, 37851, 20479, 27072, 20575, 36743, 27377, 36427, 36825, 24704],\n",
       " 35416: [24455, 35956, 35400, 22818, 24887, 28298, 22820, 27776, 22816, 28015],\n",
       " 35415: [33264, 20175, 33947, 27128, 14602, 26509, 36020, 27518, 26528, 31797],\n",
       " 35414: [24993, 26547, 30872, 20633, 27822, 25343, 20487, 25426, 31973, 25392],\n",
       " 35413: [25389, 33892, 20479, 27534, 20574, 20576, 37510, 25390, 28306, 12416],\n",
       " 35412: [19986, 31976, 31974, 38045, 28323, 37592, 31979, 27675, 26039, 25422],\n",
       " 35411: [33570, 31976, 31979, 31974, 28306, 19982, 31980, 19983, 31971, 36727],\n",
       " 35410: [27709, 20575, 25415, 27583, 28353, 36906, 31977, 27030, 35444, 20740],\n",
       " 35409: [34318, 26597, 27298, 24415, 19982, 27078, 34548, 19985, 20292, 31973],\n",
       " 35408: [31973, 31567, 19984, 31974, 19982, 35174, 35647, 19985, 31971, 20675],\n",
       " 35407: [26803, 19966, 20378, 25639, 36069, 25640, 25701, 14661, 37105, 2375],\n",
       " 35406: [20511, 25577, 19966, 25889, 26085, 27910, 24574, 26528, 20305, 20575],\n",
       " 35405: [24415, 19966, 20376, 31854, 20511, 20748, 20392, 25591, 27298, 25415],\n",
       " 35404: [20377, 25639, 36409, 27959, 28080, 19965, 33840, 35196, 17802, 36607],\n",
       " 35403: [37814, 19964, 25560, 35790, 25640, 27154, 25168, 26399, 26813, 27910],\n",
       " 35402: [28074, 27675, 27064, 20499, 37481, 37342, 37992, 27891, 37042, 28194],\n",
       " 35401: [20086, 20085, 31383, 31385, 30828, 19619, 31658, 20089, 20088, 20091],\n",
       " 35400: [26495, 19938, 26301, 27937, 25640, 28198, 26835, 27760, 27298, 20705],\n",
       " 35399: [27888, 27137, 26715, 19938, 35302, 20511, 27575, 28025, 20705, 20376],\n",
       " 35398: [26298, 31883, 19938, 20525, 20705, 27785, 31886, 27818, 28337, 31882],\n",
       " 35397: [22779, 27580, 20633, 25416, 28265, 33487, 28545, 32026, 33983, 27329],\n",
       " 35396: [33961, 33814, 20740, 20581, 28441, 35332, 28029, 24388, 28442, 27441],\n",
       " 35395: [31852, 27785, 26619, 20705, 27649, 28252, 36663, 28414, 36356, 25415],\n",
       " 35394: [37178, 26960, 27295, 36530, 27689, 27345, 26795, 31581, 31582, 19939],\n",
       " 35393: [37880, 37041, 25423, 21095, 28200, 25415, 20545, 25418, 20575, 19939],\n",
       " 35392: [27616, 20343, 36863, 36107, 27543, 38057, 25437, 25416, 20686, 36714],\n",
       " 35391: [14341, 27835, 27190, 36049, 37713, 27216, 27852, 26886, 36798, 28405],\n",
       " 35390: [26550, 27067, 26818, 32701, 26776, 19939, 36579, 27137, 24553, 27656],\n",
       " 35389: [24553, 33023, 33038, 32787, 34477, 26367, 33196, 33912, 35362, 33507],\n",
       " 35388: [31847, 20729, 14577, 20526, 28143, 26993, 26728, 31849, 34790, 26023],\n",
       " 35387: [36902, 36373, 35856, 35918, 28176, 36260, 37211, 24182, 28318, 2383],\n",
       " 35386: [26501, 27231, 25435, 25594, 24388, 2476, 14544, 27807, 27118, 24412],\n",
       " 35385: [27050, 26950, 19915, 27912, 31581, 27685, 26188, 25039, 27455, 25436],\n",
       " 35384: [31326, 25964, 26533, 26879, 27650, 26256, 26001, 37421, 27003, 14343],\n",
       " 35383: [35584, 35988, 36052, 25119, 19588, 31613, 34355, 35429, 35938, 35579],\n",
       " 35382: [25295, 26624, 26868, 25215, 27703, 27994, 25193, 26852, 28216, 24838],\n",
       " 35381: [35441, 32738, 32274, 25232, 32557, 33487, 32886, 30955, 34128, 33983],\n",
       " 35380: [20339, 20338, 25507, 35561, 20731, 25581, 26191, 19869, 26424, 36507],\n",
       " 35379: [26688, 28310, 14636, 20106, 35162, 26417, 27481, 27255, 33825, 25500],\n",
       " 35378: [27739, 28290, 26417, 24246, 34399, 33873, 26016, 34430, 35412, 26688],\n",
       " 35377: [35932, 37858, 36233, 19837, 20107, 37177, 37272, 37431, 27623, 37372],\n",
       " 35376: [34290, 37714, 20038, 19834, 20601, 19832, 29847, 19836, 20106, 19816],\n",
       " 35375: [37579, 33926, 26692, 20022, 35179, 34524, 19816, 26107, 27481, 20106],\n",
       " 35374: [25591, 26225, 25435, 25489, 26609, 27030, 26015, 26139, 27304, 27163],\n",
       " 35373: [19791, 19790, 37712, 26131, 25415, 25436, 28353, 25594, 25718, 24415],\n",
       " 35372: [20339, 20365, 34521, 35326, 34261, 20072, 27994, 25090, 26204, 29202],\n",
       " 35371: [27295, 28177, 27203, 35556, 27469, 36069, 28192, 14096, 35436, 28250],\n",
       " 35370: [27685, 28187, 35165, 35192, 35765, 20338, 20397, 28169, 31797, 14681],\n",
       " 35369: [24965, 20071, 20297, 20294, 33413, 20072, 20363, 25426, 33030, 25898],\n",
       " 35368: [19766, 12875, 25807, 31292, 14196, 20640, 24930, 24223, 34028, 25449],\n",
       " 35367: [36616, 27831, 36813, 27424, 26677, 35362, 14407, 32430, 36575, 37807],\n",
       " 35366: [30449, 24449, 35131, 33724, 27878, 36321, 27356, 28353, 27208, 35088],\n",
       " 35365: [30907, 27768, 19720, 26351, 26105, 14563, 27777, 30092, 33395, 27880],\n",
       " 35364: [25560, 27745, 20576, 27030, 26827, 25639, 31045, 28187, 28389, 26712],\n",
       " 35363: [27318, 35251, 28050, 26330, 33737, 20633, 35374, 27831, 26189, 36499],\n",
       " 35362: [31953, 34363, 24855, 26784, 34731, 25077, 24473, 23712, 25833, 33095],\n",
       " 35361: [36693, 27759, 28310, 35992, 26100, 27556, 37404, 27790, 37062, 28083],\n",
       " 35360: [35370, 34524, 36452, 28290, 35397, 36755, 35882, 33197, 37220, 36676],\n",
       " 35359: [26430, 34290, 24878, 36817, 34067, 34342, 37062, 35169, 35404, 26397],\n",
       " 35358: [20016, 36231, 34873, 34290, 31824, 36888, 35169, 19651, 35470, 36867],\n",
       " 35357: [25295, 32309, 24894, 24556, 27800, 24705, 25215, 27659, 26717, 26453],\n",
       " 35356: [26217, 34694, 34689, 24544, 21920, 25215, 34684, 27800, 24556, 26868],\n",
       " 35355: [33821, 32738, 24404, 33855, 25570, 24412, 32763, 24268, 24895, 26717],\n",
       " 35354: [35992, 27255, 35229, 35470, 37215, 35397, 34702, 27765, 26932, 36665],\n",
       " 35353: [31980, 19986, 19938, 31974, 19939, 37379, 36595, 27111, 18283, 27110],\n",
       " 35352: [24176, 30605, 35377, 31432, 32645, 24642, 32904, 32778, 33187, 37619],\n",
       " 35351: [20390, 35397, 20038, 34892, 36591, 36893, 36991, 20107, 36505, 36357],\n",
       " 35350: [25564, 27164, 34342, 26016, 27258, 26430, 19635, 27957, 26397, 27790],\n",
       " 35349: [34332, 35385, 35526, 23677, 36027, 28290, 36693, 35370, 26430, 35882],\n",
       " 35348: [37592, 28104, 31961, 20575, 37998, 37720, 20576, 36020, 24774, 7905],\n",
       " 35347: [26189, 28349, 30828, 27805, 27910, 14681, 25435, 26609, 36381, 25233],\n",
       " 35346: [27613, 36570, 25417, 26970, 25436, 36291, 27749, 24774, 27818, 25167],\n",
       " 35345: [28438, 28479, 28447, 28490, 24382, 24774, 24383, 28158, 31035, 28349],\n",
       " 35344: [24382, 35968, 24408, 28447, 32307, 28096, 35836, 24865, 35959, 34629],\n",
       " 35343: [27016, 34766, 28094, 31020, 20622, 32001, 27208, 31996, 26505, 27577],\n",
       " 35342: [20301, 31678, 32402, 25938, 31159, 31658, 34052, 30893, 19618, 32577],\n",
       " 35341: [34909, 20547, 31663, 37484, 24412, 20545, 15966, 19619, 30827, 35160],\n",
       " 35340: [36004, 35127, 35290, 25136, 37448, 38096, 36331, 27699, 28241, 27267],\n",
       " 35339: [28466, 36876, 28364, 28349, 37864, 38030, 25428, 38102, 28282, 28262],\n",
       " 35338: [35169, 34267, 33825, 35125, 26356, 26797, 35746, 27026, 26427, 27258],\n",
       " 35337: [4074, 31873, 25125, 34311, 33507, 36218, 32478, 35325, 35011, 35597],\n",
       " 35336: [27751, 37968, 34848, 37201, 35219, 28349, 37988, 33900, 30921, 37146],\n",
       " 35335: [14311, 31994, 26105, 33683, 31993, 27173, 33466, 33850, 34166, 27799],\n",
       " 35334: [26907, 26084, 14411, 20564, 26038, 25001, 20291, 20566, 25211, 33210],\n",
       " 35333: [19588, 35385, 35932, 27957, 35650, 35876, 36693, 36346, 35494, 28064],\n",
       " 35332: [24415, 27374, 14414, 27675, 27208, 14215, 25352, 26923, 26298, 20343],\n",
       " 35331: [26217, 32738, 34790, 32739, 25968, 33208, 20103, 36805, 24538, 19581],\n",
       " 35330: [33825, 34399, 26427, 35531, 34732, 34173, 24801, 14260, 28064, 26368],\n",
       " 35329: [26356, 20106, 26698, 26107, 20107, 35531, 34369, 14260, 24803, 33358],\n",
       " 35328: [36902, 28119, 24179, 24403, 31386, 37317, 35784, 37427, 37705, 36911],\n",
       " 35327: [20380, 19965, 25507, 20377, 20023, 20376, 20379, 19967, 26332, 26309],\n",
       " 35326: [35470, 27255, 20390, 19651, 27510, 34524, 27961, 36226, 36403, 36249],\n",
       " 35325: [27624, 27556, 34149, 34326, 36703, 26104, 20106, 27255, 23893, 20107],\n",
       " 35324: [26932, 26107, 25918, 27759, 36346, 33468, 33877, 35650, 20107, 36459],\n",
       " 35323: [35988, 34607, 37595, 35185, 28432, 33785, 27821, 26430, 34290, 36231],\n",
       " 35322: [33197, 33877, 36771, 35397, 37654, 27139, 35531, 27481, 35357, 27726],\n",
       " 35321: [27255, 28310, 25179, 36591, 33197, 37318, 27510, 34290, 27957, 27026],\n",
       " 35320: [37715, 35531, 33765, 20646, 26430, 31826, 26397, 33785, 35429, 34732],\n",
       " 35319: [24383, 24771, 24642, 32995, 33531, 35005, 28448, 34118, 34340, 33131],\n",
       " 35318: [25467, 20469, 36373, 35245, 24655, 26682, 32775, 27756, 24875, 24586],\n",
       " 35317: [20244, 34843, 32763, 27044, 26038, 23915, 26224, 32473, 24894, 24838],\n",
       " 35316: [25210, 26430, 20020, 31829, 32370, 36629, 34534, 20106, 35531, 37958],\n",
       " 35315: [36299, 26918, 34844, 37081, 38057, 34322, 27831, 36291, 14414, 34143],\n",
       " 35314: [25564, 34290, 26430, 26016, 34173, 26390, 26944, 26107, 33655, 27864],\n",
       " 35313: [35370, 27481, 28499, 37404, 35470, 27672, 35760, 37372, 36346, 35531],\n",
       " 35312: [34267, 33825, 33888, 33197, 19635, 36591, 35125, 27276, 31829, 34248],\n",
       " 35311: [32504, 19470, 35370, 25918, 29820, 34326, 24801, 29828, 19890, 37914],\n",
       " 35310: [31385, 31678, 37888, 36761, 29858, 30955, 36583, 24177, 24383, 20244],\n",
       " 35309: [36806, 19439, 20721, 33064, 35012, 28025, 28004, 25489, 37228, 37994],\n",
       " 35308: [37958, 36676, 37620, 37608, 37120, 36458, 38072, 37540, 35772, 35229],\n",
       " 35307: [36850, 36107, 26712, 20575, 25640, 24416, 31575, 35119, 25435, 20607],\n",
       " 35306: [27583, 36069, 26977, 20698, 37105, 35601, 26675, 27783, 27926, 28292],\n",
       " 35305: [14090, 36841, 36452, 19835, 36956, 35954, 35532, 36839, 36693, 36591],\n",
       " 35304: [27255, 31825, 27898, 35777, 37715, 35932, 20107, 34431, 27008, 36233],\n",
       " 35303: [34658, 20652, 35805, 28416, 26498, 25423, 26278, 14093, 25929, 28198],\n",
       " 35302: [14127, 35992, 37620, 26944, 34578, 34290, 25496, 27957, 35179, 35988],\n",
       " 35301: [25500, 35900, 33765, 27389, 26734, 33964, 35584, 31578, 26971, 31828],\n",
       " 35300: [35882, 14090, 36591, 36841, 36819, 38072, 28502, 37958, 36458, 36956],\n",
       " 35299: [33825, 25564, 26430, 36458, 24878, 14182, 35125, 24785, 26822, 28331],\n",
       " 35298: [36249, 33765, 32370, 31575, 31578, 35760, 29003, 32921, 36591, 37892],\n",
       " 35297: [25166, 32982, 27140, 33688, 33123, 25118, 34144, 25228, 25124, 25898],\n",
       " 35296: [20107, 37858, 27726, 36433, 35906, 37372, 27276, 27008, 27739, 20106],\n",
       " 35295: [35650, 28418, 36905, 36665, 20153, 36346, 29829, 27497, 26692, 35370],\n",
       " 35294: [37810, 37809, 37557, 36204, 32019, 37651, 38113, 36334, 19085, 36387],\n",
       " 35293: [28248, 34524, 26606, 37715, 28083, 28310, 27276, 14326, 35526, 37093],\n",
       " 35292: [34934, 34355, 31575, 34783, 32832, 36061, 5843, 33785, 25496, 26104],\n",
       " 35291: [27790, 35526, 34749, 20601, 27024, 27008, 27759, 15554, 26698, 27481],\n",
       " 35290: [34702, 34369, 37654, 37372, 27739, 36309, 36144, 27255, 36757, 35772],\n",
       " 35289: [25496, 26104, 36231, 31575, 25846, 35619, 23893, 35988, 37222, 25119],\n",
       " 35288: [31445, 24500, 34267, 24830, 27258, 26427, 27026, 33646, 25536, 35988],\n",
       " 35287: [37120, 37344, 36645, 28165, 37744, 14090, 37861, 28547, 37185, 32966],\n",
       " 35286: [35370, 28290, 36676, 36841, 19651, 37088, 37476, 37149, 36839, 36693],\n",
       " 35285: [24629, 25245, 28331, 25884, 27255, 32370, 37792, 36867, 35989, 20016],\n",
       " 35284: [35882, 36819, 35229, 36226, 14509, 36452, 24953, 36591, 35532, 36757],\n",
       " 35283: [33253, 32579, 14042, 27478, 31187, 21109, 31529, 25037, 23589, 31990],\n",
       " 35282: [35556, 26513, 37864, 20635, 24415, 25406, 28226, 20564, 25594, 35653],\n",
       " 35281: [33879, 26921, 27487, 26795, 20344, 25435, 28224, 27807, 21096, 27030],\n",
       " 35280: [22215, 22225, 22223, 22122, 22154, 22167, 22168, 22190, 22238, 22179],\n",
       " 35279: [27530, 27732, 24613, 31978, 36974, 37363, 20773, 37081, 28097, 31975],\n",
       " 35278: [28177, 37515, 28026, 20773, 38118, 37420, 20636, 31888, 37574, 14654],\n",
       " 35277: [27861, 37997, 26970, 20632, 20773, 37387, 37486, 28043, 25735, 20725],\n",
       " 35276: [25180, 25979, 26581, 26969, 25435, 35119, 36020, 26093, 27910, 36001],\n",
       " 35275: [27834, 26434, 35821, 20633, 19967, 37075, 34116, 27455, 19966, 26188],\n",
       " 35274: [25435, 26298, 36475, 27163, 25343, 35630, 37086, 25436, 27254, 37046],\n",
       " 35273: [37527, 37864, 35150, 27661, 27455, 14681, 20740, 27583, 25889, 36612],\n",
       " 35272: [26910, 37832, 34477, 26202, 28014, 19250, 25889, 37111, 25431, 25246],\n",
       " 35271: [15332, 28230, 14414, 28273, 27675, 28537, 36070, 37153, 27749, 27575],\n",
       " 35270: [34497, 25979, 27170, 26909, 36449, 37700, 37866, 25370, 27894, 28517],\n",
       " 35269: [36004, 27118, 32074, 25420, 27671, 28425, 28397, 28365, 37994, 27979],\n",
       " 35268: [34497, 37196, 27310, 36397, 28065, 28464, 36667, 26457, 38041, 28555],\n",
       " 35267: [24993, 31580, 27236, 20380, 31582, 25365, 20379, 25435, 24412, 24404],\n",
       " 35266: [20246, 29029, 16666, 18009, 20063, 24590, 32310, 32195, 8083, 14294],\n",
       " 35265: [24192, 26813, 24191, 25423, 14546, 28268, 37491, 28216, 25428, 28020],\n",
       " 35264: [27940, 27771, 35630, 26875, 19254, 37902, 37498, 20548, 28532, 36397],\n",
       " 35263: [27940, 28359, 35520, 36966, 36573, 11135, 24388, 35734, 15966, 37855],\n",
       " 35262: [27685, 36573, 27374, 20343, 27675, 25417, 25416, 26851, 21096, 28200],\n",
       " 35261: [27776, 25968, 26039, 35815, 24234, 26930, 14050, 25863, 24887, 25570],\n",
       " 35260: [20305, 20377, 25808, 20376, 25426, 25417, 24404, 25352, 35790, 20344],\n",
       " 35259: [30332, 22819, 22812, 24887, 26105, 24474, 19719, 22817, 30905, 22816],\n",
       " 35258: [20278, 20279, 32680, 32625, 19758, 19228, 33258, 14351, 36523, 25228],\n",
       " 35257: [30334, 19719, 32205, 30616, 31711, 27582, 30905, 30332, 30984, 25085],\n",
       " 35256: [26013, 36529, 20554, 14615, 14671, 38061, 37994, 31032, 37712, 35133],\n",
       " 35255: [37904, 27123, 20759, 27340, 20456, 35272, 27893, 8400, 20244, 28362],\n",
       " 35254: [30755, 33823, 33791, 14462, 34110, 34695, 34684, 34805, 34696, 35016],\n",
       " 35253: [37887, 37809, 32019, 36037, 38098, 25402, 25406, 36387, 36607, 36842],\n",
       " 35252: [35923, 32019, 38113, 36204, 26917, 36123, 38101, 35669, 28063, 36607],\n",
       " 35251: [36386, 36334, 19751, 32041, 26814, 37448, 38000, 19753, 19752, 37441],\n",
       " 35250: [26558, 30615, 20712, 31479, 31480, 26367, 35686, 30613, 33951, 29523],\n",
       " 35249: [17555, 21379, 17537, 29763, 29776, 29748, 18965, 28928, 17561, 17542],\n",
       " 35248: [17555, 21395, 21408, 18970, 21385, 21398, 21909, 17107, 29340, 21553],\n",
       " 35247: [25243, 25973, 27032, 27517, 25969, 27223, 35617, 30763, 28015, 28014],\n",
       " 35246: [25224, 26165, 28212, 20526, 36612, 27108, 25736, 24685, 25566, 14054],\n",
       " 35245: [26452, 26163, 34687, 24575, 35525, 26113, 25471, 26202, 25301, 26041],\n",
       " 35244: [25877, 25883, 25875, 34596, 34755, 37083, 36334, 4065, 8088, 10560],\n",
       " 35243: [20514, 20401, 20406, 31872, 36302, 20349, 37312, 5666, 25947, 35212],\n",
       " 35242: [18824, 22245, 22158, 22117, 22133, 22166, 22113, 22153, 22225, 22164],\n",
       " 35241: [22225, 22219, 22165, 22245, 22164, 22106, 22190, 22109, 22168, 22167],\n",
       " 35240: [22206, 22106, 22190, 22114, 22217, 22045, 22060, 22200, 22153, 22219],\n",
       " 35239: [23851, 20223, 34762, 27802, 31872, 20264, 25731, 31360, 25197, 36973],\n",
       " 35238: [28344, 37223, 36844, 32055, 28441, 26361, 32057, 35653, 20369, 20370],\n",
       " 35237: [19915, 24234, 27893, 35571, 25968, 27123, 28362, 24474, 25567, 32104],\n",
       " 35236: [34318, 24474, 26879, 33912, 32068, 24249, 20480, 26646, 33814, 36825],\n",
       " 35235: [25140, 27225, 31854, 31029, 20576, 26298, 27808, 24181, 27110, 25417],\n",
       " 35234: [36779, 36776, 36031, 36968, 35504, 30888, 36264, 37192, 38034, 37191],\n",
       " 35233: [37432, 28480, 27709, 28298, 24553, 36468, 27580, 27799, 37513, 37925],\n",
       " 35232: [35639, 35556, 34902, 37864, 38076, 28020, 28025, 31883, 27235, 26923],\n",
       " 35231: [20484, 25435, 26093, 20338, 27074, 20340, 27531, 24412, 24404, 25750],\n",
       " 35230: [25224, 25637, 18390, 34181, 32583, 28211, 26963, 24685, 26429, 24809],\n",
       " 35229: [10617, 34745, 27211, 14686, 25973, 14272, 35617, 10552, 8069, 20403],\n",
       " 35228: [31581, 19628, 18285, 33272, 24416, 14654, 24388, 25423, 35362, 24417],\n",
       " 35227: [27579, 37156, 35807, 37416, 20740, 37845, 37009, 20670, 27692, 27583],\n",
       " 35226: [34698, 26810, 36108, 20124, 27143, 26851, 24411, 28500, 37909, 25417],\n",
       " 35225: [27745, 25716, 19966, 28541, 28297, 28353, 28554, 25392, 27298, 37083],\n",
       " 35224: [36750, 31580, 24191, 34339, 20575, 24192, 31585, 26361, 26178, 35061],\n",
       " 35223: [14462, 33323, 36467, 33847, 37864, 33739, 37368, 24456, 25582, 25422],\n",
       " 35222: [36773, 27783, 36779, 37193, 34485, 27926, 37191, 27506, 26969, 36266],\n",
       " 35221: [28074, 20696, 27964, 28260, 37960, 27745, 20388, 20715, 25418, 37432],\n",
       " 35220: [19766, 23889, 19768, 24716, 34943, 19071, 35540, 27802, 25197, 31958],\n",
       " 35219: [26503, 30510, 34324, 28172, 34790, 26327, 32243, 27650, 25090, 26751],\n",
       " 35218: [36509, 26813, 35943, 35893, 27455, 27723, 36884, 38087, 27978, 32070],\n",
       " 35217: [26185, 34135, 25246, 27032, 28298, 36607, 18156, 36386, 37496, 20214],\n",
       " 35216: [31874, 35757, 34278, 27352, 25411, 17945, 8164, 31877, 24473, 23894],\n",
       " 35215: [17851, 17878, 17893, 17900, 17911, 17882, 17895, 17884, 17865, 17853],\n",
       " 35214: [24716, 25932, 33500, 28233, 25184, 25100, 25021, 20565, 25197, 19988],\n",
       " 35213: [22156, 22217, 28861, 22245, 22226, 22180, 22189, 22170, 22148, 22220],\n",
       " 35212: [35304, 32081, 20712, 31704, 30616, 30963, 31711, 33582, 31935, 14652],\n",
       " 35211: [17700, 11618, 26904, 24533, 24280, 20277, 27153, 27802, 24459, 24927],\n",
       " 35210: [27207, 27124, 17699, 17705, 28583, 25001, 27071, 20169, 27352, 35360],\n",
       " 35209: [28262, 37423, 37798, 28438, 20575, 34611, 20576, 20163, 20486, 37337],\n",
       " 35208: [17341, 36994, 35639, 37421, 38007, 36142, 36943, 37050, 37645, 26840],\n",
       " 35207: [16666, 33991, 28233, 25197, 8083, 19253, 24594, 14595, 15555, 31872],\n",
       " 35206: [20638, 27134, 34415, 35444, 33432, 27176, 26715, 35846, 27167, 27771],\n",
       " 35205: [19453, 23087, 23111, 23145, 35335, 23081, 19988, 19359, 19639, 23075],\n",
       " 35204: [21400, 21598, 18973, 21301, 19034, 4503, 21411, 19001, 21414, 19016],\n",
       " 35203: [19034, 18973, 18968, 4742, 29362, 4457, 21551, 29365, 29303, 18989],\n",
       " 35202: [17555, 18964, 21385, 17552, 19002, 29480, 19007, 17560, 18968, 18931],\n",
       " 35201: [18964, 19002, 19031, 17530, 17531, 29747, 21329, 29499, 17562, 17550],\n",
       " 35200: [18970, 19034, 18973, 17555, 18968, 21909, 18987, 19044, 17528, 19014],\n",
       " 35199: [17552, 17555, 21903, 18973, 19014, 19002, 19034, 18937, 18987, 29424],\n",
       " 35198: [17974, 17850, 17999, 17833, 17996, 17823, 18003, 17967, 17995, 17977],\n",
       " 35197: [21903, 29331, 18964, 19034, 21304, 21404, 21909, 21400, 21365, 29388],\n",
       " 35196: [19034, 18964, 21385, 18970, 21328, 19002, 17560, 17532, 19018, 19021],\n",
       " 35195: [30523, 26825, 25415, 25140, 20607, 27701, 25594, 26178, 17407, 37947],\n",
       " 35194: [27888, 28428, 27488, 20633, 28517, 20774, 37751, 36850, 27282, 36762],\n",
       " 35193: [26590, 19982, 31974, 37287, 31971, 28364, 19986, 28306, 35088, 31973],\n",
       " 35192: [26105, 36412, 27030, 31762, 34341, 20152, 19939, 26609, 34826, 17335],\n",
       " 35191: [24402, 17312, 28459, 27771, 34909, 36039, 22783, 28026, 27061, 35869],\n",
       " 35190: [36876, 28106, 17261, 17249, 28260, 25379, 28226, 20500, 20723, 28171],\n",
       " 35189: [20498, 20632, 25363, 20122, 28476, 36865, 20500, 36260, 34343, 37038],\n",
       " 35188: [17261, 31708, 26351, 27111, 27971, 30641, 14414, 30619, 27091, 30616],\n",
       " 35187: [21404, 19034, 29389, 29424, 21551, 29357, 19044, 22144, 21390, 19389],\n",
       " 35186: [27859, 26661, 26875, 26288, 24403, 35422, 30448, 33782, 33395, 28397],\n",
       " 35185: [17501, 30689, 21586, 16837, 17897, 23115, 17503, 17902, 17998, 17506],\n",
       " 35184: [17829, 17994, 16784, 17874, 17835, 16783, 17862, 16862, 16860, 17905],\n",
       " 35183: [17861, 17878, 16785, 17974, 16783, 17998, 17865, 16766, 17851, 17916],\n",
       " 35182: [20693, 17895, 17998, 17501, 22217, 17892, 16837, 17898, 31957, 17902],\n",
       " 35181: [22231, 22225, 22200, 16857, 22226, 16827, 16820, 28872, 22167, 22135],\n",
       " 35180: [17835, 16819, 16676, 16769, 16720, 16828, 16801, 5139, 17949, 17989],\n",
       " 35179: [17555, 18964, 21909, 18970, 18986, 21556, 29765, 21385, 19008, 18989],\n",
       " 35178: [27027, 28349, 27383, 35723, 28158, 27873, 20132, 34225, 37526, 37052],\n",
       " 35177: [16801, 16828, 16827, 16769, 16823, 16819, 16714, 16656, 16862, 16840],\n",
       " 35176: [16803, 21557, 18936, 18937, 17555, 16694, 18931, 18930, 19003, 9543],\n",
       " 35175: [17975, 36543, 17887, 17819, 27132, 14578, 17996, 34894, 16596, 34929],\n",
       " 35174: [25507, 32701, 32665, 32758, 32739, 25571, 32473, 26039, 32262, 25215],\n",
       " 35173: [27568, 16752, 23299, 23136, 16771, 17879, 23142, 17552, 16828, 17881],\n",
       " 35172: [18970, 29362, 13083, 29402, 21385, 21596, 16770, 19034, 29401, 4499],\n",
       " 35171: [16782, 16662, 16860, 16785, 16713, 16657, 16840, 16783, 16704, 16650],\n",
       " 35170: [19034, 29289, 16657, 18968, 29411, 21385, 16776, 16656, 16784, 29401],\n",
       " 35169: [16803, 16765, 16827, 16691, 16686, 16656, 18986, 16784, 16802, 16769],\n",
       " 35168: [16657, 16753, 16776, 16819, 16703, 16690, 16782, 29357, 16780, 16697],\n",
       " 35167: [19034, 16779, 16814, 16666, 22113, 21385, 16803, 16679, 16657, 19008],\n",
       " 35166: [17862, 16723, 17830, 17835, 17833, 17818, 17864, 17978, 17828, 17859],\n",
       " 35165: [16679, 16657, 16776, 16758, 16713, 16675, 16699, 16770, 16695, 19007],\n",
       " 35164: [16783, 16840, 16779, 16827, 16770, 16713, 16819, 16782, 16771, 16777],\n",
       " 35163: [16819, 16802, 16784, 16758, 16754, 16785, 16770, 16783, 16713, 16722],\n",
       " 35162: [16609, 16635, 16686, 16785, 16638, 17555, 16725, 16620, 17966, 16684],\n",
       " 35161: [18964, 29411, 29480, 16780, 17966, 29424, 16779, 16716, 18968, 18989],\n",
       " 35160: [16819, 23297, 16676, 16811, 16722, 16771, 16802, 16801, 16777, 16787],\n",
       " 35159: [17555, 21322, 29386, 16785, 16721, 19001, 18992, 16656, 18023, 16658],\n",
       " 35158: [16860, 16819, 16862, 16703, 16770, 16827, 16776, 16713, 16751, 16754],\n",
       " 35157: [16784, 16656, 16736, 16823, 16779, 16785, 16801, 16838, 16866, 16657],\n",
       " 35156: [16827, 16749, 17971, 16802, 16722, 16657, 16779, 16690, 16784, 16776],\n",
       " 35155: [16819, 17882, 17879, 16749, 17836, 17811, 18001, 17906, 16776, 17955],\n",
       " 35154: [16780, 16723, 16671, 16725, 16620, 16638, 16609, 16703, 16643, 16690],\n",
       " 35153: [16705, 16620, 16627, 16728, 16679, 16781, 16696, 16784, 16703, 16675],\n",
       " 35152: [16815, 16671, 16728, 16635, 16827, 16708, 16769, 16657, 16638, 16801],\n",
       " 35151: [16728, 16670, 16749, 16769, 16620, 16722, 16801, 16779, 16776, 16676],\n",
       " 35150: [16671, 16686, 16620, 16725, 16670, 16703, 16802, 16739, 16676, 16669],\n",
       " 35149: [16720, 17942, 13192, 16807, 19641, 18006, 17919, 17949, 8373, 17947],\n",
       " 35148: [16661, 16780, 16724, 16770, 16776, 16742, 16716, 16615, 16657, 16723],\n",
       " 35147: [18770, 16656, 16785, 16676, 16784, 16769, 16779, 16713, 16742, 16811],\n",
       " 35146: [33263, 15508, 16590, 15484, 17917, 26951, 17966, 32546, 32973, 16749],\n",
       " 35145: [17938, 14578, 19513, 16670, 33313, 34734, 31899, 16620, 16720, 36212],\n",
       " 35144: [35842, 8373, 17938, 17845, 16720, 17920, 32973, 8111, 8105, 17942],\n",
       " 35143: [26503, 27231, 24046, 23372, 19072, 22441, 22265, 23398, 20325, 22322],\n",
       " 35142: [19834, 31445, 19832, 31572, 29840, 19836, 29846, 29847, 29844, 19458],\n",
       " 35141: [27342, 26201, 37764, 24388, 35723, 35916, 20450, 37025, 36384, 28292],\n",
       " 35140: [28332, 28429, 36474, 27303, 37874, 36941, 35958, 26743, 37739, 35570],\n",
       " 35139: [34645, 28465, 20543, 37574, 27709, 28216, 37291, 26505, 36055, 27179],\n",
       " 35138: [28332, 27675, 36263, 37874, 36065, 36173, 27179, 36063, 27350, 36635],\n",
       " 35137: [36323, 36610, 35914, 37304, 36772, 28459, 36773, 36779, 37301, 35915],\n",
       " 35136: [17881, 26303, 26635, 27653, 18148, 26527, 22180, 17836, 22173, 20735],\n",
       " 35135: [27045, 27840, 27796, 26183, 32994, 31967, 34176, 35022, 28399, 20090],\n",
       " 35134: [26188, 25393, 16197, 25842, 27910, 20525, 27583, 16238, 27709, 34378],\n",
       " 35133: [27582, 35304, 30175, 28251, 37768, 20487, 38092, 31704, 32081, 33951],\n",
       " 35132: [31038, 27033, 27376, 37712, 36405, 37386, 37807, 14664, 35768, 37385],\n",
       " 35131: [30615, 20488, 30619, 35562, 36804, 30984, 37703, 33951, 30993, 20487],\n",
       " 35130: [34702, 19470, 15552, 14636, 20107, 26517, 25500, 34276, 32832, 20476],\n",
       " 35129: [22816, 22814, 19985, 22804, 16171, 36426, 20361, 35684, 31999, 37631],\n",
       " 35128: [26351, 37778, 26170, 36577, 37541, 37427, 25246, 33068, 37321, 26503],\n",
       " 35127: [28519, 37715, 37343, 19458, 37390, 14643, 26390, 28068, 36696, 28508],\n",
       " 35126: [31385, 27650, 36579, 34543, 27612, 35639, 28485, 21753, 35990, 29858],\n",
       " 35125: [20470, 20681, 35990, 32082, 31647, 20682, 31876, 31649, 36988, 36418],\n",
       " 35124: [20210, 27492, 10943, 32015, 34381, 37963, 27328, 36900, 36378, 20061],\n",
       " 35123: [29867, 23061, 29340, 29620, 23059, 16688, 29877, 17967, 18144, 16671],\n",
       " 35122: [37349, 36996, 36701, 34426, 16092, 16170, 37706, 35079, 36533, 16116],\n",
       " 35121: [28432, 34267, 35470, 25506, 24878, 34940, 27026, 27008, 35277, 34581],\n",
       " 35120: [34766, 26225, 33965, 27224, 31020, 33214, 20566, 25415, 34317, 33933],\n",
       " 35119: [37880, 28481, 28273, 19985, 31974, 27807, 19984, 37895, 37416, 19986],\n",
       " 35118: [26750, 28097, 27375, 27451, 27485, 15325, 26548, 27805, 27846, 24403],\n",
       " 35117: [26581, 27374, 27923, 27732, 36381, 24417, 27111, 35782, 24415, 26505],\n",
       " 35116: [25987, 24556, 25803, 24404, 33855, 22701, 26572, 26387, 25392, 26930],\n",
       " 35115: [26609, 24412, 25560, 27250, 26254, 25808, 25343, 25640, 26923, 26875],\n",
       " 35114: [18282, 26854, 24412, 20479, 26851, 27374, 25435, 27074, 26244, 20331],\n",
       " 35113: [26854, 20393, 27236, 31854, 20394, 3213, 27675, 26574, 31581, 25426],\n",
       " 35112: [23846, 25419, 25343, 24388, 25417, 23847, 35254, 25435, 24404, 25434],\n",
       " 35111: [27374, 27300, 27057, 26851, 24412, 20420, 36001, 26977, 27403, 20397],\n",
       " 35110: [26412, 28362, 27893, 24935, 37307, 27303, 35360, 26907, 25964, 24898],\n",
       " 35109: [19618, 30827, 16197, 32104, 20334, 32114, 35096, 31657, 26951, 31806],\n",
       " 35108: [28323, 28096, 27190, 28205, 36005, 28265, 27649, 38057, 35799, 37423],\n",
       " 35107: [34267, 31825, 26430, 34430, 36189, 35179, 34248, 24878, 24629, 25536],\n",
       " 35106: [33403, 7309, 33014, 34695, 14343, 20023, 34684, 25571, 36318, 20324],\n",
       " 35105: [28285, 33685, 27472, 27438, 27441, 28090, 26810, 21109, 31847, 36798],\n",
       " 35104: [26738, 15826, 26682, 26519, 27105, 25466, 20042, 26191, 24388, 25286],\n",
       " 35103: [15550, 19854, 26733, 27315, 25079, 26911, 32006, 28233, 33122, 22464],\n",
       " 35102: [16660, 16679, 16658, 16828, 16614, 16823, 16737, 16697, 16703, 16713],\n",
       " 35101: [18964, 18989, 21551, 29375, 19039, 21398, 29487, 21598, 16815, 22746],\n",
       " 35100: [35876, 35397, 37372, 26107, 26427, 26692, 20602, 32567, 26662, 35385],\n",
       " 35099: [32738, 25558, 25573, 37028, 33208, 24620, 25071, 24573, 24789, 25571],\n",
       " 35098: [25875, 25883, 16749, 16807, 16593, 22928, 8372, 16583, 33263, 16670],\n",
       " 35097: [26392, 14199, 35043, 16723, 21245, 17951, 20264, 18006, 19768, 19329],\n",
       " 35096: [18832, 18830, 22060, 22110, 18824, 22158, 17892, 17638, 22219, 28867],\n",
       " 35095: [22062, 18840, 18830, 22196, 22245, 22198, 22042, 17728, 22148, 22096],\n",
       " 35094: [18792, 29387, 22449, 30867, 19080, 19077, 22061, 17967, 31193, 31270],\n",
       " 35093: [16583, 16690, 15508, 17881, 32718, 17973, 17972, 17826, 16769, 27413],\n",
       " 35092: [30200, 17973, 17832, 17844, 5122, 17848, 17998, 17948, 17940, 18794],\n",
       " 35091: [17832, 17865, 17843, 17899, 17916, 17908, 17835, 17819, 17973, 17853],\n",
       " 35090: [16664, 16770, 16777, 16861, 16703, 16751, 16784, 30202, 16808, 16676],\n",
       " 35089: [28862, 28867, 18970, 19044, 28872, 19034, 21385, 28868, 21392, 21909],\n",
       " 35088: [29718, 18970, 18964, 21392, 21371, 17554, 29714, 19034, 17555, 21553],\n",
       " 35087: [18970, 19034, 18964, 21553, 29346, 21385, 17107, 17530, 29550, 18987],\n",
       " 35086: [36564, 31567, 36964, 28459, 20507, 20696, 28441, 28049, 37537, 37016],\n",
       " 35085: [30755, 37515, 36791, 25419, 37895, 27577, 28203, 36439, 33514, 37615],\n",
       " 35084: [28063, 27846, 27374, 26875, 28397, 27520, 27894, 27831, 27254, 26909],\n",
       " 35083: [20498, 37592, 38112, 37998, 27785, 35547, 32074, 35384, 27441, 37432],\n",
       " 35082: [35774, 34790, 27324, 20673, 19879, 25571, 36170, 27628, 26309, 19880],\n",
       " 35081: [28466, 28158, 28438, 28074, 28244, 14669, 28267, 28523, 37526, 35765],\n",
       " 35080: [20397, 25076, 20594, 27509, 34507, 24430, 14500, 31580, 26298, 14609],\n",
       " 35079: [27764, 28416, 31973, 28100, 37657, 27709, 28554, 19986, 31979, 37507],\n",
       " 35078: [32067, 20305, 26148, 36873, 37864, 25436, 28226, 25699, 25167, 25434],\n",
       " 35077: [27378, 26254, 24613, 28395, 35790, 20551, 24712, 11134, 27543, 28548],\n",
       " 35076: [28213, 36529, 37930, 27917, 28025, 19233, 31977, 27732, 37712, 37153],\n",
       " 35075: [20574, 27236, 28416, 20375, 20576, 21095, 14546, 27745, 24416, 27534],\n",
       " 35074: [28213, 27812, 28097, 36361, 35624, 27917, 28389, 36602, 36439, 20526],\n",
       " 35073: [37041, 24388, 25437, 27403, 36509, 2461, 27298, 27520, 25701, 25402],\n",
       " 35072: [29964, 13229, 28605, 29971, 29963, 18521, 21378, 18553, 22658, 22714],\n",
       " 35071: [24671, 32853, 20638, 19653, 27374, 25434, 36439, 25411, 20637, 25418],\n",
       " 35070: [17552, 17555, 21385, 17107, 18963, 17550, 21299, 21901, 17554, 18939],\n",
       " 35069: [18964, 17555, 19014, 19035, 18973, 19009, 18970, 29424, 19018, 19002],\n",
       " 35068: [16679, 16635, 16769, 16723, 16807, 18964, 17552, 29380, 16643, 16749],\n",
       " 35067: [19034, 18964, 19044, 29747, 18934, 19013, 29499, 19015, 19035, 29500],\n",
       " 35066: [25102, 34060, 22819, 26407, 22825, 27054, 34796, 27599, 25940, 26470],\n",
       " 35065: [30826, 30827, 26038, 8400, 25055, 33218, 34596, 33208, 14981, 28762],\n",
       " 35064: [27190, 14358, 35669, 36973, 17254, 36468, 28302, 37159, 35622, 37016],\n",
       " 35063: [33990, 25189, 14311, 12388, 8083, 12060, 24695, 19856, 19091, 14294],\n",
       " 35062: [35715, 36698, 26977, 35601, 35062, 26854, 37730, 37813, 27452, 20644],\n",
       " 35061: [37091, 37695, 38101, 11879, 20500, 20499, 37742, 10973, 37038, 14872],\n",
       " 35060: [37250, 33244, 25090, 35086, 32898, 32701, 36364, 26991, 35902, 34237],\n",
       " 35059: [20574, 37561, 27784, 37712, 37851, 37026, 28332, 14692, 36529, 28353],\n",
       " 35058: [37959, 27777, 37908, 38016, 37939, 38003, 20603, 37246, 32036, 37948],\n",
       " 35057: [27693, 38030, 37331, 36107, 27744, 26638, 37102, 37904, 34104, 26809],\n",
       " 35056: [28459, 27579, 26909, 26330, 34941, 26950, 25416, 36708, 34834, 36478],\n",
       " 35055: [26977, 36183, 20740, 26836, 27583, 20633, 27822, 36884, 36413, 37551],\n",
       " 35054: [27649, 14674, 31999, 27930, 28106, 36974, 37416, 36695, 28025, 11134],\n",
       " 35053: [19986, 20487, 27114, 8327, 33589, 20386, 20715, 34212, 19983, 20716],\n",
       " 35052: [31854, 20757, 20244, 27785, 27172, 22824, 36214, 20759, 24425, 34759],\n",
       " 35051: [28507, 28461, 37321, 28368, 28323, 28262, 28438, 31383, 18286, 37109],\n",
       " 35050: [28262, 28466, 28507, 20500, 28527, 27067, 35952, 14272, 37291, 28413],\n",
       " 35049: [33597, 33772, 34190, 26367, 28251, 32356, 30984, 33161, 32916, 36377],\n",
       " 35048: [26574, 35347, 35000, 30854, 35710, 34786, 35707, 31766, 20125, 34994],\n",
       " 35047: [37774, 25434, 35254, 36448, 27661, 36740, 37484, 25411, 24388, 20546],\n",
       " 35046: [14620, 37561, 28554, 27709, 20633, 27423, 14714, 28276, 28096, 37026],\n",
       " 35045: [28008, 26297, 27763, 14714, 25392, 26886, 25411, 37723, 28226, 29221],\n",
       " 35044: [20704, 20380, 14714, 20379, 27128, 37763, 36806, 31973, 28020, 35836],\n",
       " 35043: [31386, 37898, 26038, 26524, 33551, 25039, 30555, 25968, 26868, 25295],\n",
       " 35042: [36006, 27575, 24416, 37864, 20344, 28562, 14215, 14338, 20314, 37384],\n",
       " 35041: [36110, 28465, 14669, 27176, 27575, 27721, 27616, 27163, 20633, 27831],\n",
       " 35040: [14643, 36693, 36126, 27510, 37372, 35876, 28519, 28547, 37654, 37214],\n",
       " 35039: [30545, 24374, 14329, 20654, 30277, 14302, 26242, 30503, 18898, 30494],\n",
       " 35038: [14675, 38094, 25397, 20748, 36752, 18286, 35993, 28337, 27575, 35096],\n",
       " 35037: [32074, 28180, 37971, 26814, 28494, 14680, 28558, 33138, 28514, 32847],\n",
       " 35036: [16759, 17919, 17945, 17942, 16669, 30690, 29308, 28595, 20693, 16879],\n",
       " 35035: [30301, 25343, 23712, 26330, 25784, 35669, 25582, 26448, 8158, 26065],\n",
       " 35034: [27154, 36448, 20575, 27590, 27647, 27298, 20574, 24416, 31973, 27601],\n",
       " 35033: [14633, 26052, 27374, 36802, 27520, 27831, 27455, 37733, 26928, 28020],\n",
       " 35032: [27390, 27420, 28100, 27506, 27318, 27656, 26962, 26712, 32788, 20397],\n",
       " 35031: [20208, 27423, 37930, 27696, 27656, 27917, 37960, 35384, 27732, 28100],\n",
       " 35030: [26875, 24415, 25395, 27231, 31584, 24417, 38076, 27656, 25416, 14671],\n",
       " 35029: [27530, 31973, 31978, 14654, 31976, 27513, 28100, 14621, 37631, 14645],\n",
       " 35028: [27709, 27785, 34346, 27503, 28554, 28097, 27917, 20386, 28038, 37551],\n",
       " 35027: [14701, 24558, 20287, 27847, 24977, 26873, 26638, 27086, 27559, 32439],\n",
       " 35026: [26928, 20378, 25301, 28555, 25471, 27744, 27534, 37894, 37105, 20376],\n",
       " 35025: [14695, 26548, 27575, 36530, 31029, 20576, 20748, 37315, 35189, 28025],\n",
       " 35024: [14677, 20345, 28049, 25343, 36570, 25426, 25437, 28405, 37677, 36872],\n",
       " 35023: [35521, 14654, 31975, 20555, 20576, 20575, 35062, 31973, 19939, 31979],\n",
       " 35022: [20550, 20574, 20723, 31045, 27420, 27208, 36563, 18283, 28050, 26113],\n",
       " 35021: [14677, 37105, 26189, 36261, 35561, 36069, 35967, 35196, 37196, 28288],\n",
       " 35020: [23929, 25392, 25929, 25435, 23847, 23846, 25411, 20420, 37315, 20300],\n",
       " 35019: [20632, 25415, 28476, 25431, 25392, 24388, 20575, 20574, 25718, 28200],\n",
       " 35018: [25363, 20122, 27390, 14683, 26962, 38092, 36260, 34343, 7898, 36382],\n",
       " 35017: [26960, 37589, 37242, 35095, 37486, 36937, 36480, 37790, 37315, 37278],\n",
       " 35016: [28476, 34810, 27709, 27154, 28100, 37371, 14640, 37315, 37592, 35167],\n",
       " 35015: [28267, 37515, 27114, 27670, 31020, 26609, 37930, 28226, 14529, 28200],\n",
       " 35014: [20733, 38092, 36804, 27971, 30616, 30963, 20488, 20712, 36344, 30993],\n",
       " 35013: [27154, 37105, 20387, 37808, 20388, 37845, 27826, 20716, 32011, 37196],\n",
       " 35012: [27445, 28147, 28088, 37202, 28387, 14680, 14645, 34922, 36642, 34296],\n",
       " 35011: [37202, 27221, 26610, 26473, 26875, 32050, 27182, 20661, 20388, 20715],\n",
       " 35010: [36069, 27580, 27706, 36108, 15330, 32081, 25413, 25431, 19254, 27709],\n",
       " 35009: [20546, 27300, 26495, 37864, 37484, 27784, 27675, 35302, 37786, 26302],\n",
       " 35008: [37385, 36982, 36958, 27601, 37108, 36974, 14674, 37765, 37597, 35398],\n",
       " 35007: [27709, 36366, 28100, 20434, 27601, 36663, 36982, 37424, 15325, 36974],\n",
       " 35006: [34821, 14096, 27709, 36620, 27423, 20713, 28133, 27543, 27638, 36509],\n",
       " 35005: [37984, 37866, 28169, 27841, 37560, 37534, 28171, 37893, 35708, 27451],\n",
       " 35004: [27530, 35591, 27785, 36663, 27649, 24388, 28192, 36974, 25416, 36784],\n",
       " 35003: [20575, 25415, 26093, 24417, 24430, 25431, 35225, 27163, 37720, 26254],\n",
       " 35002: [37423, 36020, 37026, 28252, 36974, 37765, 27785, 37895, 37876, 27649],\n",
       " 35001: [21717, 27859, 27575, 36361, 36602, 26875, 25416, 27537, 27172, 21826],\n",
       " 35000: [37014, 26105, 24287, 32453, 35218, 14462, 34509, 26035, 36467, 32061],\n",
       " 34999: [28200, 35654, 38092, 14462, 20705, 19938, 36803, 28226, 20574, 35218],\n",
       " 34998: [26835, 27675, 27735, 37432, 36205, 37592, 37998, 27356, 27733, 27807],\n",
       " 34997: [28476, 33174, 27456, 37291, 14462, 27783, 27626, 36529, 33364, 36364],\n",
       " 34996: [20632, 26105, 27675, 36612, 36803, 28226, 27111, 27575, 27607, 27935],\n",
       " 34995: [36361, 27236, 27760, 20714, 31026, 20574, 20670, 27709, 26977, 20388],\n",
       " 34994: [27647, 26950, 20575, 27416, 26425, 34834, 27154, 20576, 37105, 37845],\n",
       " 34993: [28177, 28262, 26901, 35278, 28323, 28368, 37423, 28438, 14369, 37960],\n",
       " 34992: [25392, 25417, 25418, 25435, 27074, 20099, 24405, 28123, 25437, 25343],\n",
       " 34991: [27763, 20527, 26367, 28350, 25411, 27191, 35069, 35380, 15326, 24389],\n",
       " 34990: [35774, 31880, 20031, 20339, 20457, 31884, 20338, 36580, 20456, 20600],\n",
       " 34989: [33655, 34399, 34342, 14636, 26390, 24416, 35650, 27009, 36061, 24415],\n",
       " 34988: [20154, 37595, 37958, 38081, 28458, 37274, 37620, 28432, 37540, 36928],\n",
       " 34987: [35876, 27258, 36652, 34940, 37476, 37744, 37372, 35084, 27026, 35470],\n",
       " 34986: [33825, 34399, 28432, 35277, 33873, 34732, 34749, 26397, 35899, 35330],\n",
       " 34985: [36008, 35992, 34578, 33358, 35397, 35404, 35876, 35882, 26430, 27024],\n",
       " 34984: [26397, 25564, 28064, 35579, 27008, 26314, 35749, 27024, 24877, 34940],\n",
       " 34983: [36693, 35777, 37388, 36458, 36905, 36839, 37565, 37858, 36591, 24500],\n",
       " 34982: [27497, 34524, 24465, 26368, 35531, 20107, 28499, 35397, 35610, 35277],\n",
       " 34981: [28260, 27445, 20393, 37882, 26505, 25970, 35174, 34212, 28389, 37081],\n",
       " 34980: [28260, 26581, 28540, 27143, 28187, 27591, 37959, 37033, 35119, 28525],\n",
       " 34979: [26276, 37992, 28353, 27619, 28100, 26082, 27242, 28289, 36583, 27709],\n",
       " 34978: [31567, 14654, 37729, 20644, 34658, 28554, 27709, 20523, 28415, 27423],\n",
       " 34977: [34064, 26918, 27908, 14676, 24742, 36150, 27777, 34602, 27601, 31854],\n",
       " 34976: [36405, 36806, 25966, 27812, 32643, 28200, 36150, 34507, 28137, 11135],\n",
       " 34975: [20324, 26811, 26013, 20326, 28119, 37398, 36154, 35217, 28148, 36902],\n",
       " 34974: [25224, 27216, 29029, 33863, 37498, 24888, 17964, 32719, 18009, 35360],\n",
       " 34973: [36702, 28126, 14640, 27540, 20675, 26707, 37827, 36876, 28555, 26921],\n",
       " 34972: [20593, 30169, 26712, 35307, 27537, 28389, 28219, 36070, 37963, 35790],\n",
       " 34971: [25415, 36336, 36934, 28226, 26727, 36903, 20739, 36069, 25431, 36123],\n",
       " 34970: [32226, 28100, 28048, 31994, 32001, 31999, 14563, 27239, 27423, 38021],\n",
       " 34969: [27216, 37989, 27191, 35928, 37246, 27345, 27943, 28177, 27231, 28281],\n",
       " 34968: [36452, 36249, 35882, 26606, 14090, 35438, 14643, 37748, 37995, 28083],\n",
       " 34967: [27739, 35988, 26906, 28068, 36346, 37792, 37958, 37079, 24552, 14271],\n",
       " 34966: [20684, 20469, 28132, 31033, 32082, 37908, 37997, 28470, 37851, 14671],\n",
       " 34965: [37039, 28363, 36816, 35127, 28371, 35175, 26015, 35034, 27814, 37833],\n",
       " 34964: [28443, 27859, 14664, 35624, 28200, 37843, 28203, 38065, 36655, 37725],\n",
       " 34963: [27908, 33724, 31979, 34896, 37681, 27989, 28504, 28354, 28442, 36984],\n",
       " 34962: [20340, 26394, 26006, 27264, 36105, 24411, 26395, 20731, 24686, 2476],\n",
       " 34961: [27191, 27199, 27288, 20752, 37866, 36832, 37342, 20525, 36378, 25432],\n",
       " 34960: [27732, 25416, 20575, 37575, 28106, 24385, 37763, 25423, 27377, 25415],\n",
       " 34959: [27910, 26188, 27374, 34714, 27375, 37380, 20511, 25415, 36530, 20376],\n",
       " 34958: [20137, 14654, 20479, 27583, 20608, 14096, 37845, 28500, 26851, 37484],\n",
       " 34957: [27378, 37515, 20574, 37713, 20724, 31031, 20723, 27768, 31034, 25423],\n",
       " 34956: [34902, 20723, 36844, 25392, 28282, 38045, 25413, 12565, 36636, 36070],\n",
       " 34955: [36873, 25432, 20388, 28273, 37515, 20724, 36005, 26610, 27437, 37880],\n",
       " 34954: [37387, 28077, 20633, 20724, 27607, 27935, 37675, 36020, 27304, 27001],\n",
       " 34953: [27427, 26590, 31581, 20576, 27298, 22783, 30169, 1660, 1775, 37808],\n",
       " 34952: [27575, 14654, 20392, 28226, 25415, 21096, 26254, 38069, 27236, 27298],\n",
       " 34951: [37058, 27313, 20724, 26559, 20723, 26225, 28097, 26827, 25591, 27030],\n",
       " 34950: [35556, 27407, 31979, 26810, 20637, 37058, 26969, 20723, 37168, 27067],\n",
       " 34949: [28527, 26410, 37640, 20723, 35160, 20615, 28480, 35302, 31035, 14621],\n",
       " 34948: [37423, 37178, 28438, 20575, 20724, 28281, 20723, 20576, 27750, 28364],\n",
       " 34947: [28509, 28158, 28459, 37813, 34343, 36919, 36260, 35831, 35919, 20677],\n",
       " 34946: [20574, 20576, 31975, 19986, 21096, 14338, 27298, 31977, 27575, 38092],\n",
       " 34945: [20575, 27236, 20566, 28226, 31977, 26750, 31973, 19983, 26254, 27675],\n",
       " 34944: [31350, 37527, 20574, 37713, 20576, 34791, 28333, 20725, 24416, 14100],\n",
       " 34943: [28428, 34767, 28251, 27989, 38092, 36402, 20712, 14621, 37721, 30037],\n",
       " 34942: [28164, 27575, 34623, 35967, 37432, 20232, 37530, 28441, 28126, 27878],\n",
       " 34941: [35556, 31722, 28260, 38073, 31725, 20634, 37851, 31978, 20576, 31678],\n",
       " 34940: [25102, 28301, 37026, 28172, 27588, 37130, 28129, 28097, 28267, 27781],\n",
       " 34939: [28077, 36582, 28158, 27432, 37491, 28208, 34909, 38107, 20210, 20485],\n",
       " 34938: [32081, 27201, 36804, 26754, 37667, 22387, 27971, 37560, 36402, 20777],\n",
       " 34937: [30331, 28292, 27675, 20392, 20705, 20344, 25431, 25392, 21095, 30855],\n",
       " 34936: [27295, 27445, 28180, 27831, 36373, 20713, 26715, 36798, 37217, 28400],\n",
       " 34935: [36397, 28349, 27154, 28226, 27760, 27403, 26936, 20633, 27439, 35790],\n",
       " 34934: [28476, 31978, 36261, 31980, 37677, 27647, 31975, 28023, 28349, 37551],\n",
       " 34933: [37289, 26820, 19939, 37578, 37711, 28100, 14632, 37791, 36583, 38092],\n",
       " 34932: [32068, 20575, 20594, 14579, 20344, 27298, 14654, 20137, 25392, 25437],\n",
       " 34931: [31765, 28368, 27709, 36919, 27423, 32041, 28353, 28466, 37759, 35227],\n",
       " 34930: [37026, 37462, 37960, 36781, 37432, 28414, 38087, 20630, 37942, 28402],\n",
       " 34929: [36210, 25080, 34496, 31763, 26894, 19467, 24579, 31920, 25962, 35425],\n",
       " 34928: [23677, 35041, 29847, 37372, 36297, 31573, 27164, 27276, 27983, 37717],\n",
       " 34927: [35370, 27497, 36126, 28374, 20601, 27510, 27255, 35531, 37792, 26688],\n",
       " 34926: [34211, 35468, 34783, 36703, 34290, 35550, 14689, 27759, 20657, 27624],\n",
       " 34925: [27164, 34332, 35992, 35397, 28432, 28454, 36849, 37023, 35777, 37404],\n",
       " 34924: [36800, 33751, 34372, 37215, 23677, 14418, 36629, 36144, 27255, 14482],\n",
       " 34923: [26606, 37388, 34795, 24246, 35229, 20107, 25210, 26944, 31446, 34783],\n",
       " 34922: [36529, 36362, 27785, 28065, 36070, 38011, 38013, 37422, 36784, 37712],\n",
       " 34921: [37026, 28106, 30872, 14546, 27580, 27701, 31976, 27047, 28268, 27913],\n",
       " 34920: [28401, 28049, 37041, 37813, 28282, 28200, 37510, 20644, 36729, 37816],\n",
       " 34919: [30808, 38094, 37537, 27696, 27732, 28203, 27963, 37720, 14096, 37712],\n",
       " 34918: [31563, 25389, 27805, 27575, 27495, 28309, 36444, 37780, 37286, 26422],\n",
       " 34917: [31565, 38030, 26424, 37798, 27750, 21096, 37525, 35374, 27346, 14546],\n",
       " 34916: [26597, 37108, 36740, 28097, 27079, 28226, 28200, 25415, 28545, 37289],\n",
       " 34915: [19939, 34397, 35521, 20375, 34631, 27552, 20697, 25370, 15330, 15966],\n",
       " 34914: [37289, 35879, 36616, 20635, 37172, 27756, 37561, 14621, 28260, 37718],\n",
       " 34913: [28476, 37041, 20575, 27709, 36069, 27423, 28554, 27723, 27154, 37791],\n",
       " 34912: [27760, 28053, 27647, 26619, 20739, 36570, 27744, 26851, 28104, 37886],\n",
       " 34911: [24575, 20575, 27675, 27111, 27231, 25423, 24417, 27067, 28126, 25415],\n",
       " 34910: [37259, 37840, 37763, 28260, 28126, 37753, 37998, 37481, 34923, 37592],\n",
       " 34909: [20388, 28226, 27163, 25437, 20305, 25426, 37400, 28415, 38054, 28200],\n",
       " 34908: [27616, 20632, 37472, 26436, 36247, 27693, 28384, 28402, 36714, 28549],\n",
       " 34907: [27525, 28078, 28441, 27458, 25591, 14100, 28333, 37872, 37793, 20575],\n",
       " 34906: [20632, 34573, 37998, 28333, 20575, 20576, 21096, 28260, 28562, 28516],\n",
       " 34905: [37108, 19939, 27744, 36394, 26886, 19938, 20727, 36872, 20344, 28200],\n",
       " 34904: [27888, 20633, 20632, 37960, 37400, 27575, 27675, 37289, 31043, 36529],\n",
       " 34903: [20397, 31725, 31723, 35393, 28479, 34561, 27337, 31179, 26429, 25122],\n",
       " 34902: [27888, 37774, 20766, 28065, 36107, 27027, 28123, 37716, 27329, 36414],\n",
       " 34901: [28476, 37791, 26269, 20575, 20725, 28397, 26559, 38092, 37497, 25392],\n",
       " 34900: [31350, 24993, 24415, 27636, 34810, 28292, 25908, 35715, 20748, 35269],\n",
       " 34899: [19936, 27844, 19938, 28441, 31980, 27750, 25406, 32068, 35150, 19935],\n",
       " 34898: [27610, 31989, 36616, 31336, 26960, 26476, 27835, 33983, 38082, 37960],\n",
       " 34897: [30951, 30950, 36737, 26827, 36370, 24774, 27745, 26225, 36821, 25503],\n",
       " 34896: [26960, 20494, 28555, 27543, 27270, 27720, 23712, 35797, 25352, 26922],\n",
       " 34895: [28364, 28442, 37510, 36230, 37731, 20576, 32009, 26818, 37727, 37168],\n",
       " 34894: [26688, 24629, 14636, 37206, 27726, 36800, 36233, 35882, 34332, 29829],\n",
       " 34893: [31577, 35185, 31612, 28071, 36226, 32897, 19477, 34226, 19475, 33785],\n",
       " 34892: [20015, 35187, 37715, 15552, 35771, 16175, 35179, 35370, 26944, 28071],\n",
       " 34891: [28374, 28310, 34067, 26944, 37062, 26417, 33358, 36073, 27759, 32832],\n",
       " 34890: [26797, 24878, 26944, 26430, 27008, 19458, 20107, 34686, 35169, 27258],\n",
       " 34889: [33197, 23677, 26688, 26944, 24629, 27008, 34873, 36340, 36178, 27024],\n",
       " 34888: [14182, 34399, 25564, 34940, 37088, 36980, 35988, 37785, 19458, 25884],\n",
       " 34887: [26390, 37206, 25210, 19458, 27510, 35370, 35412, 34342, 27759, 27009],\n",
       " 34886: [28547, 38081, 28304, 37344, 25564, 26427, 35283, 26688, 36346, 35963],\n",
       " 34885: [27864, 27739, 28083, 37120, 26932, 26944, 35876, 35882, 32038, 35397],\n",
       " 34884: [36233, 36676, 37620, 25884, 35370, 36346, 35882, 37206, 36693, 36505],\n",
       " 34883: [27487, 20705, 36749, 27242, 27163, 27047, 26162, 19983, 20500, 20344],\n",
       " 34882: [27616, 28222, 28482, 27280, 14596, 21111, 27964, 28503, 34253, 20633],\n",
       " 34881: [36806, 25077, 25343, 28226, 25639, 24575, 36570, 28192, 26332, 37105],\n",
       " 34880: [20575, 27744, 34857, 14682, 37315, 24553, 25001, 20498, 37466, 20576],\n",
       " 34879: [20704, 36840, 27530, 25416, 36480, 14567, 36784, 27709, 37424, 27601],\n",
       " 34878: [27469, 26664, 24885, 35669, 27535, 20469, 26330, 27140, 35428, 25515],\n",
       " 34877: [25286, 25415, 25465, 20412, 35253, 27349, 27785, 25301, 25466, 27649],\n",
       " 34876: [27137, 26918, 27777, 20574, 36129, 28097, 14682, 31031, 20576, 28187],\n",
       " 34875: [27193, 37515, 28213, 27723, 27135, 27917, 27732, 14682, 37725, 20748],\n",
       " 34874: [27622, 28349, 37561, 28194, 31968, 28464, 31967, 38115, 14473, 37355],\n",
       " 34873: [25365, 27505, 28100, 20479, 34397, 27768, 25699, 25413, 26918, 27818],\n",
       " 34872: [26962, 14683, 20633, 35157, 27374, 28137, 27701, 26727, 35765, 28337],\n",
       " 34871: [20338, 28500, 35601, 36884, 20731, 20740, 30163, 20698, 3108, 36413],\n",
       " 34870: [35889, 26597, 19629, 25039, 22825, 26600, 26018, 36012, 25795, 37646],\n",
       " 34869: [36738, 37940, 27345, 36844, 36866, 28561, 14671, 14626, 37525, 35548],\n",
       " 34868: [28262, 20575, 14671, 14615, 37423, 26909, 31034, 27693, 27443, 31035],\n",
       " 34867: [27047, 28507, 27485, 24575, 27616, 26922, 20481, 33976, 33961, 25594],\n",
       " 34866: [28549, 20576, 20633, 28088, 27744, 28025, 36642, 28476, 27959, 37925],\n",
       " 34865: [14654, 31020, 20723, 28200, 21096, 20632, 14579, 28354, 37665, 37763],\n",
       " 34864: [36750, 31581, 28275, 31585, 31580, 26886, 14621, 36994, 37713, 28333],\n",
       " 34863: [28260, 28264, 37864, 28275, 28354, 27693, 37832, 14645, 31977, 28549],\n",
       " 34862: [37712, 28164, 37998, 27675, 28252, 28275, 37960, 36069, 37807, 27154],\n",
       " 34861: [35734, 27403, 27583, 14621, 35520, 36129, 20551, 27143, 37864, 28549],\n",
       " 34860: [20575, 31580, 31582, 28275, 28226, 27721, 20209, 37665, 37479, 28549],\n",
       " 34859: [36069, 36409, 20576, 20740, 27143, 26851, 37845, 28294, 28147, 36884],\n",
       " 34858: [28275, 27917, 27625, 37041, 20485, 15966, 32050, 35521, 28301, 27812],\n",
       " 34857: [28549, 27580, 27723, 28554, 35366, 20575, 27709, 31726, 20484, 27423],\n",
       " 34856: [27057, 28285, 27963, 38093, 36699, 37083, 37139, 31797, 37001, 28245],\n",
       " 34855: [27588, 28549, 37880, 28267, 26342, 37432, 28349, 26126, 28158, 38094],\n",
       " 34854: [32069, 20748, 37472, 27783, 36781, 14645, 27926, 28428, 20682, 27445],\n",
       " 34853: [20485, 28476, 19234, 36781, 37925, 14645, 37331, 36647, 14680, 37552],\n",
       " 34852: [20574, 36993, 31947, 31020, 37428, 35653, 27745, 27534, 37329, 14579],\n",
       " 34851: [36806, 20632, 27575, 37108, 31021, 26977, 27723, 19255, 28476, 35715],\n",
       " 34850: [28441, 36726, 14621, 28052, 14680, 37832, 28402, 36381, 37737, 37871],\n",
       " 34849: [36950, 35841, 27723, 37012, 24553, 28275, 20484, 28549, 24958, 28292],\n",
       " 34848: [37882, 24416, 31582, 26126, 24415, 31585, 26809, 31580, 28088, 31581],\n",
       " 34847: [37472, 27693, 26960, 27771, 37925, 36863, 14645, 25140, 35012, 28275],\n",
       " 34846: [36570, 20499, 28065, 27831, 20498, 36881, 37038, 37742, 24192, 36698],\n",
       " 34845: [37888, 14677, 28077, 36726, 28441, 38066, 37832, 37634, 28275, 25423],\n",
       " 34844: [31327, 17313, 37481, 20499, 28158, 37331, 27693, 27831, 28349, 27654],\n",
       " 34843: [36123, 28409, 20696, 37723, 37105, 28476, 28053, 28416, 28275, 37341],\n",
       " 34842: [28158, 14621, 37882, 27573, 37315, 28334, 37942, 26369, 28241, 36383],\n",
       " 34841: [31382, 36961, 28104, 26501, 31021, 20124, 31385, 28267, 26424, 27110],\n",
       " 34840: [36948, 28275, 36480, 28049, 28496, 14680, 28402, 36247, 38065, 28200],\n",
       " 34839: [14621, 27693, 37331, 36247, 36107, 38045, 28298, 38057, 37496, 37472],\n",
       " 34838: [37561, 28252, 37895, 27580, 38117, 28273, 37925, 37331, 37960, 20576],\n",
       " 34837: [28459, 27811, 28275, 37081, 28169, 37371, 35708, 26163, 27140, 20338],\n",
       " 34836: [20574, 28549, 36069, 25077, 20344, 36397, 27917, 28226, 21096, 14338],\n",
       " 34835: [28275, 36726, 37925, 28402, 20633, 27709, 28476, 36642, 28100, 27445],\n",
       " 34834: [27374, 27579, 36150, 14714, 28275, 23846, 20305, 38076, 26959, 28187],\n",
       " 34833: [36988, 32082, 27774, 36781, 21826, 14462, 21717, 35654, 36467, 36895],\n",
       " 34832: [27830, 28285, 14560, 26766, 28005, 14473, 31968, 36305, 26664, 31606],\n",
       " 34831: [20757, 20756, 33706, 20754, 20073, 20295, 31879, 34352, 33717, 20293],\n",
       " 34830: [25851, 14620, 28063, 37832, 36616, 27140, 37723, 27835, 28066, 26163],\n",
       " 34829: [23175, 24382, 37400, 36992, 31381, 28550, 31385, 28436, 28049, 31383],\n",
       " 34828: [14620, 20617, 20638, 28368, 28344, 37423, 38027, 20210, 20705, 20637],\n",
       " 34827: [20484, 27830, 28456, 28554, 27693, 27455, 37472, 37156, 36714, 38087],\n",
       " 34826: [31567, 27575, 27912, 20608, 31581, 27208, 28353, 26712, 25591, 20575],\n",
       " 34825: [35742, 28355, 35050, 35914, 35557, 36088, 36089, 37758, 36323, 36324],\n",
       " 34824: [38044, 27599, 32068, 28256, 37462, 26188, 27031, 21886, 34972, 34261],\n",
       " 34823: [24993, 26501, 27172, 24388, 25343, 25436, 25417, 26886, 23712, 14317],\n",
       " 34822: [28549, 31034, 31027, 27723, 37930, 14615, 26387, 27917, 27859, 31035],\n",
       " 34821: [37592, 28132, 37998, 26709, 31989, 37289, 27110, 37905, 31026, 31027],\n",
       " 34820: [38119, 28474, 26835, 14671, 26861, 28413, 14588, 26379, 27978, 26709],\n",
       " 34819: [28353, 28038, 27106, 25431, 37960, 25415, 20525, 36714, 37472, 27693],\n",
       " 34818: [25362, 14676, 26254, 26541, 31994, 31027, 27118, 31978, 27580, 31029],\n",
       " 34817: [31034, 37423, 28438, 31027, 14671, 14588, 26697, 31025, 21717, 14623],\n",
       " 34816: [20483, 31563, 24416, 31035, 20575, 31031, 14671, 20574, 38039, 35850],\n",
       " 34815: [31025, 31034, 37105, 31027, 26886, 28275, 37960, 26505, 36616, 14671],\n",
       " 34814: [26712, 31027, 31032, 20511, 31035, 28108, 14714, 28097, 14671, 20725],\n",
       " 34813: [38039, 25418, 36439, 25432, 30301, 14626, 25661, 26420, 26697, 26379],\n",
       " 34812: [36608, 31034, 20633, 20638, 27675, 26040, 14623, 28050, 28106, 27231],\n",
       " 34811: [31033, 28474, 28476, 14671, 31032, 31028, 26379, 26697, 14623, 31035],\n",
       " 34810: [28262, 28527, 31029, 31035, 28438, 14615, 14671, 38039, 31034, 24774],\n",
       " 34809: [20638, 36397, 32068, 14680, 20377, 28088, 20378, 36642, 37925, 25699],\n",
       " 34808: [27649, 27709, 28287, 34674, 28137, 14612, 28097, 20523, 36397, 31977],\n",
       " 34807: [26486, 35458, 34812, 30890, 26720, 8044, 36724, 26993, 19675, 20600],\n",
       " 34806: [37870, 27831, 36214, 20633, 25608, 36463, 38021, 37321, 27498, 1935],\n",
       " 34805: [20575, 37178, 31034, 31026, 14615, 14671, 36844, 37882, 20523, 28282],\n",
       " 34804: [24949, 24991, 14582, 26480, 37075, 36021, 14563, 14661, 33778, 27192],\n",
       " 34803: [26963, 27001, 25136, 26661, 35527, 37834, 31725, 19980, 19718, 17337],\n",
       " 34802: [27739, 26698, 27026, 33785, 26016, 28547, 28499, 27024, 35330, 36755],\n",
       " 34801: [35229, 27624, 37062, 34892, 14428, 36178, 27890, 36629, 37850, 26932],\n",
       " 34800: [27216, 26794, 23840, 35253, 36863, 27520, 14626, 14623, 37498, 14681],\n",
       " 34799: [27541, 31854, 37022, 26909, 28096, 26959, 28200, 34507, 27047, 31043],\n",
       " 34798: [32655, 26298, 21095, 25415, 23712, 24429, 27709, 25392, 24412, 28020],\n",
       " 34797: [26107, 36538, 26427, 27892, 25496, 27814, 37372, 35370, 33271, 35397],\n",
       " 34796: [35149, 14602, 26194, 33947, 35773, 25392, 24388, 25436, 25435, 25393],\n",
       " 34795: [37930, 31020, 27917, 25428, 27732, 27826, 26522, 36637, 28306, 25343],\n",
       " 34794: [27724, 26794, 31606, 27647, 19939, 27345, 36069, 37105, 31881, 35587],\n",
       " 34793: [36806, 32067, 26254, 27452, 20632, 32069, 28476, 27231, 34212, 27723],\n",
       " 34792: [27583, 25392, 31886, 31885, 28096, 31887, 20714, 27298, 31884, 27763],\n",
       " 34791: [26207, 26992, 27300, 31021, 20723, 31581, 37484, 20546, 14579, 20525],\n",
       " 34790: [20575, 27300, 27225, 25426, 25418, 25434, 27671, 28226, 25417, 25168],\n",
       " 34789: [20684, 20305, 20681, 27495, 26505, 36988, 36698, 32082, 32048, 14680],\n",
       " 34788: [27790, 37120, 36144, 37149, 36226, 35882, 35650, 36346, 26606, 37748],\n",
       " 34787: [35509, 32716, 25854, 34247, 8095, 37807, 33394, 34387, 24936, 25962],\n",
       " 34786: [31372, 19767, 19766, 19768, 26021, 32597, 5651, 31520, 10520, 11245],\n",
       " 34785: [28364, 31035, 28262, 28368, 31034, 26697, 31027, 14593, 38119, 37880],\n",
       " 34784: [31026, 27760, 27427, 14593, 26541, 38039, 14627, 14625, 26254, 14544],\n",
       " 34783: [31033, 14623, 14625, 31035, 14588, 14544, 26262, 31025, 14659, 20316],\n",
       " 34782: [14590, 31994, 37416, 20576, 19899, 31027, 25415, 14588, 19900, 31031],\n",
       " 34781: [31029, 14615, 14544, 31035, 14588, 25415, 31032, 38039, 26379, 20530],\n",
       " 34780: [32158, 27626, 36151, 14671, 31033, 31032, 38119, 31031, 26170, 31034],\n",
       " 34779: [14591, 31027, 24993, 27374, 21717, 14588, 37290, 20594, 31035, 20575],\n",
       " 34778: [27530, 14627, 19938, 14623, 27709, 37720, 20499, 27074, 32048, 31027],\n",
       " 34777: [14625, 31028, 14544, 14588, 11079, 25402, 14651, 25406, 26686, 25397],\n",
       " 34776: [31032, 31027, 14588, 38039, 14544, 31028, 25411, 25402, 31025, 25406],\n",
       " 34775: [26697, 14430, 31035, 31026, 31029, 14588, 20575, 14544, 25419, 27208],\n",
       " 34774: [26697, 14591, 14671, 14626, 38039, 14588, 14544, 31025, 38119, 28100],\n",
       " 34773: [31029, 14623, 14588, 14659, 14544, 31025, 20576, 20574, 20575, 25411],\n",
       " 34772: [14593, 14591, 31029, 14615, 14659, 14544, 14588, 31028, 25402, 26379],\n",
       " 34771: [36595, 38119, 14659, 14588, 14544, 31028, 34119, 31025, 14651, 26379],\n",
       " 34770: [31033, 14625, 31027, 38119, 31028, 14659, 14588, 14544, 25419, 28262],\n",
       " 34769: [14593, 31035, 31034, 14623, 14588, 14544, 14659, 26686, 31030, 14651],\n",
       " 34768: [14625, 14588, 14544, 31028, 31025, 25411, 26541, 25402, 25406, 14651],\n",
       " 34767: [14590, 31034, 38039, 31025, 28523, 26541, 37908, 14588, 28504, 14659],\n",
       " 34766: [31033, 14590, 31026, 38039, 14659, 31028, 14588, 14544, 25436, 25435],\n",
       " 34765: [31996, 27917, 26709, 14591, 31034, 27732, 14615, 31994, 31031, 33355],\n",
       " 34764: [31027, 14615, 14659, 31028, 14544, 14588, 31025, 25402, 25406, 14651],\n",
       " 34763: [14627, 31026, 31032, 36694, 31993, 31996, 28262, 14671, 14623, 14626],\n",
       " 34762: [14591, 31034, 31035, 31027, 36844, 14544, 14588, 28282, 38119, 28472],\n",
       " 34761: [27943, 35928, 28295, 26240, 27345, 37886, 37116, 37463, 33963, 28269],\n",
       " 34760: [37287, 28282, 36504, 35548, 14544, 27689, 38013, 27419, 14096, 27744],\n",
       " 34759: [28405, 27462, 20574, 28368, 26715, 14544, 27978, 20576, 27661, 25471],\n",
       " 34758: [27784, 14544, 23929, 27030, 27772, 23712, 25435, 27163, 12458, 30856],\n",
       " 34757: [27037, 14544, 26619, 20705, 19967, 25415, 26188, 27170, 35302, 35647],\n",
       " 34756: [26697, 14627, 14544, 14615, 31033, 14625, 31032, 31035, 31025, 38039],\n",
       " 34755: [34791, 27706, 26351, 14544, 14671, 31033, 25431, 25417, 31027, 31031],\n",
       " 34754: [24427, 21728, 20494, 27135, 34941, 21730, 27625, 24425, 24428, 24416],\n",
       " 34753: [35414, 30950, 30952, 37065, 30954, 19731, 31581, 37497, 31583, 35189],\n",
       " 34752: [30951, 30950, 26856, 27466, 24836, 30954, 37454, 25503, 27317, 20532],\n",
       " 34751: [20343, 19254, 37042, 37432, 38115, 25643, 34412, 26923, 24426, 27575],\n",
       " 34750: [14639, 27677, 36570, 20727, 31021, 28023, 27661, 27852, 24388, 25428],\n",
       " 34749: [19868, 31985, 19898, 37396, 36291, 28117, 28024, 37280, 37729, 20600],\n",
       " 34748: [37428, 14563, 26682, 28048, 26917, 32074, 34641, 32056, 32055, 36308],\n",
       " 34747: [28110, 35654, 35218, 25136, 26963, 14462, 25545, 14582, 30780, 35435],\n",
       " 34746: [35270, 27390, 14563, 24949, 28048, 20499, 26285, 26330, 37350, 36708],\n",
       " 34745: [36463, 14563, 26354, 25797, 25187, 14567, 14613, 27517, 14661, 37241],\n",
       " 34744: [26351, 14563, 20315, 14613, 35362, 31963, 30907, 20317, 35527, 36255],\n",
       " 34743: [14690, 27231, 31725, 14563, 28014, 31973, 28192, 34135, 27675, 14613],\n",
       " 34742: [33650, 25369, 28353, 28004, 20500, 20386, 27632, 20715, 35012, 20714],\n",
       " 34741: [26861, 27304, 25365, 25369, 27001, 25413, 37426, 26336, 25406, 27040],\n",
       " 34740: [26891, 20575, 26424, 26861, 35435, 26619, 25417, 14546, 35091, 28268],\n",
       " 34739: [27356, 31581, 19967, 20511, 34573, 27505, 19938, 37604, 27042, 14654],\n",
       " 34738: [36608, 28389, 27675, 20748, 28200, 20574, 30775, 20575, 28226, 32071],\n",
       " 34737: [28158, 31021, 36844, 20485, 28281, 32057, 28316, 20483, 37105, 14692],\n",
       " 34736: [27375, 20574, 20576, 14654, 28200, 12565, 28343, 14498, 37763, 20723],\n",
       " ...}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_prediction_dict = decode_predictions(categorical_training_dataframe, drop_ids=False)\n",
    "submission_prediction_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_recommendation(recommendation: list[str]) -> str:\n",
    "    return \" \".join([str(item) for item in recommendation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>572 7547 9911 399 7703 11580 14888 531 4232 28958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>6348 14748 13766 9812 18964 11149 7139 572 313...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>8403 22692 15542 29963 13252 22714 21363 22589...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>25140 25079 6189 25643 23023 23712 11362 6827 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>15902 9812 17624 8612 9447 18304 4695 18647 34...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34226</th>\n",
       "      <td>35729</td>\n",
       "      <td>36844 26093 24415 24417 35548 36527 26581 2753...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34227</th>\n",
       "      <td>35730</td>\n",
       "      <td>28247 38027 37211 37420 37739 37874 27350 2811...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34228</th>\n",
       "      <td>35731</td>\n",
       "      <td>37739 36263 38027 37427 36525 37623 28119 3539...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34229</th>\n",
       "      <td>35734</td>\n",
       "      <td>37069 36610 36168 35345 37550 37067 35093 3688...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34230</th>\n",
       "      <td>35735</td>\n",
       "      <td>37657 36493 36773 36917 36034 37445 37660 3692...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34231 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_id                                          item_list\n",
       "0            0  572 7547 9911 399 7703 11580 14888 531 4232 28958\n",
       "1            1  6348 14748 13766 9812 18964 11149 7139 572 313...\n",
       "2            2  8403 22692 15542 29963 13252 22714 21363 22589...\n",
       "3            3  25140 25079 6189 25643 23023 23712 11362 6827 ...\n",
       "4            4  15902 9812 17624 8612 9447 18304 4695 18647 34...\n",
       "...        ...                                                ...\n",
       "34226    35729  36844 26093 24415 24417 35548 36527 26581 2753...\n",
       "34227    35730  28247 38027 37211 37420 37739 37874 27350 2811...\n",
       "34228    35731  37739 36263 38027 37427 36525 37623 28119 3539...\n",
       "34229    35734  37069 36610 36168 35345 37550 37067 35093 3688...\n",
       "34230    35735  37657 36493 36773 36917 36034 37445 37660 3692...\n",
       "\n",
       "[34231 rows x 2 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[\"item_list\"] = [\n",
    "    encode_recommendation(recommendation)\n",
    "    for recommendation in pd.Series(submission_prediction_dict).loc[test_df[\"user_id\"]]\n",
    "]\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv(SUBMISSION_PATH, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
